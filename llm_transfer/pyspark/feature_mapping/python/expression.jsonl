{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.__getattr__.html#pyspark.sql.DataFrame.__getattr__"], "Title": ["DataFrame.__getattr__"], "Feature": ["DataFrame.__getattr__"], "Description": "Returns theColumndenoted byname.", "Examples": [">>> df = spark.createDataFrame([\n...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])", ">>> df.select(df.age).show()\n+---+\n|age|\n+---+\n|  2|\n|  5|\n+---+"], "Parameters": [["name str", "Column name to return as Column ."]], "Returns": [["Column", "Requested column."]], "Category": ["DataFrame"], "index": 0}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.__getitem__.html#pyspark.sql.DataFrame.__getitem__"], "Title": ["DataFrame.__getitem__"], "Feature": ["DataFrame.__getitem__"], "Description": "Returns the column as aColumn.", "Examples": [">>> df = spark.createDataFrame([\n...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])", ">>> df.select(df['age']).show()\n+---+\n|age|\n+---+\n|  2|\n|  5|\n+---+", ">>> df.select(df[1]).show()\n+-----+\n| name|\n+-----+\n|Alice|\n|  Bob|\n+-----+", ">>> df[[\"name\", \"age\"]].show()\n+-----+---+\n| name|age|\n+-----+---+\n|Alice|  2|\n|  Bob|  5|\n+-----+---+\n>>> df[df.age > 3].show()\n+---+----+\n|age|name|\n+---+----+\n|  5| Bob|\n+---+----+\n>>> df[df[0] > 3].show()\n+---+----+\n|age|name|\n+---+----+\n|  5| Bob|\n+---+----+"], "Parameters": [["item int, str, Column , list or tuple", "column index, column name, column, or a list or tuple of columns"]], "Returns": [["Column or DataFrame", "a specified column, or a filtered or projected dataframe. If the input item is an int or str, the output is a Column . If the input item is a Column , the output is a DataFrame filtered by this given Column . If the input item is a list or tuple, the output is a DataFrame projected by this given list or tuple."], ["If the input item is a Column , the output is a DataFrame", "filtered by this given Column ."], ["If the input item is a list or tuple, the output is a DataFrame", "projected by this given list or tuple."]], "Category": ["DataFrame"], "index": 1}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.collect.html#pyspark.sql.DataFrame.collect"], "Title": ["DataFrame.collect"], "Feature": ["DataFrame.collect"], "Description": "Returns all the records in the DataFrame as a list ofRow.\nSee alsoDataFrame.takeReturns the firstnrows.DataFrame.headReturns the firstnrows.DataFrame.toPandasReturns the data as a pandas DataFrame.DataFrame.toArrowReturns the data as a PyArrow Table.\nNotes\nThis method should only be used if the resulting list is expected to be small,\nas all the data is loaded into the driver’s memory.", "Examples": [">>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.collect()\n[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]", ">>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.filter(df.age > 15).collect()\n[Row(age=23, name='Alice'), Row(age=16, name='Bob')]", ">>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.select(\"name\").collect()\n[Row(name='Tom'), Row(name='Alice'), Row(name='Bob')]", ">>> from pyspark.sql.functions import upper\n>>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.select(upper(df.name)).collect()\n[Row(upper(name)='TOM'), Row(upper(name)='ALICE'), Row(upper(name)='BOB')]", ">>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> rows = df.collect()\n>>> [row[\"name\"] for row in rows]\n['Tom', 'Alice', 'Bob']", ">>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> rows = df.collect()\n>>> [row.asDict() for row in rows]\n[{'age': 14, 'name': 'Tom'}, {'age': 23, 'name': 'Alice'}, {'age': 16, 'name': 'Bob'}]"], "Parameters": [], "Returns": [["list", "A list of Row objects, each representing a row in the DataFrame."]], "Category": ["DataFrame"], "index": 2}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameNaFunctions.replace.html#pyspark.sql.DataFrameNaFunctions.replace"], "Title": ["DataFrameNaFunctions.replace"], "Feature": ["DataFrameNaFunctions.replace"], "Description": "Returns a newDataFramereplacing a value with another value.DataFrame.replace()andDataFrameNaFunctions.replace()are\naliases of each other.\nValues to_replace and value must have the same type and can only be numerics, booleans,\nor strings. Value can have None. When replacing, the new value will be cast\nto the type of the existing column.\nFor numeric replacements all values to be replaced should have unique\nfloating point representation. In case of conflicts (for example with{42: -1, 42.0: 1})\nand arbitrary replacement will be used.", "Examples": [">>> df = spark.createDataFrame([\n...     (10, 80, \"Alice\"),\n...     (5, None, \"Bob\"),\n...     (None, 10, \"Tom\"),\n...     (None, None, None)],\n...     schema=[\"age\", \"height\", \"name\"])", ">>> df.na.replace(10, 20).show()\n+----+------+-----+\n| age|height| name|\n+----+------+-----+\n|  20|    80|Alice|\n|   5|  NULL|  Bob|\n|NULL|    20|  Tom|\n|NULL|  NULL| NULL|\n+----+------+-----+", ">>> df.na.replace('Alice', None).show()\n+----+------+----+\n| age|height|name|\n+----+------+----+\n|  10|    80|NULL|\n|   5|  NULL| Bob|\n|NULL|    10| Tom|\n|NULL|  NULL|NULL|\n+----+------+----+", ">>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n+----+------+----+\n| age|height|name|\n+----+------+----+\n|  10|    80|   A|\n|   5|  NULL|   B|\n|NULL|    10| Tom|\n|NULL|  NULL|NULL|\n+----+------+----+", ">>> df.na.replace(10, 18, 'age').show()\n+----+------+-----+\n| age|height| name|\n+----+------+-----+\n|  18|    80|Alice|\n|   5|  NULL|  Bob|\n|NULL|    10|  Tom|\n|NULL|  NULL| NULL|\n+----+------+-----+"], "Parameters": [["to_replace bool, int, float, string, list or dict, the value to be replaced.", "If the value is a dict, then value is ignored or can be omitted, and to_replace must be a mapping between a value and a replacement."], ["value bool, int, float, string or None, optional", "The replacement value must be a bool, int, float, string or None. If value is a\nlist, value should be of the same length and type as to_replace .\nIf value is a scalar and to_replace is a sequence, then value is\nused as a replacement for each item in to_replace ."], ["subset list, optional", "optional list of column names to consider.\nColumns specified in subset that do not have matching data types are ignored.\nFor example, if value is a string, and subset contains a non-string column,\nthen the non-string column is simply ignored."]], "Returns": [["DataFrame", "DataFrame with replaced values."]], "Category": ["DataFrame"], "index": 3}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameStatFunctions.approxQuantile.html#pyspark.sql.DataFrameStatFunctions.approxQuantile"], "Title": ["DataFrameStatFunctions.approxQuantile"], "Feature": ["DataFrameStatFunctions.approxQuantile"], "Description": "Calculates the approximate quantiles of numerical columns of aDataFrame.\nThe result of this algorithm has the following deterministic bound:\nIf theDataFramehas N elements and if we request the quantile at\nprobabilitypup to errorerr, then the algorithm will return\na samplexfrom theDataFrameso that theexactrank ofxis\nclose to (p * N). More precisely,\nThis method implements a variation of the Greenwald-Khanna\nalgorithm (with some speed optimizations). The algorithm was first\npresent in [[https://doi.org/10.1145/375663.375670Space-efficient Online Computation of Quantile Summaries]]\nby Greenwald and Khanna.\nNotes\nNull values will be ignored in numerical columns before calculation.\nFor columns only containing null values, an empty list is returned.", "Examples": [">>> data = [(1,), (2,), (3,), (4,), (5,)]\n>>> df = spark.createDataFrame(data, [\"values\"])\n>>> quantiles = df.approxQuantile(\"values\", [0.0, 0.5, 1.0], 0.05)\n>>> quantiles\n[1.0, 3.0, 5.0]", ">>> data = [(1, 10), (2, 20), (3, 30), (4, 40), (5, 50)]\n>>> df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n>>> quantiles = df.approxQuantile([\"col1\", \"col2\"], [0.0, 0.5, 1.0], 0.05)\n>>> quantiles\n[[1.0, 3.0, 5.0], [10.0, 30.0, 50.0]]", ">>> data = [(1,), (None,), (3,), (4,), (None,)]\n>>> df = spark.createDataFrame(data, [\"values\"])\n>>> quantiles = df.approxQuantile(\"values\", [0.0, 0.5, 1.0], 0.05)\n>>> quantiles\n[1.0, 3.0, 4.0]", ">>> data = [(1,), (2,), (3,), (4,), (5,)]\n>>> df = spark.createDataFrame(data, [\"values\"])\n>>> quantiles = df.approxQuantile(\"values\", [0.0, 0.2, 1.0], 0.1)\n>>> quantiles\n[1.0, 1.0, 5.0]"], "Parameters": [["col: str, tuple or list", "Can be a single column name, or a list of names for multiple columns. Changed in version 2.2.0: Added support for multiple columns."], ["probabilities list or tuple of floats", "a list of quantile probabilities\nEach number must be a float in the range [0, 1].\nFor example 0.0 is the minimum, 0.5 is the median, 1.0 is the maximum."], ["relativeError float", "The relative target precision to achieve\n(>= 0). If set to zero, the exact quantiles are computed, which\ncould be very expensive. Note that values greater than 1 are\naccepted but gives the same result as 1."]], "Returns": [["list", "the approximate quantiles at the given probabilities. If the input col is a string, the output is a list of floats. If the input col is a list or tuple of strings, the output is also a list, but each element in it is a list of floats, i.e., the output\nis a list of list of floats."], ["If the input col is a list or tuple of strings, the output is also a", "list, but each element in it is a list of floats, i.e., the output\nis a list of list of floats."]], "Category": ["DataFrame"], "index": 4}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameStatFunctions.corr.html#pyspark.sql.DataFrameStatFunctions.corr"], "Title": ["DataFrameStatFunctions.corr"], "Feature": ["DataFrameStatFunctions.corr"], "Description": "Calculates the correlation of two columns of aDataFrameas a double value.\nCurrently only supports the Pearson Correlation Coefficient.DataFrame.corr()andDataFrameStatFunctions.corr()are aliases of each other.", "Examples": [">>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n>>> df.corr(\"c1\", \"c2\")\n-0.3592106040535498\n>>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n>>> df.corr(\"small\", \"bigger\")\n1.0"], "Parameters": [["col1 str", "The name of the first column"], ["col2 str", "The name of the second column"], ["method str, optional", "The correlation method. Currently only supports “pearson”"]], "Returns": [["float", "Pearson Correlation Coefficient of two columns."]], "Category": ["DataFrame"], "index": 5}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameStatFunctions.cov.html#pyspark.sql.DataFrameStatFunctions.cov"], "Title": ["DataFrameStatFunctions.cov"], "Feature": ["DataFrameStatFunctions.cov"], "Description": "Calculate the sample covariance for the given columns, specified by their names, as a\ndouble value.DataFrame.cov()andDataFrameStatFunctions.cov()are aliases.", "Examples": [">>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n>>> df.cov(\"c1\", \"c2\")\n-18.0\n>>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n>>> df.cov(\"small\", \"bigger\")\n1.0"], "Parameters": [["col1 str", "The name of the first column"], ["col2 str", "The name of the second column"]], "Returns": [["float", "Covariance of two columns."]], "Category": ["DataFrame"], "index": 6}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameStatFunctions.crosstab.html#pyspark.sql.DataFrameStatFunctions.crosstab"], "Title": ["DataFrameStatFunctions.crosstab"], "Feature": ["DataFrameStatFunctions.crosstab"], "Description": "Computes a pair-wise frequency table of the given columns. Also known as a contingency\ntable.\nThe first column of each row will be the distinct values ofcol1and the column names\nwill be the distinct values ofcol2. The name of the first column will be$col1_$col2.\nPairs that have no occurrences will have zero as their counts.DataFrame.crosstab()andDataFrameStatFunctions.crosstab()are aliases.", "Examples": [">>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n>>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n+-----+---+---+---+\n|c1_c2| 10| 11|  8|\n+-----+---+---+---+\n|    1|  0|  2|  0|\n|    3|  1|  0|  0|\n|    4|  0|  0|  2|\n+-----+---+---+---+"], "Parameters": [["col1 str", "The name of the first column. Distinct items will make the first item of\neach row."], ["col2 str", "The name of the second column. Distinct items will make the column names\nof the DataFrame ."]], "Returns": [["DataFrame", "Frequency matrix of two columns."]], "Category": ["DataFrame"], "index": 7}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameStatFunctions.freqItems.html#pyspark.sql.DataFrameStatFunctions.freqItems"], "Title": ["DataFrameStatFunctions.freqItems"], "Feature": ["DataFrameStatFunctions.freqItems"], "Description": "Finding frequent items for columns, possibly with false positives. Using the\nfrequent element count algorithm described in\n“https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou”.DataFrame.freqItems()andDataFrameStatFunctions.freqItems()are aliases.\nNotes\nThis function is meant for exploratory data analysis, as we make no\nguarantee about the backward compatibility of the schema of the resultingDataFrame.", "Examples": [">>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n>>> df.freqItems([\"c1\", \"c2\"]).show()  \n+------------+------------+\n|c1_freqItems|c2_freqItems|\n+------------+------------+\n|   [4, 1, 3]| [8, 11, 10]|\n+------------+------------+"], "Parameters": [["cols list or tuple", "Names of the columns to calculate frequent items for as a list or tuple of\nstrings."], ["support float, optional", "The frequency with which to consider an item ‘frequent’. Default is 1%.\nThe support must be greater than 1e-4."]], "Returns": [["DataFrame", "DataFrame with frequent items."]], "Category": ["DataFrame"], "index": 8}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameStatFunctions.sampleBy.html#pyspark.sql.DataFrameStatFunctions.sampleBy"], "Title": ["DataFrameStatFunctions.sampleBy"], "Feature": ["DataFrameStatFunctions.sampleBy"], "Description": "Returns a stratified sample without replacement based on the\nfraction given on each stratum.", "Examples": [">>> from pyspark.sql.functions import col\n>>> dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n>>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n>>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n+---+-----+\n|key|count|\n+---+-----+\n|  0|    3|\n|  1|    6|\n+---+-----+\n>>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n33"], "Parameters": [["col Column or str", "column that defines strata Changed in version 3.0.0: Added sampling by a column of Column"], ["fractions dict", "sampling fraction for each stratum. If a stratum is not\nspecified, we treat its fraction as zero."], ["seed int, optional", "random seed"]], "Returns": [["a new DataFrame that represents the stratified sample", ""]], "Category": ["DataFrame"], "index": 9}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.table_arg.TableArg.partitionBy.html#pyspark.sql.table_arg.TableArg.partitionBy"], "Title": ["TableArg.partitionBy"], "Feature": ["TableArg.partitionBy"], "Description": "Partitions the data based on the specified columns.", "Examples": [], "Parameters": [], "Returns": [], "Category": ["DataFrame"], "index": 10}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.table_arg.TableArg.orderBy.html#pyspark.sql.table_arg.TableArg.orderBy"], "Title": ["TableArg.orderBy"], "Feature": ["TableArg.orderBy"], "Description": "Orders the data within each partition by the specified columns.", "Examples": [], "Parameters": [], "Returns": [], "Category": ["DataFrame"], "index": 11}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.table_arg.TableArg.withSinglePartition.html#pyspark.sql.table_arg.TableArg.withSinglePartition"], "Title": ["TableArg.withSinglePartition"], "Feature": ["TableArg.withSinglePartition"], "Description": "Forces the data to be processed in a single partition.", "Examples": [], "Parameters": [], "Returns": [], "Category": ["DataFrame"], "index": 12}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.columns.html#pyspark.sql.DataFrame.columns"], "Title": ["DataFrame.columns"], "Feature": ["DataFrame.columns"], "Description": "Retrieves the names of all columns in theDataFrameas a list.\nThe order of the column names in the list reflects their order in the DataFrame.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\", \"CA\"), (23, \"Alice\", \"NY\"), (16, \"Bob\", \"TX\")],\n...     [\"age\", \"name\", \"state\"]\n... )\n>>> df.columns\n['age', 'name', 'state']", ">>> selected_cols = [col for col in df.columns if col != \"age\"]\n>>> df.select(selected_cols).show()\n+-----+-----+\n| name|state|\n+-----+-----+\n|  Tom|   CA|\n|Alice|   NY|\n|  Bob|   TX|\n+-----+-----+", ">>> \"state\" in df.columns\nTrue\n>>> \"salary\" in df.columns\nFalse", ">>> import pyspark.sql.functions as f\n>>> for col_name in df.columns:\n...     df = df.withColumn(col_name, f.upper(f.col(col_name)))\n>>> df.show()\n+---+-----+-----+\n|age| name|state|\n+---+-----+-----+\n| 14|  TOM|   CA|\n| 23|ALICE|   NY|\n| 16|  BOB|   TX|\n+---+-----+-----+", ">>> df = df.withColumnRenamed(\"name\", \"first_name\")\n>>> df.columns\n['age', 'first_name', 'state']", ">>> df2 = spark.createDataFrame(\n...     [(30, \"Eve\", \"FL\"), (40, \"Sam\", \"WA\")], [\"age\", \"name\", \"location\"])\n>>> df.columns == df2.columns\nFalse"], "Parameters": [], "Returns": [["list", "List of column names in the DataFrame."]], "Category": ["DataFrame"], "index": 13}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.plot.core.PySparkPlotAccessor.area.html#pyspark.sql.plot.core.PySparkPlotAccessor.area"], "Title": ["PySparkPlotAccessor.area"], "Feature": ["PySparkPlotAccessor.area"], "Description": "Draw a stacked area plot.\nAn area plot displays quantitative data visually.", "Examples": [">>> from datetime import datetime\n>>> data = [\n...     (3, 5, 20, datetime(2018, 1, 31)),\n...     (2, 5, 42, datetime(2018, 2, 28)),\n...     (3, 6, 28, datetime(2018, 3, 31)),\n...     (9, 12, 62, datetime(2018, 4, 30))\n... ]\n>>> columns = [\"sales\", \"signups\", \"visits\", \"date\"]\n>>> df = spark.createDataFrame(data, columns)\n>>> df.plot.area(x='date', y=['sales', 'signups', 'visits'])"], "Parameters": [["x str", "Name of column to use for the horizontal axis."], ["y str or list of str", "Name(s) of the column(s) to plot."], ["**kwargs: Optional", "Additional keyword arguments."]], "Returns": [["plotly.graph_objs.Figure", ""]], "Category": ["DataFrame"], "index": 14}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.plot.core.PySparkPlotAccessor.bar.html#pyspark.sql.plot.core.PySparkPlotAccessor.bar"], "Title": ["PySparkPlotAccessor.bar"], "Feature": ["PySparkPlotAccessor.bar"], "Description": "Vertical bar plot.\nA bar plot is a plot that presents categorical data with rectangular bars with lengths\nproportional to the values that they represent. A bar plot shows comparisons among\ndiscrete categories. One axis of the plot shows the specific categories being compared,\nand the other axis represents a measured value.", "Examples": [">>> data = [(\"A\", 10, 1.5), (\"B\", 30, 2.5), (\"C\", 20, 3.5)]\n>>> columns = [\"category\", \"int_val\", \"float_val\"]\n>>> df = spark.createDataFrame(data, columns)\n>>> df.plot.bar(x=\"category\", y=\"int_val\")  \n>>> df.plot.bar(x=\"category\", y=[\"int_val\", \"float_val\"])"], "Parameters": [["x str", "Name of column to use for the horizontal axis."], ["y str or list of str", "Name(s) of the column(s) to use for the vertical axis.\nMultiple columns can be plotted."], ["**kwargs optional", "Additional keyword arguments."]], "Returns": [["plotly.graph_objs.Figure", ""]], "Category": ["DataFrame"], "index": 15}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.plot.core.PySparkPlotAccessor.barh.html#pyspark.sql.plot.core.PySparkPlotAccessor.barh"], "Title": ["PySparkPlotAccessor.barh"], "Feature": ["PySparkPlotAccessor.barh"], "Description": "Make a horizontal bar plot.\nA horizontal bar plot is a plot that presents quantitative data with\nrectangular bars with lengths proportional to the values that they\nrepresent. A bar plot shows comparisons among discrete categories. One\naxis of the plot shows the specific categories being compared, and the\nother axis represents a measured value.\nNotes\nIn Plotly and Matplotlib, the interpretation ofxandyforbarhplots differs.\nIn Plotly,xrefers to the values andyrefers to the categories.\nIn Matplotlib,xrefers to the categories andyrefers to the values.\nEnsure correct axis labeling based on the backend used.", "Examples": [">>> data = [(\"A\", 10, 1.5), (\"B\", 30, 2.5), (\"C\", 20, 3.5)]\n>>> columns = [\"category\", \"int_val\", \"float_val\"]\n>>> df = spark.createDataFrame(data, columns)\n>>> df.plot.barh(x=\"int_val\", y=\"category\")  \n>>> df.plot.barh(\n...     x=[\"int_val\", \"float_val\"], y=\"category\"\n... )"], "Parameters": [["x str or list of str", "Name(s) of the column(s) to use for the horizontal axis.\nMultiple columns can be plotted."], ["y str or list of str", "Name(s) of the column(s) to use for the vertical axis.\nMultiple columns can be plotted."], ["**kwargs optional", "Additional keyword arguments."]], "Returns": [["plotly.graph_objs.Figure", ""]], "Category": ["DataFrame"], "index": 16}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.plot.core.PySparkPlotAccessor.line.html#pyspark.sql.plot.core.PySparkPlotAccessor.line"], "Title": ["PySparkPlotAccessor.line"], "Feature": ["PySparkPlotAccessor.line"], "Description": "Plot DataFrame as lines.", "Examples": [">>> data = [(\"A\", 10, 1.5), (\"B\", 30, 2.5), (\"C\", 20, 3.5)]\n>>> columns = [\"category\", \"int_val\", \"float_val\"]\n>>> df = spark.createDataFrame(data, columns)\n>>> df.plot.line(x=\"category\", y=\"int_val\")  \n>>> df.plot.line(x=\"category\", y=[\"int_val\", \"float_val\"])"], "Parameters": [["x str", "Name of column to use for the horizontal axis."], ["y str or list of str", "Name(s) of the column(s) to use for the vertical axis. Multiple columns can be plotted."], ["**kwargs optional", "Additional keyword arguments."]], "Returns": [["plotly.graph_objs.Figure", ""]], "Category": ["DataFrame"], "index": 17}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.plot.core.PySparkPlotAccessor.pie.html#pyspark.sql.plot.core.PySparkPlotAccessor.pie"], "Title": ["PySparkPlotAccessor.pie"], "Feature": ["PySparkPlotAccessor.pie"], "Description": "Generate a pie plot.\nA pie plot is a proportional representation of the numerical data in a\ncolumn.", "Examples": [">>> from datetime import datetime\n>>> data = [\n...     (3, 5, 20, datetime(2018, 1, 31)),\n...     (2, 5, 42, datetime(2018, 2, 28)),\n...     (3, 6, 28, datetime(2018, 3, 31)),\n...     (9, 12, 62, datetime(2018, 4, 30))\n... ]\n>>> columns = [\"sales\", \"signups\", \"visits\", \"date\"]\n>>> df = spark.createDataFrame(data, columns)\n>>> df.plot.pie(x='date', y='sales')  \n>>> df.plot.pie(x='date', subplots=True)"], "Parameters": [["x str", "Name of column to be used as the category labels for the pie plot."], ["y str, optional", "Name of the column to plot. If not provided, subplots=True must be passed at kwargs ."], ["**kwargs", "Additional keyword arguments."]], "Returns": [["plotly.graph_objs.Figure", ""]], "Category": ["DataFrame"], "index": 18}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.plot.core.PySparkPlotAccessor.scatter.html#pyspark.sql.plot.core.PySparkPlotAccessor.scatter"], "Title": ["PySparkPlotAccessor.scatter"], "Feature": ["PySparkPlotAccessor.scatter"], "Description": "Create a scatter plot with varying marker point size and color.\nThe coordinates of each point are defined by two dataframe columns and\nfilled circles are used to represent each point. This kind of plot is\nuseful to see complex correlations between two variables. Points could\nbe for instance natural 2D coordinates like longitude and latitude in\na map or, in general, any pair of metrics that can be plotted against\neach other.", "Examples": [">>> data = [(5.1, 3.5, 0), (4.9, 3.0, 0), (7.0, 3.2, 1), (6.4, 3.2, 1), (5.9, 3.0, 2)]\n>>> columns = ['length', 'width', 'species']\n>>> df = spark.createDataFrame(data, columns)\n>>> df.plot.scatter(x='length', y='width')"], "Parameters": [["x str", "Name of column to use as horizontal coordinates for each point."], ["y str or list of str", "Name of column to use as vertical coordinates for each point."], ["**kwargs: Optional", "Additional keyword arguments."]], "Returns": [["plotly.graph_objs.Figure", ""]], "Category": ["DataFrame"], "index": 19}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.plot.core.PySparkPlotAccessor.box.html#pyspark.sql.plot.core.PySparkPlotAccessor.box"], "Title": ["PySparkPlotAccessor.box"], "Feature": ["PySparkPlotAccessor.box"], "Description": "Make a box plot of the DataFrame columns.\nMake a box-and-whisker plot from DataFrame columns, optionally grouped by some\nother columns. A box plot is a method for graphically depicting groups of numerical\ndata through their quartiles. The box extends from the Q1 to Q3 quartile values of\nthe data, with a line at the median (Q2). The whiskers extend from the edges of box\nto show the range of the data. By default, they extend no more than\n1.5 * IQR (IQR = Q3 - Q1) from the edges of the box, ending at the farthest data point\nwithin that interval. Outliers are plotted as separate dots.", "Examples": [">>> data = [\n...     (\"A\", 50, 55),\n...     (\"B\", 55, 60),\n...     (\"C\", 60, 65),\n...     (\"D\", 65, 70),\n...     (\"E\", 70, 75),\n...     (\"F\", 10, 15),\n...     (\"G\", 85, 90),\n...     (\"H\", 5, 150),\n... ]\n>>> columns = [\"student\", \"math_score\", \"english_score\"]\n>>> df = spark.createDataFrame(data, columns)\n>>> df.plot.box()  \n>>> df.plot.box(column=\"math_score\")  \n>>> df.plot.box(column=[\"math_score\", \"english_score\"])"], "Parameters": [["column: str or list of str, optional", "Column name or list of names to be used for creating the box plot.\nIf None (default), all numeric columns will be used."], ["**kwargs", "Extra arguments to precision : refer to a float that is used by\npyspark to compute approximate statistics for building a boxplot.\nThe default value is 0.01. Use smaller values to get more precise statistics."]], "Returns": [["plotly.graph_objs.Figure", ""]], "Category": ["DataFrame"], "index": 20}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.plot.core.PySparkPlotAccessor.kde.html#pyspark.sql.plot.core.PySparkPlotAccessor.kde"], "Title": ["PySparkPlotAccessor.kde"], "Feature": ["PySparkPlotAccessor.kde"], "Description": "Generate Kernel Density Estimate plot using Gaussian kernels.\nIn statistics, kernel density estimation (KDE) is a non-parametric way to\nestimate the probability density function (PDF) of a random variable. This\nfunction uses Gaussian kernels and includes automatic bandwidth determination.", "Examples": [">>> data = [(5.1, 3.5, 0), (4.9, 3.0, 0), (7.0, 3.2, 1), (6.4, 3.2, 1), (5.9, 3.0, 2)]\n>>> columns = [\"length\", \"width\", \"species\"]\n>>> df = spark.createDataFrame(data, columns)\n>>> df.plot.kde(bw_method=0.3, ind=100)  \n>>> df.plot.kde(column=[\"length\", \"width\"], bw_method=0.3, ind=100)  \n>>> df.plot.kde(column=\"length\", bw_method=0.3, ind=100)"], "Parameters": [["bw_method int or float", "The method used to calculate the estimator bandwidth.\nSee KernelDensity in PySpark for more information."], ["column: str or list of str, optional", "Column name or list of names to be used for creating the kde plot.\nIf None (default), all numeric columns will be used."], ["ind List of float, NumPy array or integer, optional", "Evaluation points for the estimated PDF. If None (default),\n1000 equally spaced points are used. If ind is a NumPy array, the\nKDE is evaluated at the points passed. If ind is an integer, ind number of equally spaced points are used."], ["**kwargs optional", "Additional keyword arguments."]], "Returns": [["plotly.graph_objs.Figure", ""]], "Category": ["DataFrame"], "index": 21}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.plot.core.PySparkPlotAccessor.hist.html#pyspark.sql.plot.core.PySparkPlotAccessor.hist"], "Title": ["PySparkPlotAccessor.hist"], "Feature": ["PySparkPlotAccessor.hist"], "Description": "Draw one histogram of the DataFrame’s columns.\nAhistogramis a representation of the distribution of data.", "Examples": [">>> data = [(5.1, 3.5, 0), (4.9, 3.0, 0), (7.0, 3.2, 1), (6.4, 3.2, 1), (5.9, 3.0, 2)]\n>>> columns = [\"length\", \"width\", \"species\"]\n>>> df = spark.createDataFrame(data, columns)\n>>> df.plot.hist(bins=4)  \n>>> df.plot.hist(column=[\"length\", \"width\"])  \n>>> df.plot.hist(column=\"length\", bins=4)"], "Parameters": [["column: str or list of str, optional", "Column name or list of names to be used for creating the hostogram plot.\nIf None (default), all numeric columns will be used."], ["bins integer, default 10", "Number of histogram bins to be used."], ["**kwargs", "Additional keyword arguments."]], "Returns": [["plotly.graph_objs.Figure", ""]], "Category": ["DataFrame"], "index": 22}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.__getattr__.html#pyspark.sql.Column.__getattr__"], "Title": ["Column.__getattr__"], "Feature": ["Column.__getattr__"], "Description": "An expression that gets an item at positionordinalout of a list,\nor gets an item by key out of a dict.", "Examples": [">>> df = spark.createDataFrame([('abcedfg', {\"key\": \"value\"})], [\"l\", \"d\"])\n>>> df.select(df.d.key).show()\n+------+\n|d[key]|\n+------+\n| value|\n+------+"], "Parameters": [["item", "a literal value."]], "Returns": [["Column", "Column representing the item got by key out of a dict."]], "Category": ["Column"], "index": 23}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.corr.html#pyspark.sql.DataFrame.corr"], "Title": ["DataFrame.corr"], "Feature": ["DataFrame.corr"], "Description": "Calculates the correlation of two columns of aDataFrameas a double value.\nCurrently only supports the Pearson Correlation Coefficient.DataFrame.corr()andDataFrameStatFunctions.corr()are aliases of each other.", "Examples": [">>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n>>> df.corr(\"c1\", \"c2\")\n-0.3592106040535498\n>>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n>>> df.corr(\"small\", \"bigger\")\n1.0"], "Parameters": [["col1 str", "The name of the first column"], ["col2 str", "The name of the second column"], ["method str, optional", "The correlation method. Currently only supports “pearson”"]], "Returns": [["float", "Pearson Correlation Coefficient of two columns."]], "Category": ["DataFrame"], "index": 24}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.__getitem__.html#pyspark.sql.Column.__getitem__"], "Title": ["Column.__getitem__"], "Feature": ["Column.__getitem__"], "Description": "An expression that gets an item at positionordinalout of a list,\nor gets an item by key out of a dict.", "Examples": [">>> df = spark.createDataFrame([('abcedfg', {\"key\": \"value\"})], [\"l\", \"d\"])\n>>> df.select(df.l[slice(1, 3)], df.d['key']).show()\n+---------------+------+\n|substr(l, 1, 3)|d[key]|\n+---------------+------+\n|            abc| value|\n+---------------+------+"], "Parameters": [["k", "a literal value, or a slice object without step."]], "Returns": [["Column", "Column representing the item got by key out of a dict, or substrings sliced by\nthe given slice object."]], "Category": ["Column"], "index": 25}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.alias.html#pyspark.sql.Column.alias"], "Title": ["Column.alias"], "Feature": ["Column.alias"], "Description": "Returns this column aliased with a new name or names (in the case of expressions that\nreturn more than one column, such as explode).", "Examples": [">>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> df.select(df.age.alias(\"age2\")).collect()\n[Row(age2=2), Row(age2=5)]\n>>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n99"], "Parameters": [["alias str", "desired column names (collects all positional arguments passed)"]], "Returns": [["Column", "Column representing whether each element of Column is aliased with new name or names."]], "Category": ["Column"], "index": 26}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.asc.html#pyspark.sql.Column.asc"], "Title": ["Column.asc"], "Feature": ["Column.asc"], "Description": "Returns a sort expression based on the ascending order of the column.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n>>> df.select(df.name).orderBy(df.name.asc()).collect()\n[Row(name='Alice'), Row(name='Tom')]"], "Parameters": [], "Returns": [], "Category": ["Column"], "index": 27}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.asc_nulls_first.html#pyspark.sql.Column.asc_nulls_first"], "Title": ["Column.asc_nulls_first"], "Feature": ["Column.asc_nulls_first"], "Description": "Returns a sort expression based on ascending order of the column, and null values\nreturn before non-null values.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame(\n...     [('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n>>> df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()\n[Row(name=None), Row(name='Alice'), Row(name='Tom')]"], "Parameters": [], "Returns": [], "Category": ["Column"], "index": 28}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.asc_nulls_last.html#pyspark.sql.Column.asc_nulls_last"], "Title": ["Column.asc_nulls_last"], "Feature": ["Column.asc_nulls_last"], "Description": "Returns a sort expression based on ascending order of the column, and null values\nappear after non-null values.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame(\n...     [('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n>>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n[Row(name='Alice'), Row(name='Tom'), Row(name=None)]"], "Parameters": [], "Returns": [], "Category": ["Column"], "index": 29}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.astype.html#pyspark.sql.Column.astype"], "Title": ["Column.astype"], "Feature": ["Column.astype"], "Description": "astype()is an alias forcast().", "Examples": [], "Parameters": [], "Returns": [], "Category": ["Column"], "index": 30}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.between.html#pyspark.sql.Column.between"], "Title": ["Column.between"], "Feature": ["Column.between"], "Description": "Check if the current column’s values are between the specified lower and upper\nbounds, inclusive.", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> df.select(df.name, df.age.between(2, 4)).show()\n+-----+---------------------------+\n| name|((age >= 2) AND (age <= 4))|\n+-----+---------------------------+\n|Alice|                       true|\n|  Bob|                      false|\n+-----+---------------------------+", ">>> df = spark.createDataFrame([(\"Alice\", \"A\"), (\"Bob\", \"B\")], [\"name\", \"initial\"])\n>>> df.select(df.name, df.initial.between(\"A\", \"B\")).show()\n+-----+-----------------------------------+\n| name|((initial >= A) AND (initial <= B))|\n+-----+-----------------------------------+\n|Alice|                               true|\n|  Bob|                               true|\n+-----+-----------------------------------+", ">>> df = spark.createDataFrame(\n...     [(2.5, \"Alice\"), (5.5, \"Bob\")], [\"height\", \"name\"])\n>>> df.select(df.name, df.height.between(2.0, 5.0)).show()\n+-----+-------------------------------------+\n| name|((height >= 2.0) AND (height <= 5.0))|\n+-----+-------------------------------------+\n|Alice|                                 true|\n|  Bob|                                false|\n+-----+-------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame(\n...     [(\"Alice\", \"2023-01-01\"), (\"Bob\", \"2023-02-01\")], [\"name\", \"date\"])\n>>> df = df.withColumn(\"date\", sf.to_date(df.date))\n>>> df.select(df.name, df.date.between(\"2023-01-01\", \"2023-01-15\")).show()\n+-----+-----------------------------------------------+\n| name|((date >= 2023-01-01) AND (date <= 2023-01-15))|\n+-----+-----------------------------------------------+\n|Alice|                                           true|\n|  Bob|                                          false|\n+-----+-----------------------------------------------+\n>>> from datetime import date\n>>> df.select(df.name, df.date.between(date(2023, 1, 1), date(2023, 1, 15))).show()\n+-----+-------------------------------------------------------------+\n| name|((date >= DATE '2023-01-01') AND (date <= DATE '2023-01-15'))|\n+-----+-------------------------------------------------------------+\n|Alice|                                                         true|\n|  Bob|                                                        false|\n+-----+-------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame(\n...     [(\"Alice\", \"2023-01-01 10:00:00\"), (\"Bob\", \"2023-02-01 10:00:00\")],\n...     schema=[\"name\", \"timestamp\"])\n>>> df = df.withColumn(\"timestamp\", sf.to_timestamp(df.timestamp))\n>>> df.select(df.name, df.timestamp.between(\"2023-01-01\", \"2023-02-01\")).show()\n+-----+---------------------------------------------------------+\n| name|((timestamp >= 2023-01-01) AND (timestamp <= 2023-02-01))|\n+-----+---------------------------------------------------------+\n|Alice|                                                     true|\n|  Bob|                                                    false|\n+-----+---------------------------------------------------------+\n>>> df.select(df.name, df.timestamp.between(\"2023-01-01\", \"2023-02-01 12:00:00\")).show()\n+-----+------------------------------------------------------------------+\n| name|((timestamp >= 2023-01-01) AND (timestamp <= 2023-02-01 12:00:00))|\n+-----+------------------------------------------------------------------+\n|Alice|                                                              true|\n|  Bob|                                                              true|\n+-----+------------------------------------------------------------------+"], "Parameters": [["lowerBound Column , int, float, string, bool, datetime, date or Decimal", "The lower boundary value, inclusive."], ["upperBound Column , int, float, string, bool, datetime, date or Decimal", "The upper boundary value, inclusive."]], "Returns": [["Column", "A new column of boolean values indicating whether each element in the original\ncolumn is within the specified range (inclusive)."]], "Category": ["Column"], "index": 31}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.bitwiseAND.html#pyspark.sql.Column.bitwiseAND"], "Title": ["Column.bitwiseAND"], "Feature": ["Column.bitwiseAND"], "Description": "Compute bitwise AND of this expression with another expression.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([Row(a=170, b=75)])\n>>> df.select(df.a.bitwiseAND(df.b)).collect()\n[Row((a & b)=10)]"], "Parameters": [["other", "a value or Column to calculate bitwise and(&) with\nthis Column ."]], "Returns": [], "Category": ["Column"], "index": 32}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.bitwiseOR.html#pyspark.sql.Column.bitwiseOR"], "Title": ["Column.bitwiseOR"], "Feature": ["Column.bitwiseOR"], "Description": "”\nCompute bitwise OR of this expression with another expression.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([Row(a=170, b=75)])\n>>> df.select(df.a.bitwiseOR(df.b)).collect()\n[Row((a | b)=235)]"], "Parameters": [["other", "a value or Column to calculate bitwise or(|) with\nthis Column ."]], "Returns": [], "Category": ["Column"], "index": 33}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.bitwiseXOR.html#pyspark.sql.Column.bitwiseXOR"], "Title": ["Column.bitwiseXOR"], "Feature": ["Column.bitwiseXOR"], "Description": "Compute bitwise XOR of this expression with another expression.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([Row(a=170, b=75)])\n>>> df.select(df.a.bitwiseXOR(df.b)).collect()\n[Row((a ^ b)=225)]"], "Parameters": [["other", "a value or Column to calculate bitwise xor(^) with\nthis Column ."]], "Returns": [], "Category": ["Column"], "index": 34}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.count.html#pyspark.sql.DataFrame.count"], "Title": ["DataFrame.count"], "Feature": ["DataFrame.count"], "Description": "Returns the number of rows in thisDataFrame.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])", ">>> df.count()\n3"], "Parameters": [], "Returns": [["int", "Number of rows."]], "Category": ["DataFrame"], "index": 35}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.cast.html#pyspark.sql.Column.cast"], "Title": ["Column.cast"], "Feature": ["Column.cast"], "Description": "Casts the column into typedataType.", "Examples": [">>> from pyspark.sql.types import StringType\n>>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n[Row(ages='2'), Row(ages='5')]\n>>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n[Row(ages='2'), Row(ages='5')]"], "Parameters": [["dataType DataType or str", "a DataType or Python string literal with a DDL-formatted string\nto use when parsing the column to the same type."]], "Returns": [["Column", "Column representing whether each element of Column is cast into new type."]], "Category": ["Column"], "index": 36}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.contains.html#pyspark.sql.Column.contains"], "Title": ["Column.contains"], "Feature": ["Column.contains"], "Description": "Contains the other element. Returns a booleanColumnbased on a string match.", "Examples": [">>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> df.filter(df.name.contains('o')).collect()\n[Row(age=5, name='Bob')]"], "Parameters": [["other", "string in line. A value as a literal or a Column ."]], "Returns": [], "Category": ["Column"], "index": 37}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.desc.html#pyspark.sql.Column.desc"], "Title": ["Column.desc"], "Feature": ["Column.desc"], "Description": "Returns a sort expression based on the descending order of the column.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n>>> df.select(df.name).orderBy(df.name.desc()).collect()\n[Row(name='Tom'), Row(name='Alice')]"], "Parameters": [], "Returns": [], "Category": ["Column"], "index": 38}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.desc_nulls_first.html#pyspark.sql.Column.desc_nulls_first"], "Title": ["Column.desc_nulls_first"], "Feature": ["Column.desc_nulls_first"], "Description": "Returns a sort expression based on the descending order of the column, and null values\nappear before non-null values.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame(\n...     [('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n>>> df.select(df.name).orderBy(df.name.desc_nulls_first()).collect()\n[Row(name=None), Row(name='Tom'), Row(name='Alice')]"], "Parameters": [], "Returns": [], "Category": ["Column"], "index": 39}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.desc_nulls_last.html#pyspark.sql.Column.desc_nulls_last"], "Title": ["Column.desc_nulls_last"], "Feature": ["Column.desc_nulls_last"], "Description": "Returns a sort expression based on the descending order of the column, and null values\nappear after non-null values.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame(\n...     [('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n>>> df.select(df.name).orderBy(df.name.desc_nulls_last()).collect()\n[Row(name='Tom'), Row(name='Alice'), Row(name=None)]"], "Parameters": [], "Returns": [], "Category": ["Column"], "index": 40}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.dropFields.html#pyspark.sql.Column.dropFields"], "Title": ["Column.dropFields"], "Feature": ["Column.dropFields"], "Description": "An expression that drops fields inStructTypeby name.\nThis is a no-op if the schema doesn’t contain field name(s).", "Examples": [">>> from pyspark.sql import Row\n>>> from pyspark.sql.functions import col, lit\n>>> df = spark.createDataFrame([\n...     Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n>>> df.withColumn('a', df['a'].dropFields('b')).show()\n+-----------------+\n|                a|\n+-----------------+\n|{2, 3, {4, 5, 6}}|\n+-----------------+", ">>> df.withColumn('a', df['a'].dropFields('b', 'c')).show()\n+--------------+\n|             a|\n+--------------+\n|{3, {4, 5, 6}}|\n+--------------+", ">>> df.withColumn(\"a\", col(\"a\").dropFields(\"e.g\", \"e.h\")).show()\n+--------------+\n|             a|\n+--------------+\n|{1, 2, 3, {4}}|\n+--------------+", ">>> df.select(col(\"a\").withField(\n...     \"e\", col(\"a.e\").dropFields(\"g\", \"h\")).alias(\"a\")\n... ).show()\n+--------------+\n|             a|\n+--------------+\n|{1, 2, 3, {4}}|\n+--------------+"], "Parameters": [["fieldNames str", "Desired field names (collects all positional arguments passed)\nThe result will drop at a location if any field matches in the Column."]], "Returns": [["Column", "Column representing whether each element of Column with field dropped by fieldName."]], "Category": ["Column"], "index": 41}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.endswith.html#pyspark.sql.Column.endswith"], "Title": ["Column.endswith"], "Feature": ["Column.endswith"], "Description": "String ends with. Returns a booleanColumnbased on a string match.", "Examples": [">>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> df.filter(df.name.endswith('ice')).collect()\n[Row(age=2, name='Alice')]\n>>> df.filter(df.name.endswith('ice$')).collect()\n[]"], "Parameters": [["other Column or str", "string at end of line (do not use a regex $ )"]], "Returns": [], "Category": ["Column"], "index": 42}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.eqNullSafe.html#pyspark.sql.Column.eqNullSafe"], "Title": ["Column.eqNullSafe"], "Feature": ["Column.eqNullSafe"], "Description": "Equality test that is safe for null values.\nNotes\nUnlike Pandas, PySpark doesn’t consider NaN values to be NULL. See theNaN Semanticsfor details.", "Examples": [">>> from pyspark.sql import Row\n>>> df1 = spark.createDataFrame([\n...     Row(id=1, value='foo'),\n...     Row(id=2, value=None)\n... ])\n>>> df1.select(\n...     df1['value'] == 'foo',\n...     df1['value'].eqNullSafe('foo'),\n...     df1['value'].eqNullSafe(None)\n... ).show()\n+-------------+---------------+----------------+\n|(value = foo)|(value <=> foo)|(value <=> NULL)|\n+-------------+---------------+----------------+\n|         true|           true|           false|\n|         NULL|          false|            true|\n+-------------+---------------+----------------+\n>>> df2 = spark.createDataFrame([\n...     Row(value = 'bar'),\n...     Row(value = None)\n... ])\n>>> df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n0\n>>> df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n1\n>>> df2 = spark.createDataFrame([\n...     Row(id=1, value=float('NaN')),\n...     Row(id=2, value=42.0),\n...     Row(id=3, value=None)\n... ])\n>>> df2.select(\n...     df2['value'].eqNullSafe(None),\n...     df2['value'].eqNullSafe(float('NaN')),\n...     df2['value'].eqNullSafe(42.0)\n... ).show()\n+----------------+---------------+----------------+\n|(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n+----------------+---------------+----------------+\n|           false|           true|           false|\n|           false|          false|            true|\n|            true|          false|           false|\n+----------------+---------------+----------------+"], "Parameters": [["other", "a value or Column"]], "Returns": [], "Category": ["Column"], "index": 43}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.getField.html#pyspark.sql.Column.getField"], "Title": ["Column.getField"], "Feature": ["Column.getField"], "Description": "An expression that gets a field by name in aStructType.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n>>> df.select(df.r.getField(\"b\")).show()\n+---+\n|r.b|\n+---+\n|  b|\n+---+\n>>> df.select(df.r.a).show()\n+---+\n|r.a|\n+---+\n|  1|\n+---+"], "Parameters": [["name", "a literal value, or a Column expression.\nThe result will only be true at a location if the field matches in the Column. Deprecated since version 3.0.0: Column as a parameter is deprecated."], ["Returns", ""], ["——-", ""], [":class:`Column`", "Column representing whether each element of Column got by name."]], "Returns": [], "Category": ["Column"], "index": 44}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.getItem.html#pyspark.sql.Column.getItem"], "Title": ["Column.getItem"], "Feature": ["Column.getItem"], "Description": "An expression that gets an item at positionordinalout of a list,\nor gets an item by key out of a dict.", "Examples": [">>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n>>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n+----+------+\n|l[0]|d[key]|\n+----+------+\n|   1| value|\n+----+------+"], "Parameters": [["key", "a literal value, or a Column expression.\nThe result will only be true at a location if the item matches in the column. Deprecated since version 3.0.0: Column as a parameter is deprecated."]], "Returns": [["Column", "Column representing the item(s) got at position out of a list or by key out of a dict."]], "Category": ["Column"], "index": 45}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.cov.html#pyspark.sql.DataFrame.cov"], "Title": ["DataFrame.cov"], "Feature": ["DataFrame.cov"], "Description": "Calculate the sample covariance for the given columns, specified by their names, as a\ndouble value.DataFrame.cov()andDataFrameStatFunctions.cov()are aliases.", "Examples": [">>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n>>> df.cov(\"c1\", \"c2\")\n-18.0\n>>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n>>> df.cov(\"small\", \"bigger\")\n1.0"], "Parameters": [["col1 str", "The name of the first column"], ["col2 str", "The name of the second column"]], "Returns": [["float", "Covariance of two columns."]], "Category": ["DataFrame"], "index": 46}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.ilike.html#pyspark.sql.Column.ilike"], "Title": ["Column.ilike"], "Feature": ["Column.ilike"], "Description": "SQL ILIKE expression (case insensitive LIKE). Returns a booleanColumnbased on a case insensitive match.\nSee alsopyspark.sql.Column.rlike", "Examples": [">>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> df.filter(df.name.ilike('%Ice')).collect()\n[Row(age=2, name='Alice')]"], "Parameters": [["other str", "a SQL LIKE pattern"]], "Returns": [["Column", "Column of booleans showing whether each element\nin the Column is matched by SQL LIKE pattern."]], "Category": ["Column"], "index": 47}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.isNaN.html#pyspark.sql.Column.isNaN"], "Title": ["Column.isNaN"], "Feature": ["Column.isNaN"], "Description": "True if the current expression is NaN.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame(\n...     [Row(name='Tom', height=80.0), Row(name='Alice', height=float('nan'))])\n>>> df.filter(df.height.isNaN()).collect()\n[Row(name='Alice', height=nan)]"], "Parameters": [], "Returns": [], "Category": ["Column"], "index": 48}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.isNotNull.html#pyspark.sql.Column.isNotNull"], "Title": ["Column.isNotNull"], "Feature": ["Column.isNotNull"], "Description": "True if the current expression is NOT null.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n>>> df.filter(df.height.isNotNull()).collect()\n[Row(name='Tom', height=80)]"], "Parameters": [], "Returns": [], "Category": ["Column"], "index": 49}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.isNull.html#pyspark.sql.Column.isNull"], "Title": ["Column.isNull"], "Feature": ["Column.isNull"], "Description": "True if the current expression is null.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n>>> df.filter(df.height.isNull()).collect()\n[Row(name='Alice', height=None)]"], "Parameters": [], "Returns": [], "Category": ["Column"], "index": 50}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.isin.html#pyspark.sql.Column.isin"], "Title": ["Column.isin"], "Feature": ["Column.isin"], "Description": "A boolean expression that is evaluated to true if the value of this\nexpression is contained by the evaluated values of the arguments.", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\"), (8, \"Mike\")], [\"age\", \"name\"])", ">>> df[df.name.isin(\"Bob\", \"Mike\")].show()\n+---+----+\n|age|name|\n+---+----+\n|  5| Bob|\n|  8|Mike|\n+---+----+", ">>> df[df.age.isin([1, 2, 3])].show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n+---+-----+", ">>> df[~df.name.isin(\"Alice\", \"Bob\")].show()\n+---+----+\n|age|name|\n+---+----+\n|  8|Mike|\n+---+----+"], "Parameters": [["cols Any", "The values to compare with the column values. The result will only be true at a location\nif any value matches in the Column."]], "Returns": [["Column", "Column of booleans showing whether each element in the Column is contained in cols."]], "Category": ["Column"], "index": 51}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.name.html#pyspark.sql.Column.name"], "Title": ["Column.name"], "Feature": ["Column.name"], "Description": "name()is an alias foralias().", "Examples": [], "Parameters": [], "Returns": [], "Category": ["Column"], "index": 52}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.otherwise.html#pyspark.sql.Column.otherwise"], "Title": ["Column.otherwise"], "Feature": ["Column.otherwise"], "Description": "Evaluates a list of conditions and returns one of multiple possible result expressions.\nIfColumn.otherwise()is not invoked, None is returned for unmatched conditions.\nSee alsopyspark.sql.functions.when", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> df.select(df.name, sf.when(df.age > 3, 1).otherwise(0)).show()\n+-----+-------------------------------------+\n| name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n+-----+-------------------------------------+\n|Alice|                                    0|\n|  Bob|                                    1|\n+-----+-------------------------------------+"], "Parameters": [["value", "a literal value, or a Column expression."]], "Returns": [["Column", "Column representing whether each element of Column is unmatched conditions."]], "Category": ["Column"], "index": 53}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.outer.html#pyspark.sql.Column.outer"], "Title": ["Column.outer"], "Feature": ["Column.outer"], "Description": "Mark this column as an outer column if its expression refers to columns from an outer query.\nThis is used to trigger lazy analysis of Spark Classic DataFrame, so that we can use it\nto build subquery expressions. Spark Connect DataFrame is always lazily analyzed and\ndoes not need to use this function.\nSee alsopyspark.sql.dataframe.DataFrame.scalarpyspark.sql.dataframe.DataFrame.exists", "Examples": [], "Parameters": [], "Returns": [], "Category": ["Column"], "index": 54}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.over.html#pyspark.sql.Column.over"], "Title": ["Column.over"], "Feature": ["Column.over"], "Description": "Define a windowing column.", "Examples": [">>> from pyspark.sql import Window\n>>> window = (\n...     Window.partitionBy(\"name\")\n...     .orderBy(\"age\")\n...     .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n... )\n>>> from pyspark.sql.functions import rank, min, desc\n>>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> df.withColumn(\n...      \"rank\", rank().over(window)\n... ).withColumn(\n...      \"min\", min('age').over(window)\n... ).sort(desc(\"age\")).show()\n+---+-----+----+---+\n|age| name|rank|min|\n+---+-----+----+---+\n|  5|  Bob|   1|  5|\n|  2|Alice|   1|  2|\n+---+-----+----+---+"], "Parameters": [["window WindowSpec", ""]], "Returns": [["Column", ""]], "Category": ["Column"], "index": 55}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.rlike.html#pyspark.sql.Column.rlike"], "Title": ["Column.rlike"], "Feature": ["Column.rlike"], "Description": "SQL RLIKE expression (LIKE with Regex). Returns a booleanColumnbased on a regex\nmatch.", "Examples": [">>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> df.filter(df.name.rlike('ice$')).collect()\n[Row(age=2, name='Alice')]"], "Parameters": [["other str", "an extended regex expression"]], "Returns": [["Column", "Column of booleans showing whether each element\nin the Column is matched by extended regex expression."]], "Category": ["Column"], "index": 56}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.createGlobalTempView.html#pyspark.sql.DataFrame.createGlobalTempView"], "Title": ["DataFrame.createGlobalTempView"], "Feature": ["DataFrame.createGlobalTempView"], "Description": "Creates a global temporary view with thisDataFrame.\nNotes\nThe lifetime of this temporary view is tied to this Spark application.\nthrowsTempTableAlreadyExistsException, if the view name already exists in the\ncatalog.", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.createGlobalTempView(\"people\")\n>>> df2 = spark.sql(\"SELECT * FROM global_temp.people\")\n>>> df2.show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+", ">>> df.createGlobalTempView(\"people\")  \nTraceback (most recent call last):\n...\nAnalysisException: \"Temporary table 'people' already exists;\"", ">>> spark.catalog.dropGlobalTempView(\"people\")\nTrue"], "Parameters": [["name str", "Name of the view."]], "Returns": [], "Category": ["DataFrame"], "index": 57}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.startswith.html#pyspark.sql.Column.startswith"], "Title": ["Column.startswith"], "Feature": ["Column.startswith"], "Description": "String starts with. Returns a booleanColumnbased on a string match.", "Examples": [">>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> df.filter(df.name.startswith('Al')).collect()\n[Row(age=2, name='Alice')]\n>>> df.filter(df.name.startswith('^Al')).collect()\n[]"], "Parameters": [["other Column or str", "string at start of line (do not use a regex ^ )"]], "Returns": [], "Category": ["Column"], "index": 58}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.substr.html#pyspark.sql.Column.substr"], "Title": ["Column.substr"], "Feature": ["Column.substr"], "Description": "Return aColumnwhich is a substring of the column.", "Examples": [">>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n[Row(col='Ali'), Row(col='Bob')]", ">>> df = spark.createDataFrame(\n...      [(3, 4, \"Alice\"), (2, 3, \"Bob\")], [\"sidx\", \"eidx\", \"name\"])\n>>> df.select(df.name.substr(df.sidx, df.eidx).alias(\"col\")).collect()\n[Row(col='ice'), Row(col='ob')]"], "Parameters": [["startPos Column or int", "start position"], ["length Column or int", "length of the substring"]], "Returns": [["Column", "Column representing whether each element of Column is substr of origin Column."]], "Category": ["Column"], "index": 59}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.try_cast.html#pyspark.sql.Column.try_cast"], "Title": ["Column.try_cast"], "Feature": ["Column.try_cast"], "Description": "This is a special version ofcastthat performs the same operation, but returns a NULL\nvalue instead of raising an error if the invoke method throws exception.", "Examples": [">>> from pyspark.sql.types import LongType\n>>> df = spark.createDataFrame(\n...      [(2, \"123\"), (5, \"Bob\"), (3, None)], [\"age\", \"name\"])\n>>> df.select(df.name.try_cast(LongType())).show()\n+----+\n|name|\n+----+\n| 123|\n|NULL|\n|NULL|\n+----+", ">>> df = spark.createDataFrame(\n...      [(2, \"123\"), (5, \"Bob\"), (3, None)], [\"age\", \"name\"])\n>>> df.select(df.name.try_cast(\"double\")).show()\n+-----+\n| name|\n+-----+\n|123.0|\n| NULL|\n| NULL|\n+-----+"], "Parameters": [["dataType DataType or str", "a DataType or Python string literal with a DDL-formatted string\nto use when parsing the column to the same type."]], "Returns": [["Column", "Column representing whether each element of Column is cast into new type."]], "Category": ["Column"], "index": 60}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.when.html#pyspark.sql.Column.when"], "Title": ["Column.when"], "Feature": ["Column.when"], "Description": "Evaluates a list of conditions and returns one of multiple possible result expressions.\nIfColumn.otherwise()is not invoked, None is returned for unmatched conditions.\nSee alsopyspark.sql.functions.when", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> result = df.select(df.name, sf.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0))\n>>> result.show()\n+-----+------------------------------------------------------------+\n| name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n+-----+------------------------------------------------------------+\n|Alice|                                                          -1|\n|  Bob|                                                           1|\n+-----+------------------------------------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, \"Alice\"), (4, \"Bob\"), (6, \"Charlie\")], [\"age\", \"name\"])\n>>> result = df.select(\n...     df.name,\n...     sf.when(df.age < 3, \"Young\").when(df.age < 5, \"Middle-aged\").otherwise(\"Old\")\n... )\n>>> result.show()\n+-------+---------------------------------------------------------------------------+\n|   name|CASE WHEN (age < 3) THEN Young WHEN (age < 5) THEN Middle-aged ELSE Old END|\n+-------+---------------------------------------------------------------------------+\n|  Alice|                                                                      Young|\n|    Bob|                                                                Middle-aged|\n|Charlie|                                                                        Old|\n+-------+---------------------------------------------------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> result = df.select(\n...     df.name, sf.when(sf.lit(True), 1).otherwise(\n...         sf.raise_error(\"unreachable\")).alias(\"when\"))\n>>> result.show()\n+-----+----+\n| name|when|\n+-----+----+\n|Alice|   1|\n|  Bob|   1|\n+-----+----+"], "Parameters": [["condition Column", "a boolean Column expression."], ["value", "a literal value, or a Column expression."]], "Returns": [["Column", "Column representing whether each element of Column is in conditions."]], "Category": ["Column"], "index": 61}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.withField.html#pyspark.sql.Column.withField"], "Title": ["Column.withField"], "Feature": ["Column.withField"], "Description": "An expression that adds/replaces a field inStructTypeby name.", "Examples": [">>> from pyspark.sql import Row\n>>> from pyspark.sql.functions import lit\n>>> df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n>>> df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n+---+\n|  b|\n+---+\n|  3|\n+---+\n>>> df.withColumn('a', df['a'].withField('d', lit(4))).select('a.d').show()\n+---+\n|  d|\n+---+\n|  4|\n+---+"], "Parameters": [["fieldName str", "a literal value.\nThe result will only be true at a location if any field matches in the Column."], ["col Column", "A Column expression for the column with fieldName ."]], "Returns": [["Column", "Column representing whether each element of Column\nwhich field was added/replaced by fieldName."]], "Category": ["Column"], "index": 62}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.broadcast.html#pyspark.sql.functions.broadcast"], "Title": ["broadcast"], "Feature": ["broadcast"], "Description": "Marks a DataFrame as small enough for use in broadcast joins.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([1, 2, 3, 3, 4], \"int\")\n>>> df_small = spark.range(3)\n>>> df_b = sf.broadcast(df_small)\n>>> df.join(df_b, df.value == df_small.id).show()\n+-----+---+\n|value| id|\n+-----+---+\n|    1|  1|\n|    2|  2|\n+-----+---+"], "Parameters": [], "Returns": [["DataFrame", "DataFrame marked as ready for broadcast join."]], "Category": ["Functions"], "index": 63}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.call_function.html#pyspark.sql.functions.call_function"], "Title": ["call_function"], "Feature": ["call_function"], "Description": "Call a SQL function.", "Examples": [">>> from pyspark.sql.functions import call_udf, col\n>>> from pyspark.sql.types import IntegerType, StringType\n>>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n>>> _ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n>>> df.select(call_function(\"intX2\", \"id\")).show()\n+---------+\n|intX2(id)|\n+---------+\n|        2|\n|        4|\n|        6|\n+---------+\n>>> _ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n>>> df.select(call_function(\"strX2\", col(\"name\"))).show()\n+-----------+\n|strX2(name)|\n+-----------+\n|         aa|\n|         bb|\n|         cc|\n+-----------+\n>>> df.select(call_function(\"avg\", col(\"id\"))).show()\n+-------+\n|avg(id)|\n+-------+\n|    2.0|\n+-------+\n>>> _ = spark.sql(\"CREATE FUNCTION custom_avg AS 'test.org.apache.spark.sql.MyDoubleAvg'\")\n... \n>>> df.select(call_function(\"custom_avg\", col(\"id\"))).show()\n... \n+------------------------------------+\n|spark_catalog.default.custom_avg(id)|\n+------------------------------------+\n|                               102.0|\n+------------------------------------+\n>>> df.select(call_function(\"spark_catalog.default.custom_avg\", col(\"id\"))).show()\n... \n+------------------------------------+\n|spark_catalog.default.custom_avg(id)|\n+------------------------------------+\n|                               102.0|\n+------------------------------------+"], "Parameters": [["funcName str", "function name that follows the SQL identifier syntax (can be quoted, can be qualified)"], ["cols Column or str", "column names or Column s to be used in the function"]], "Returns": [["Column", "result of executed function."]], "Category": ["Functions"], "index": 64}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.col.html#pyspark.sql.functions.col"], "Title": ["col"], "Feature": ["col"], "Description": "Returns aColumnbased on the given column name.", "Examples": [">>> col('x')\nColumn<'x'>\n>>> column('x')\nColumn<'x'>"], "Parameters": [["col column name", "the name for the column"]], "Returns": [["Column", "the corresponding column instance."]], "Category": ["Functions"], "index": 65}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.column.html#pyspark.sql.functions.column"], "Title": ["column"], "Feature": ["column"], "Description": "Returns aColumnbased on the given column name.", "Examples": [">>> col('x')\nColumn<'x'>\n>>> column('x')\nColumn<'x'>"], "Parameters": [["col column name", "the name for the column"]], "Returns": [["Column", "the corresponding column instance."]], "Category": ["Functions"], "index": 66}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.lit.html#pyspark.sql.functions.lit"], "Title": ["lit"], "Feature": ["lit"], "Description": "Creates aColumnof literal value.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.range(1)\n>>> df.select(sf.lit(5).alias('height'), df.id).show()\n+------+---+\n|height| id|\n+------+---+\n|     5|  0|\n+------+---+", ">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.lit([1, 2, 3])).show()\n+--------------+\n|array(1, 2, 3)|\n+--------------+\n|     [1, 2, 3]|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.range(1)\n>>> df.select(sf.lit(\"PySpark\").alias('framework'), df.id).show()\n+---------+---+\n|framework| id|\n+---------+---+\n|  PySpark|  0|\n+---------+---+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(True, \"Yes\"), (False, \"No\")], [\"flag\", \"response\"])\n>>> df.select(sf.lit(False).alias('is_approved'), df.response).show()\n+-----------+--------+\n|is_approved|response|\n+-----------+--------+\n|      false|     Yes|\n|      false|      No|\n+-----------+--------+", ">>> from pyspark.sql import functions as sf\n>>> import numpy as np \n>>> spark.range(1).select(\n...     sf.lit(np.bool_(True)),\n...     sf.lit(np.int64(123)),\n...     sf.lit(np.float64(0.456)),\n...     sf.lit(np.str_(\"xyz\"))\n... ).show() \n+----+---+-----+---+\n|true|123|0.456|xyz|\n+----+---+-----+---+\n|true|123|0.456|xyz|\n+----+---+-----+---+", ">>> from pyspark.sql import functions as sf\n>>> import numpy as np \n>>> spark.range(1).select(\n...     sf.lit(np.array([True, False], np.bool_)),\n...     sf.lit(np.array([], np.int8)),\n...     sf.lit(np.array([1.5, 0.1], np.float64)),\n...     sf.lit(np.array([\"a\", \"b\", \"c\"], np.str_)),\n... ).show() \n+------------------+-------+-----------------+--------------------+\n|ARRAY(true, false)|ARRAY()|ARRAY(1.5D, 0.1D)|ARRAY('a', 'b', 'c')|\n+------------------+-------+-----------------+--------------------+\n|     [true, false]|     []|       [1.5, 0.1]|           [a, b, c]|\n+------------------+-------+-----------------+--------------------+"], "Parameters": [["col Column , str, int, float, bool or list, NumPy literals or ndarray.", "the value to make it as a PySpark literal. If a column is passed,\nit returns the column as is. Changed in version 3.4.0: Since 3.4.0, it supports the list type."]], "Returns": [["Column", "the literal instance."]], "Category": ["Functions"], "index": 67}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.createOrReplaceGlobalTempView.html#pyspark.sql.DataFrame.createOrReplaceGlobalTempView"], "Title": ["DataFrame.createOrReplaceGlobalTempView"], "Feature": ["DataFrame.createOrReplaceGlobalTempView"], "Description": "Creates or replaces a global temporary view using the given name.\nThe lifetime of this temporary view is tied to this Spark application..", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.createOrReplaceGlobalTempView(\"people\")", ">>> df2 = df.filter(df.age > 3)\n>>> df2.createOrReplaceGlobalTempView(\"people\")\n>>> df3 = spark.table(\"global_temp.people\")\n>>> sorted(df3.collect()) == sorted(df2.collect())\nTrue"], "Parameters": [["name str", "Name of the view."]], "Returns": [], "Category": ["DataFrame"], "index": 68}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.expr.html#pyspark.sql.functions.expr"], "Title": ["expr"], "Feature": ["expr"], "Description": "Parses the expression string into the column that it represents", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[\"Alice\"], [\"Bob\"]], [\"name\"])\n>>> df.select(\"*\", sf.expr(\"length(name)\")).show()\n+-----+------------+\n| name|length(name)|\n+-----+------------+\n|Alice|           5|\n|  Bob|           3|\n+-----+------------+"], "Parameters": [["str expression string", "expression defined in string."]], "Returns": [["Column", "column representing the expression."]], "Category": ["Functions"], "index": 69}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.coalesce.html#pyspark.sql.functions.coalesce"], "Title": ["coalesce"], "Feature": ["coalesce"], "Description": "Returns the first column that is not null.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n>>> df.show()\n+----+----+\n|   a|   b|\n+----+----+\n|NULL|NULL|\n|   1|NULL|\n|NULL|   2|\n+----+----+", ">>> df.select('*', sf.coalesce(\"a\", df[\"b\"])).show()\n+----+----+--------------+\n|   a|   b|coalesce(a, b)|\n+----+----+--------------+\n|NULL|NULL|          NULL|\n|   1|NULL|             1|\n|NULL|   2|             2|\n+----+----+--------------+", ">>> df.select('*', sf.coalesce(df[\"a\"], lit(0.0))).show()\n+----+----+----------------+\n|   a|   b|coalesce(a, 0.0)|\n+----+----+----------------+\n|NULL|NULL|             0.0|\n|   1|NULL|             1.0|\n|NULL|   2|             0.0|\n+----+----+----------------+"], "Parameters": [["cols Column or column name", "list of columns to work on."]], "Returns": [["Column", "value of the first column that is not null."]], "Category": ["Functions"], "index": 70}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ifnull.html#pyspark.sql.functions.ifnull"], "Title": ["ifnull"], "Feature": ["ifnull"], "Description": "Returnscol2ifcol1is null, orcol1otherwise.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(None,), (1,)], [\"e\"])\n>>> df.select(sf.ifnull(df.e, sf.lit(8))).show()\n+------------+\n|ifnull(e, 8)|\n+------------+\n|           8|\n|           1|\n+------------+"], "Parameters": [["col1 Column or str", ""], ["col2 Column or str", ""]], "Returns": [], "Category": ["Functions"], "index": 71}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.nanvl.html#pyspark.sql.functions.nanvl"], "Title": ["nanvl"], "Feature": ["nanvl"], "Description": "Returns col1 if it is not NaN, or col2 if col1 is NaN.\nBoth inputs should be floating point columns (DoubleTypeorFloatType).", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n>>> df.select(\"*\", sf.nanvl(\"a\", \"b\"), sf.nanvl(df.a, df.b)).show()\n+---+---+-----------+-----------+\n|  a|  b|nanvl(a, b)|nanvl(a, b)|\n+---+---+-----------+-----------+\n|1.0|NaN|        1.0|        1.0|\n|NaN|2.0|        2.0|        2.0|\n+---+---+-----------+-----------+"], "Parameters": [["col1 Column or column name", "first column to check."], ["col2 Column or column name", "second column to return if first is NaN."]], "Returns": [["Column", "value from first column or second if first is NaN ."]], "Category": ["Functions"], "index": 72}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.nullif.html#pyspark.sql.functions.nullif"], "Title": ["nullif"], "Feature": ["nullif"], "Description": "Returns null ifcol1equals tocol2, orcol1otherwise.", "Examples": [">>> df = spark.createDataFrame([(None, None,), (1, 9,)], [\"a\", \"b\"])\n>>> df.select(nullif(df.a, df.b).alias('r')).collect()\n[Row(r=None), Row(r=1)]"], "Parameters": [["col1 Column or str", ""], ["col2 Column or str", ""]], "Returns": [], "Category": ["Functions"], "index": 73}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.nullifzero.html#pyspark.sql.functions.nullifzero"], "Title": ["nullifzero"], "Feature": ["nullifzero"], "Description": "Returns null ifcolis equal to zero, orcolotherwise.", "Examples": [">>> df = spark.createDataFrame([(0,), (1,)], [\"a\"])\n>>> df.select(nullifzero(df.a).alias(\"result\")).show()\n+------+\n|result|\n+------+\n|  NULL|\n|     1|\n+------+"], "Parameters": [["col Column or str", ""]], "Returns": [], "Category": ["Functions"], "index": 74}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.nvl.html#pyspark.sql.functions.nvl"], "Title": ["nvl"], "Feature": ["nvl"], "Description": "Returnscol2ifcol1is null, orcol1otherwise.", "Examples": [">>> df = spark.createDataFrame([(None, 8,), (1, 9,)], [\"a\", \"b\"])\n>>> df.select(nvl(df.a, df.b).alias('r')).collect()\n[Row(r=8), Row(r=1)]"], "Parameters": [["col1 Column or str", ""], ["col2 Column or str", ""]], "Returns": [], "Category": ["Functions"], "index": 75}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.nvl2.html#pyspark.sql.functions.nvl2"], "Title": ["nvl2"], "Feature": ["nvl2"], "Description": "Returnscol2ifcol1is not null, orcol3otherwise.", "Examples": [">>> df = spark.createDataFrame([(None, 8, 6,), (1, 9, 9,)], [\"a\", \"b\", \"c\"])\n>>> df.select(nvl2(df.a, df.b, df.c).alias('r')).collect()\n[Row(r=6), Row(r=9)]"], "Parameters": [["col1 Column or str", ""], ["col2 Column or str", ""], ["col3 Column or str", ""]], "Returns": [], "Category": ["Functions"], "index": 76}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.zeroifnull.html#pyspark.sql.functions.zeroifnull"], "Title": ["zeroifnull"], "Feature": ["zeroifnull"], "Description": "Returns zero ifcolis null, orcolotherwise.", "Examples": [">>> df = spark.createDataFrame([(None,), (1,)], [\"a\"])\n>>> df.select(zeroifnull(df.a).alias(\"result\")).show()\n+------+\n|result|\n+------+\n|     0|\n|     1|\n+------+"], "Parameters": [["col Column or str", ""]], "Returns": [], "Category": ["Functions"], "index": 77}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.equal_null.html#pyspark.sql.functions.equal_null"], "Title": ["equal_null"], "Feature": ["equal_null"], "Description": "Returns same result as the EQUAL(=) operator for non-null operands,\nbut returns true if both are null, false if one of them is null.", "Examples": [">>> df = spark.createDataFrame([(None, None,), (1, 9,)], [\"a\", \"b\"])\n>>> df.select(equal_null(df.a, df.b).alias('r')).collect()\n[Row(r=True), Row(r=False)]"], "Parameters": [["col1 Column or str", ""], ["col2 Column or str", ""]], "Returns": [], "Category": ["Functions"], "index": 78}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.createOrReplaceTempView.html#pyspark.sql.DataFrame.createOrReplaceTempView"], "Title": ["DataFrame.createOrReplaceTempView"], "Feature": ["DataFrame.createOrReplaceTempView"], "Description": "Creates or replaces a local temporary view with thisDataFrame.\nNotes\nThe lifetime of this temporary table is tied to theSparkSessionthat was used to create thisDataFrame.", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.createOrReplaceTempView(\"people\")", ">>> df2 = df.filter(df.age > 3)\n>>> # Replace the local temporary view with the filtered DataFrame\n>>> df2.createOrReplaceTempView(\"people\")\n>>> # Query the temporary view\n>>> df3 = spark.sql(\"SELECT * FROM people\")\n>>> # Check if the DataFrames are equal\n... assert sorted(df3.collect()) == sorted(df2.collect())", ">>> # Drop the local temporary view\n... spark.catalog.dropTempView(\"people\")\nTrue"], "Parameters": [["name str", "Name of the view."]], "Returns": [], "Category": ["DataFrame"], "index": 79}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ilike.html#pyspark.sql.functions.ilike"], "Title": ["ilike"], "Feature": ["ilike"], "Description": "Returns true if str matchespatternwithescapecase-insensitively,\nnull if any arguments are null, false otherwise.\nThe default escape character is the ‘’.", "Examples": [">>> df = spark.createDataFrame([(\"Spark\", \"_park\")], ['a', 'b'])\n>>> df.select(ilike(df.a, df.b).alias('r')).collect()\n[Row(r=True)]", ">>> df = spark.createDataFrame(\n...     [(\"%SystemDrive%/Users/John\", \"/%SystemDrive/%//Users%\")],\n...     ['a', 'b']\n... )\n>>> df.select(ilike(df.a, df.b, lit('/')).alias('r')).collect()\n[Row(r=True)]"], "Parameters": [["str Column or str", "A string."], ["pattern Column or str", "A string. The pattern is a string which is matched literally, with\nexception to the following special symbols:\n_ matches any one character in the input (similar to . in posix regular expressions)\n% matches zero or more characters in the input (similar to .* in posix regular\nexpressions)\nSince Spark 2.0, string literals are unescaped in our SQL parser. For example, in order\nto match “\u0007bc”, the pattern should be “abc”.\nWhen SQL config ‘spark.sql.parser.escapedStringLiterals’ is enabled, it falls back\nto Spark 1.6 behavior regarding string literal parsing. For example, if the config is\nenabled, the pattern to match “\u0007bc” should be “\u0007bc”."], ["escapeChar Column , optional", "An character added since Spark 3.0. The default escape character is the ‘’.\nIf an escape character precedes a special symbol or another escape character, the\nfollowing character is matched literally. It is invalid to escape any other character."]], "Returns": [], "Category": ["Functions"], "index": 80}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.isnan.html#pyspark.sql.functions.isnan"], "Title": ["isnan"], "Feature": ["isnan"], "Description": "An expression that returns true if the column is NaN.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n>>> df.select(\"*\", sf.isnan(\"a\"), sf.isnan(df.b)).show()\n+---+---+--------+--------+\n|  a|  b|isnan(a)|isnan(b)|\n+---+---+--------+--------+\n|1.0|NaN|   false|    true|\n|NaN|2.0|    true|   false|\n+---+---+--------+--------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "True if value is NaN and False otherwise."]], "Category": ["Functions"], "index": 81}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.isnotnull.html#pyspark.sql.functions.isnotnull"], "Title": ["isnotnull"], "Feature": ["isnotnull"], "Description": "Returns true ifcolis not null, or false otherwise.", "Examples": [">>> df = spark.createDataFrame([(None,), (1,)], [\"e\"])\n>>> df.select(isnotnull(df.e).alias('r')).collect()\n[Row(r=False), Row(r=True)]"], "Parameters": [["col Column or str", ""]], "Returns": [], "Category": ["Functions"], "index": 82}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.isnull.html#pyspark.sql.functions.isnull"], "Title": ["isnull"], "Feature": ["isnull"], "Description": "An expression that returns true if the column is null.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n>>> df.select(\"*\", sf.isnull(\"a\"), isnull(df.b)).show()\n+----+----+-----------+-----------+\n|   a|   b|(a IS NULL)|(b IS NULL)|\n+----+----+-----------+-----------+\n|   1|NULL|      false|       true|\n|NULL|   2|       true|      false|\n+----+----+-----------+-----------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "True if value is null and False otherwise."]], "Category": ["Functions"], "index": 83}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.like.html#pyspark.sql.functions.like"], "Title": ["like"], "Feature": ["like"], "Description": "Returns true if str matchespatternwithescape,\nnull if any arguments are null, false otherwise.\nThe default escape character is the ‘’.", "Examples": [">>> df = spark.createDataFrame([(\"Spark\", \"_park\")], ['a', 'b'])\n>>> df.select(like(df.a, df.b).alias('r')).collect()\n[Row(r=True)]", ">>> df = spark.createDataFrame(\n...     [(\"%SystemDrive%/Users/John\", \"/%SystemDrive/%//Users%\")],\n...     ['a', 'b']\n... )\n>>> df.select(like(df.a, df.b, lit('/')).alias('r')).collect()\n[Row(r=True)]"], "Parameters": [["str Column or str", "A string."], ["pattern Column or str", "A string. The pattern is a string which is matched literally, with\nexception to the following special symbols:\n_ matches any one character in the input (similar to . in posix regular expressions)\n% matches zero or more characters in the input (similar to .* in posix regular\nexpressions)\nSince Spark 2.0, string literals are unescaped in our SQL parser. For example, in order\nto match “\u0007bc”, the pattern should be “abc”.\nWhen SQL config ‘spark.sql.parser.escapedStringLiterals’ is enabled, it falls back\nto Spark 1.6 behavior regarding string literal parsing. For example, if the config is\nenabled, the pattern to match “\u0007bc” should be “\u0007bc”."], ["escapeChar Column , optional", "An character added since Spark 3.0. The default escape character is the ‘’.\nIf an escape character precedes a special symbol or another escape character, the\nfollowing character is matched literally. It is invalid to escape any other character."]], "Returns": [], "Category": ["Functions"], "index": 84}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp.html#pyspark.sql.functions.regexp"], "Title": ["regexp"], "Feature": ["regexp"], "Description": "Returns true ifstrmatches the Java regexregexp, or false otherwise.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n... ).select(sf.regexp('str', sf.lit(r'(\\d+)'))).show()\n+------------------+\n|REGEXP(str, (\\d+))|\n+------------------+\n|              true|\n+------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n... ).select(sf.regexp('str', sf.lit(r'\\d{2}b'))).show()\n+-------------------+\n|REGEXP(str, \\d{2}b)|\n+-------------------+\n|              false|\n+-------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n... ).select(sf.regexp('str', sf.col(\"regexp\"))).show()\n+-------------------+\n|REGEXP(str, regexp)|\n+-------------------+\n|               true|\n+-------------------+"], "Parameters": [["str Column or str", "target column to work on."], ["regexp Column or str", "regex pattern to apply."]], "Returns": [["Column", "true if str matches a Java regex, or false otherwise."]], "Category": ["Functions"], "index": 85}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_like.html#pyspark.sql.functions.regexp_like"], "Title": ["regexp_like"], "Feature": ["regexp_like"], "Description": "Returns true ifstrmatches the Java regexregexp, or false otherwise.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n... ).select(sf.regexp_like('str', sf.lit(r'(\\d+)'))).show()\n+-----------------------+\n|REGEXP_LIKE(str, (\\d+))|\n+-----------------------+\n|                   true|\n+-----------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n... ).select(sf.regexp_like('str', sf.lit(r'\\d{2}b'))).show()\n+------------------------+\n|REGEXP_LIKE(str, \\d{2}b)|\n+------------------------+\n|                   false|\n+------------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n... ).select(sf.regexp_like('str', sf.col(\"regexp\"))).show()\n+------------------------+\n|REGEXP_LIKE(str, regexp)|\n+------------------------+\n|                    true|\n+------------------------+"], "Parameters": [["str Column or str", "target column to work on."], ["regexp Column or str", "regex pattern to apply."]], "Returns": [["Column", "true if str matches a Java regex, or false otherwise."]], "Category": ["Functions"], "index": 86}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.rlike.html#pyspark.sql.functions.rlike"], "Title": ["rlike"], "Feature": ["rlike"], "Description": "Returns true ifstrmatches the Java regexregexp, or false otherwise.", "Examples": [">>> df = spark.createDataFrame([(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"])\n>>> df.select(rlike('str', lit(r'(\\d+)')).alias('d')).collect()\n[Row(d=True)]\n>>> df.select(rlike('str', lit(r'\\d{2}b')).alias('d')).collect()\n[Row(d=False)]\n>>> df.select(rlike(\"str\", col(\"regexp\")).alias('d')).collect()\n[Row(d=True)]"], "Parameters": [["str Column or str", "target column to work on."], ["regexp Column or str", "regex pattern to apply."]], "Returns": [["Column", "true if str matches a Java regex, or false otherwise."]], "Category": ["Functions"], "index": 87}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.asc.html#pyspark.sql.functions.asc"], "Title": ["asc"], "Feature": ["asc"], "Description": "Returns a sort expression for the target column in ascending order.\nThis function is used insortandorderByfunctions.\nSee alsopyspark.sql.functions.asc_nulls_first()pyspark.sql.functions.asc_nulls_last()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(4, 'B'), (3, 'A'), (2, 'C')], ['id', 'value'])\n>>> df.sort(sf.asc(\"id\")).show()\n+---+-----+\n| id|value|\n+---+-----+\n|  2|    C|\n|  3|    A|\n|  4|    B|\n+---+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(4, 'B'), (3, 'A'), (2, 'C')], ['id', 'value'])\n>>> df.orderBy(sf.asc(\"value\")).show()\n+---+-----+\n| id|value|\n+---+-----+\n|  3|    A|\n|  4|    B|\n|  2|    C|\n+---+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...     [(2, 'A', 4), (1, 'B', 3), (3, 'A', 2)],\n...     ['id', 'group', 'value'])\n>>> df.sort(sf.asc(\"group\"), sf.desc(\"value\")).show()\n+---+-----+-----+\n| id|group|value|\n+---+-----+-----+\n|  2|    A|    4|\n|  3|    A|    2|\n|  1|    B|    3|\n+---+-----+-----+", ">>> df = spark.createDataFrame([(4, 'B'), (3, 'A'), (2, 'C')], ['id', 'value'])\n>>> df.sort(df.id.asc()).show()\n+---+-----+\n| id|value|\n+---+-----+\n|  2|    C|\n|  3|    A|\n|  4|    B|\n+---+-----+"], "Parameters": [["col Column or column name", "Target column to sort by in the ascending order."]], "Returns": [["Column", "The column specifying the sort order."]], "Category": ["Functions"], "index": 88}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.asc_nulls_first.html#pyspark.sql.functions.asc_nulls_first"], "Title": ["asc_nulls_first"], "Feature": ["asc_nulls_first"], "Description": "Sort Function: Returns a sort expression based on the ascending order of the given\ncolumn name, and null values return before non-null values.\nSee alsopyspark.sql.functions.asc()pyspark.sql.functions.asc_nulls_last()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, \"Bob\"), (0, None), (2, \"Alice\")], [\"age\", \"name\"])\n>>> df.sort(sf.asc_nulls_first(df.name)).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  0| NULL|\n|  2|Alice|\n|  1|  Bob|\n+---+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(1, \"Bob\", None), (0, None, \"Z\"), (2, \"Alice\", \"Y\")], [\"age\", \"name\", \"grade\"])\n>>> df.sort(sf.asc_nulls_first(df.name), sf.asc_nulls_first(df.grade)).show()\n+---+-----+-----+\n|age| name|grade|\n+---+-----+-----+\n|  0| NULL|    Z|\n|  2|Alice|    Y|\n|  1|  Bob| NULL|\n+---+-----+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, \"Bob\"), (0, None), (2, \"Alice\")], [\"age\", \"name\"])\n>>> df.sort(sf.asc_nulls_first(\"name\")).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  0| NULL|\n|  2|Alice|\n|  1|  Bob|\n+---+-----+"], "Parameters": [["col Column or column name", "target column to sort by in the ascending order."]], "Returns": [["Column", "the column specifying the order."]], "Category": ["Functions"], "index": 89}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.createTempView.html#pyspark.sql.DataFrame.createTempView"], "Title": ["DataFrame.createTempView"], "Feature": ["DataFrame.createTempView"], "Description": "Creates a local temporary view with thisDataFrame.\nThe lifetime of this temporary table is tied to theSparkSessionthat was used to create thisDataFrame.\nthrowsTempTableAlreadyExistsException, if the view name already exists in the\ncatalog.", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.createTempView(\"people\")\n>>> spark.sql(\"SELECT * FROM people\").show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+", ">>> df.createTempView(\"people\")  \nTraceback (most recent call last):\n...\nAnalysisException: \"Temporary table 'people' already exists;\"", ">>> spark.catalog.dropTempView(\"people\")\nTrue\n>>> df.createTempView(\"people\")", ">>> df1 = spark.createDataFrame([(1, \"John\"), (2, \"Jane\")], schema=[\"id\", \"name\"])\n>>> df2 = spark.createDataFrame([(3, \"Jake\"), (4, \"Jill\")], schema=[\"id\", \"name\"])\n>>> df1.createTempView(\"table1\")\n>>> df2.createTempView(\"table2\")\n>>> result_df = spark.table(\"table1\").union(spark.table(\"table2\"))\n>>> result_df.show()\n+---+----+\n| id|name|\n+---+----+\n|  1|John|\n|  2|Jane|\n|  3|Jake|\n|  4|Jill|\n+---+----+"], "Parameters": [["name str", "Name of the view."]], "Returns": [], "Category": ["DataFrame"], "index": 90}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.asc_nulls_last.html#pyspark.sql.functions.asc_nulls_last"], "Title": ["asc_nulls_last"], "Feature": ["asc_nulls_last"], "Description": "Sort Function: Returns a sort expression based on the ascending order of the given\ncolumn name, and null values appear after non-null values.\nSee alsopyspark.sql.functions.asc()pyspark.sql.functions.asc_nulls_first()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(0, None), (1, \"Bob\"), (2, \"Alice\")], [\"age\", \"name\"])\n>>> df.sort(sf.asc_nulls_last(df.name)).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  1|  Bob|\n|  0| NULL|\n+---+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(0, None, \"Z\"), (1, \"Bob\", None), (2, \"Alice\", \"Y\")], [\"age\", \"name\", \"grade\"])\n>>> df.sort(sf.asc_nulls_last(df.name), sf.asc_nulls_last(df.grade)).show()\n+---+-----+-----+\n|age| name|grade|\n+---+-----+-----+\n|  2|Alice|    Y|\n|  1|  Bob| NULL|\n|  0| NULL|    Z|\n+---+-----+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(0, None), (1, \"Bob\"), (2, \"Alice\")], [\"age\", \"name\"])\n>>> df.sort(sf.asc_nulls_last(\"name\")).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  1|  Bob|\n|  0| NULL|\n+---+-----+"], "Parameters": [["col Column or column name", "target column to sort by in the ascending order."]], "Returns": [["Column", "the column specifying the order."]], "Category": ["Functions"], "index": 91}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.desc.html#pyspark.sql.functions.desc"], "Title": ["desc"], "Feature": ["desc"], "Description": "Returns a sort expression for the target column in descending order.\nThis function is used insortandorderByfunctions.\nSee alsopyspark.sql.functions.desc_nulls_first()pyspark.sql.functions.desc_nulls_last()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(4, 'B'), (3, 'A'), (2, 'C')], ['id', 'value'])\n>>> df.sort(sf.desc(\"id\")).show()\n+---+-----+\n| id|value|\n+---+-----+\n|  4|    B|\n|  3|    A|\n|  2|    C|\n+---+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(4, 'B'), (3, 'A'), (2, 'C')], ['id', 'value'])\n>>> df.orderBy(sf.desc(\"value\")).show()\n+---+-----+\n| id|value|\n+---+-----+\n|  2|    C|\n|  4|    B|\n|  3|    A|\n+---+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...     [(2, 'A', 4), (1, 'B', 3), (3, 'A', 2)],\n...     ['id', 'group', 'value'])\n>>> df.sort(sf.desc(\"group\"), sf.asc(\"value\")).show()\n+---+-----+-----+\n| id|group|value|\n+---+-----+-----+\n|  1|    B|    3|\n|  3|    A|    2|\n|  2|    A|    4|\n+---+-----+-----+", ">>> df = spark.createDataFrame([(4, 'B'), (3, 'A'), (2, 'C')], ['id', 'value'])\n>>> df.sort(df.id.desc()).show()\n+---+-----+\n| id|value|\n+---+-----+\n|  4|    B|\n|  3|    A|\n|  2|    C|\n+---+-----+"], "Parameters": [["col Column or column name", "Target column to sort by in the descending order."]], "Returns": [["Column", "The column specifying the sort order."]], "Category": ["Functions"], "index": 92}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.desc_nulls_first.html#pyspark.sql.functions.desc_nulls_first"], "Title": ["desc_nulls_first"], "Feature": ["desc_nulls_first"], "Description": "Sort Function: Returns a sort expression based on the descending order of the given\ncolumn name, and null values appear before non-null values.\nSee alsopyspark.sql.functions.desc()pyspark.sql.functions.desc_nulls_last()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, \"Bob\"), (0, None), (2, \"Alice\")], [\"age\", \"name\"])\n>>> df.sort(sf.desc_nulls_first(df.name)).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  0| NULL|\n|  1|  Bob|\n|  2|Alice|\n+---+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(1, \"Bob\", None), (0, None, \"Z\"), (2, \"Alice\", \"Y\")], [\"age\", \"name\", \"grade\"])\n>>> df.sort(sf.desc_nulls_first(df.name), sf.desc_nulls_first(df.grade)).show()\n+---+-----+-----+\n|age| name|grade|\n+---+-----+-----+\n|  0| NULL|    Z|\n|  1|  Bob| NULL|\n|  2|Alice|    Y|\n+---+-----+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, \"Bob\"), (0, None), (2, \"Alice\")], [\"age\", \"name\"])\n>>> df.sort(sf.desc_nulls_first(\"name\")).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  0| NULL|\n|  1|  Bob|\n|  2|Alice|\n+---+-----+"], "Parameters": [["col Column or column name", "target column to sort by in the descending order."]], "Returns": [["Column", "the column specifying the order."]], "Category": ["Functions"], "index": 93}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.desc_nulls_last.html#pyspark.sql.functions.desc_nulls_last"], "Title": ["desc_nulls_last"], "Feature": ["desc_nulls_last"], "Description": "Sort Function: Returns a sort expression based on the descending order of the given\ncolumn name, and null values appear after non-null values.\nSee alsopyspark.sql.functions.desc()pyspark.sql.functions.desc_nulls_first()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(0, None), (1, \"Bob\"), (2, \"Alice\")], [\"age\", \"name\"])\n>>> df.sort(sf.desc_nulls_last(df.name)).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  1|  Bob|\n|  2|Alice|\n|  0| NULL|\n+---+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(0, None, \"Z\"), (1, \"Bob\", None), (2, \"Alice\", \"Y\")], [\"age\", \"name\", \"grade\"])\n>>> df.sort(sf.desc_nulls_last(df.name), sf.desc_nulls_last(df.grade)).show()\n+---+-----+-----+\n|age| name|grade|\n+---+-----+-----+\n|  1|  Bob| NULL|\n|  2|Alice|    Y|\n|  0| NULL|    Z|\n+---+-----+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(0, None), (1, \"Bob\"), (2, \"Alice\")], [\"age\", \"name\"])\n>>> df.sort(sf.desc_nulls_last(\"name\")).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  1|  Bob|\n|  2|Alice|\n|  0| NULL|\n+---+-----+"], "Parameters": [["col Column or column name", "target column to sort by in the descending order."]], "Returns": [["Column", "the column specifying the order."]], "Category": ["Functions"], "index": 94}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.abs.html#pyspark.sql.functions.abs"], "Title": ["abs"], "Feature": ["abs"], "Description": "Mathematical Function: Computes the absolute value of the given column or expression.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(-1,), (-2,), (-3,), (None,)], [\"value\"])\n>>> df.select(\"*\", sf.abs(df.value)).show()\n+-----+----------+\n|value|abs(value)|\n+-----+----------+\n|   -1|         1|\n|   -2|         2|\n|   -3|         3|\n| NULL|      NULL|\n+-----+----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(-1.5,), (-2.5,), (None,), (float(\"nan\"),)], [\"value\"])\n>>> df.select(\"*\", sf.abs(df.value)).show()\n+-----+----------+\n|value|abs(value)|\n+-----+----------+\n| -1.5|       1.5|\n| -2.5|       2.5|\n| NULL|      NULL|\n|  NaN|       NaN|\n+-----+----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, 1), (2, -2), (3, 3)], [\"id\", \"value\"])\n>>> df.select(\"*\", sf.abs(df.id - df.value)).show()\n+---+-----+-----------------+\n| id|value|abs((id - value))|\n+---+-----+-----------------+\n|  1|    1|                0|\n|  2|   -2|                4|\n|  3|    3|                0|\n+---+-----+-----------------+"], "Parameters": [["col Column or column name", "The target column or expression to compute the absolute value on."]], "Returns": [["Column", "A new column object representing the absolute value of the input."]], "Category": ["Functions"], "index": 95}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.acos.html#pyspark.sql.functions.acos"], "Title": ["acos"], "Feature": ["acos"], "Description": "Mathematical Function: Computes the inverse cosine (also known as arccosine)\nof the given column or expression.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(-1.0,), (-0.5,), (0.0,), (0.5,), (1.0,)], [\"value\"])\n>>> df.select(\"*\", sf.acos(\"value\")).show()\n+-----+------------------+\n|value|       ACOS(value)|\n+-----+------------------+\n| -1.0| 3.141592653589...|\n| -0.5|2.0943951023931...|\n|  0.0|1.5707963267948...|\n|  0.5|1.0471975511965...|\n|  1.0|               0.0|\n+-----+------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (-2), (2), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.acos(\"value\")).show()\n+-----+-----------+\n|value|ACOS(value)|\n+-----+-----------+\n|   -2|        NaN|\n|    2|        NaN|\n| NULL|       NULL|\n+-----+-----------+"], "Parameters": [["col Column or column name", "The target column or expression to compute the inverse cosine on."]], "Returns": [["Column", "A new column object representing the inverse cosine of the input."]], "Category": ["Functions"], "index": 96}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.acosh.html#pyspark.sql.functions.acosh"], "Title": ["acosh"], "Feature": ["acosh"], "Description": "Mathematical Function: Computes the inverse hyperbolic cosine (also known as arcosh)\nof the given column or expression.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1,), (2,)], [\"value\"])\n>>> df.select(\"*\", sf.acosh(df.value)).show()\n+-----+------------------+\n|value|      ACOSH(value)|\n+-----+------------------+\n|    1|               0.0|\n|    2|1.3169578969248...|\n+-----+------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (-0.5), (0.5), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.acosh(\"value\")).show()\n+-----+------------+\n|value|ACOSH(value)|\n+-----+------------+\n| -0.5|         NaN|\n|  0.5|         NaN|\n| NULL|        NULL|\n+-----+------------+"], "Parameters": [["col Column or column name", "The target column or expression to compute the inverse hyperbolic cosine on."]], "Returns": [["Column", "A new column object representing the inverse hyperbolic cosine of the input."]], "Category": ["Functions"], "index": 97}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.asin.html#pyspark.sql.functions.asin"], "Title": ["asin"], "Feature": ["asin"], "Description": "Computes inverse sine of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(-0.5,), (0.0,), (0.5,)], [\"value\"])\n>>> df.select(\"*\", sf.asin(df.value)).show()\n+-----+-------------------+\n|value|        ASIN(value)|\n+-----+-------------------+\n| -0.5|-0.5235987755982...|\n|  0.0|                0.0|\n|  0.5| 0.5235987755982...|\n+-----+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (-2), (2), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.asin(\"value\")).show()\n+-----+-----------+\n|value|ASIN(value)|\n+-----+-----------+\n|   -2|        NaN|\n|    2|        NaN|\n| NULL|       NULL|\n+-----+-----------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "inverse sine of col , as if computed by java.lang.Math.asin()"]], "Category": ["Functions"], "index": 98}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.asinh.html#pyspark.sql.functions.asinh"], "Title": ["asinh"], "Feature": ["asinh"], "Description": "Computes inverse hyperbolic sine of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(-0.5,), (0.0,), (0.5,)], [\"value\"])\n>>> df.select(\"*\", sf.asinh(df.value)).show()\n+-----+--------------------+\n|value|        ASINH(value)|\n+-----+--------------------+\n| -0.5|-0.48121182505960...|\n|  0.0|                 0.0|\n|  0.5| 0.48121182505960...|\n+-----+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.asinh(\"value\")).show()\n+-----+------------+\n|value|ASINH(value)|\n+-----+------------+\n|  NaN|         NaN|\n| NULL|        NULL|\n+-----+------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 99}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.atan.html#pyspark.sql.functions.atan"], "Title": ["atan"], "Feature": ["atan"], "Description": "Compute inverse tangent of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(-0.5,), (0.0,), (0.5,)], [\"value\"])\n>>> df.select(\"*\", sf.atan(df.value)).show()\n+-----+-------------------+\n|value|        ATAN(value)|\n+-----+-------------------+\n| -0.5|-0.4636476090008...|\n|  0.0|                0.0|\n|  0.5| 0.4636476090008...|\n+-----+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.atan(\"value\")).show()\n+-----+-----------+\n|value|ATAN(value)|\n+-----+-----------+\n|  NaN|        NaN|\n| NULL|       NULL|\n+-----+-----------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "inverse tangent of col , as if computed by java.lang.Math.atan()"]], "Category": ["Functions"], "index": 100}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.crossJoin.html#pyspark.sql.DataFrame.crossJoin"], "Title": ["DataFrame.crossJoin"], "Feature": ["DataFrame.crossJoin"], "Description": "Returns the cartesian product with anotherDataFrame.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df2 = spark.createDataFrame(\n...     [Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n>>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").show()\n+---+-----+------+\n|age| name|height|\n+---+-----+------+\n| 14|  Tom|    80|\n| 14|  Tom|    85|\n| 23|Alice|    80|\n| 23|Alice|    85|\n| 16|  Bob|    80|\n| 16|  Bob|    85|\n+---+-----+------+"], "Parameters": [["other DataFrame", "Right side of the cartesian product."]], "Returns": [["DataFrame", "Joined DataFrame."]], "Category": ["DataFrame"], "index": 101}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.atan2.html#pyspark.sql.functions.atan2"], "Title": ["atan2"], "Feature": ["atan2"], "Description": "Compute the angle in radians between the positive x-axis of a plane\nand the point given by the coordinates", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.atan2(sf.lit(1), sf.lit(2))).show()\n+------------------+\n|       ATAN2(1, 2)|\n+------------------+\n|0.4636476090008...|\n+------------------+"], "Parameters": [["col1 Column , column name or float", "coordinate on y-axis"], ["col2 Column , column name or float", "coordinate on x-axis"]], "Returns": [["Column", "the theta component of the point\n( r , theta )\nin polar coordinates that corresponds to the point\n( x , y ) in Cartesian coordinates,\nas if computed by java.lang.Math.atan2()"]], "Category": ["Functions"], "index": 102}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.atanh.html#pyspark.sql.functions.atanh"], "Title": ["atanh"], "Feature": ["atanh"], "Description": "Computes inverse hyperbolic tangent of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(-0.5,), (0.0,), (0.5,)], [\"value\"])\n>>> df.select(\"*\", sf.atanh(df.value)).show()\n+-----+-------------------+\n|value|       ATANH(value)|\n+-----+-------------------+\n| -0.5|-0.5493061443340...|\n|  0.0|                0.0|\n|  0.5| 0.5493061443340...|\n+-----+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (-2), (2), (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.atanh(\"value\")).show()\n+-----+------------+\n|value|ATANH(value)|\n+-----+------------+\n| -2.0|         NaN|\n|  2.0|         NaN|\n|  NaN|         NaN|\n| NULL|        NULL|\n+-----+------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 103}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bin.html#pyspark.sql.functions.bin"], "Title": ["bin"], "Feature": ["bin"], "Description": "Returns the string representation of the binary value of the given column.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(10).select(\"*\", sf.bin(\"id\")).show()\n+---+-------+\n| id|bin(id)|\n+---+-------+\n|  0|      0|\n|  1|      1|\n|  2|     10|\n|  3|     11|\n|  4|    100|\n|  5|    101|\n|  6|    110|\n|  7|    111|\n|  8|   1000|\n|  9|   1001|\n+---+-------+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "binary representation of given value as string."]], "Category": ["Functions"], "index": 104}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bround.html#pyspark.sql.functions.bround"], "Title": ["bround"], "Feature": ["bround"], "Description": "Round the given value toscaledecimal places using HALF_EVEN rounding mode ifscale>= 0\nor at integral part whenscale< 0.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.bround(sf.lit(2.5))).show()\n+--------------+\n|bround(2.5, 0)|\n+--------------+\n|           2.0|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.bround(sf.lit(2.1267), sf.lit(2))).show()\n+-----------------+\n|bround(2.1267, 2)|\n+-----------------+\n|             2.13|\n+-----------------+"], "Parameters": [["col Column or column name", "The target column or column name to compute the round on."], ["scale Column or int, optional", "An optional parameter to control the rounding behavior. Changed in version 4.0.0: Support Column type."]], "Returns": [["Column", "A column for the rounded value."]], "Category": ["Functions"], "index": 105}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.cbrt.html#pyspark.sql.functions.cbrt"], "Title": ["cbrt"], "Feature": ["cbrt"], "Description": "Computes the cube-root of the given value.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(-8,), (0,), (8,)], [\"value\"])\n>>> df.select(\"*\", sf.cbrt(df.value)).show()\n+-----+-----------+\n|value|CBRT(value)|\n+-----+-----------+\n|   -8|       -2.0|\n|    0|        0.0|\n|    8|        2.0|\n+-----+-----------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.cbrt(\"value\")).show()\n+-----+-----------+\n|value|CBRT(value)|\n+-----+-----------+\n|  NaN|        NaN|\n| NULL|       NULL|\n+-----+-----------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 106}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ceil.html#pyspark.sql.functions.ceil"], "Title": ["ceil"], "Feature": ["ceil"], "Description": "Computes the ceiling of the given value.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.ceil(sf.lit(-0.1))).show()\n+----------+\n|CEIL(-0.1)|\n+----------+\n|         0|\n+----------+", ">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.ceil(sf.lit(-0.1), 1)).show()\n+-------------+\n|ceil(-0.1, 1)|\n+-------------+\n|         -0.1|\n+-------------+"], "Parameters": [["col Column or column name", "The target column or column name to compute the ceiling on."], ["scale Column or int, optional", "An optional parameter to control the rounding behavior. New in version 4.0.0."]], "Returns": [["Column", "A column for the computed results."]], "Category": ["Functions"], "index": 107}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ceiling.html#pyspark.sql.functions.ceiling"], "Title": ["ceiling"], "Feature": ["ceiling"], "Description": "Computes the ceiling of the given value.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.ceiling(sf.lit(-0.1))).show()\n+-------------+\n|ceiling(-0.1)|\n+-------------+\n|            0|\n+-------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.ceiling(sf.lit(-0.1), 1)).show()\n+----------------+\n|ceiling(-0.1, 1)|\n+----------------+\n|            -0.1|\n+----------------+"], "Parameters": [["col Column or column name", "The target column or column name to compute the ceiling on."], ["scale Column or int", "An optional parameter to control the rounding behavior. New in version 4.0.0."]], "Returns": [["Column", "A column for the computed results."]], "Category": ["Functions"], "index": 108}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.conv.html#pyspark.sql.functions.conv"], "Title": ["conv"], "Feature": ["conv"], "Description": "Convert a number in a string column from one base to another.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"010101\",), ( \"101\",), (\"001\",)], ['n'])\n>>> df.select(\"*\", sf.conv(df.n, 2, 16)).show()\n+------+--------------+\n|     n|conv(n, 2, 16)|\n+------+--------------+\n|010101|            15|\n|   101|             5|\n|   001|             1|\n+------+--------------+"], "Parameters": [["col Column or str", "a column to convert base for."], ["fromBase: int", "from base number."], ["toBase: int", "to base number."]], "Returns": [["Column", "logariphm of given value."]], "Category": ["Functions"], "index": 109}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.cos.html#pyspark.sql.functions.cos"], "Title": ["cos"], "Feature": ["cos"], "Description": "Computes cosine of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (PI()), (PI() / 4), (PI() / 16) AS TAB(value)\"\n... ).select(\"*\", sf.cos(\"value\")).show()\n+-------------------+------------------+\n|              value|        COS(value)|\n+-------------------+------------------+\n|  3.141592653589...|              -1.0|\n| 0.7853981633974...|0.7071067811865...|\n|0.19634954084936...|0.9807852804032...|\n+-------------------+------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.cos(\"value\")).show()\n+-----+----------+\n|value|COS(value)|\n+-----+----------+\n|  NaN|       NaN|\n| NULL|      NULL|\n+-----+----------+"], "Parameters": [["col Column or column name", "angle in radians"]], "Returns": [["Column", "cosine of the angle, as if computed by java.lang.Math.cos() ."]], "Category": ["Functions"], "index": 110}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.cosh.html#pyspark.sql.functions.cosh"], "Title": ["cosh"], "Feature": ["cosh"], "Description": "Computes hyperbolic cosine of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(-1,), (0,), (1,)], [\"value\"])\n>>> df.select(\"*\", sf.cosh(df.value)).show()\n+-----+-----------------+\n|value|      COSH(value)|\n+-----+-----------------+\n|   -1|1.543080634815...|\n|    0|              1.0|\n|    1|1.543080634815...|\n+-----+-----------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.cosh(\"value\")).show()\n+-----+-----------+\n|value|COSH(value)|\n+-----+-----------+\n|  NaN|        NaN|\n| NULL|       NULL|\n+-----+-----------+"], "Parameters": [["col Column or column name", "hyperbolic angle"]], "Returns": [["Column", "hyperbolic cosine of the angle, as if computed by java.lang.Math.cosh()"]], "Category": ["Functions"], "index": 111}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.agg.html#pyspark.sql.DataFrame.agg"], "Title": ["DataFrame.agg"], "Feature": ["DataFrame.agg"], "Description": "Aggregate on the entireDataFramewithout groups\n(shorthand fordf.groupBy().agg()).", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.agg({\"age\": \"max\"}).show()\n+--------+\n|max(age)|\n+--------+\n|       5|\n+--------+\n>>> df.agg(sf.min(df.age)).show()\n+--------+\n|min(age)|\n+--------+\n|       2|\n+--------+"], "Parameters": [["exprs Column or dict of key and value strings", "Columns or expressions to aggregate DataFrame by."]], "Returns": [["DataFrame", "Aggregated DataFrame."]], "Category": ["DataFrame"], "index": 112}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.crosstab.html#pyspark.sql.DataFrame.crosstab"], "Title": ["DataFrame.crosstab"], "Feature": ["DataFrame.crosstab"], "Description": "Computes a pair-wise frequency table of the given columns. Also known as a contingency\ntable.\nThe first column of each row will be the distinct values ofcol1and the column names\nwill be the distinct values ofcol2. The name of the first column will be$col1_$col2.\nPairs that have no occurrences will have zero as their counts.DataFrame.crosstab()andDataFrameStatFunctions.crosstab()are aliases.", "Examples": [">>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n>>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n+-----+---+---+---+\n|c1_c2| 10| 11|  8|\n+-----+---+---+---+\n|    1|  0|  2|  0|\n|    3|  1|  0|  0|\n|    4|  0|  0|  2|\n+-----+---+---+---+"], "Parameters": [["col1 str", "The name of the first column. Distinct items will make the first item of\neach row."], ["col2 str", "The name of the second column. Distinct items will make the column names\nof the DataFrame ."]], "Returns": [["DataFrame", "Frequency matrix of two columns."]], "Category": ["DataFrame"], "index": 113}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.cot.html#pyspark.sql.functions.cot"], "Title": ["cot"], "Feature": ["cot"], "Description": "Computes cotangent of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (PI() / 4), (PI() / 16) AS TAB(value)\"\n... ).select(\"*\", sf.cot(\"value\")).show()\n+-------------------+------------------+\n|              value|        COT(value)|\n+-------------------+------------------+\n| 0.7853981633974...|1.0000000000000...|\n|0.19634954084936...| 5.027339492125...|\n+-------------------+------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (0.0), (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.cot(\"value\")).show()\n+-----+----------+\n|value|COT(value)|\n+-----+----------+\n|  0.0|  Infinity|\n|  NaN|       NaN|\n| NULL|      NULL|\n+-----+----------+"], "Parameters": [["col Column or column name", "angle in radians."]], "Returns": [["Column", "cotangent of the angle."]], "Category": ["Functions"], "index": 114}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.csc.html#pyspark.sql.functions.csc"], "Title": ["csc"], "Feature": ["csc"], "Description": "Computes cosecant of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (PI() / 2), (PI() / 4) AS TAB(value)\"\n... ).select(\"*\", sf.csc(\"value\")).show()\n+------------------+------------------+\n|             value|        CSC(value)|\n+------------------+------------------+\n|1.5707963267948...|               1.0|\n|0.7853981633974...|1.4142135623730...|\n+------------------+------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (0.0), (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.csc(\"value\")).show()\n+-----+----------+\n|value|CSC(value)|\n+-----+----------+\n|  0.0|  Infinity|\n|  NaN|       NaN|\n| NULL|      NULL|\n+-----+----------+"], "Parameters": [["col Column or column name", "angle in radians."]], "Returns": [["Column", "cosecant of the angle."]], "Category": ["Functions"], "index": 115}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.degrees.html#pyspark.sql.functions.degrees"], "Title": ["degrees"], "Feature": ["degrees"], "Description": "Converts an angle measured in radians to an approximately equivalent angle\nmeasured in degrees.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (0.0), (PI()), (PI() / 2), (PI() / 4) AS TAB(value)\"\n... ).select(\"*\", sf.degrees(\"value\")).show()\n+------------------+--------------+\n|             value|DEGREES(value)|\n+------------------+--------------+\n|               0.0|           0.0|\n| 3.141592653589...|         180.0|\n|1.5707963267948...|          90.0|\n|0.7853981633974...|          45.0|\n+------------------+--------------+"], "Parameters": [["col Column or column name", "angle in radians"]], "Returns": [["Column", "angle in degrees, as if computed by java.lang.Math.toDegrees()"]], "Category": ["Functions"], "index": 116}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.e.html#pyspark.sql.functions.e"], "Title": ["e"], "Feature": ["e"], "Description": "Returns Euler’s number.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.e()).show()\n+-----------------+\n|              E()|\n+-----------------+\n|2.718281828459045|\n+-----------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 117}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.exp.html#pyspark.sql.functions.exp"], "Title": ["exp"], "Feature": ["exp"], "Description": "Computes the exponential of the given value.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT id AS value FROM RANGE(5)\")\n>>> df.select(\"*\", sf.exp(df.value)).show()\n+-----+------------------+\n|value|        EXP(value)|\n+-----+------------------+\n|    0|               1.0|\n|    1|2.7182818284590...|\n|    2|  7.38905609893...|\n|    3|20.085536923187...|\n|    4|54.598150033144...|\n+-----+------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.exp(\"value\")).show()\n+-----+----------+\n|value|EXP(value)|\n+-----+----------+\n|  NaN|       NaN|\n| NULL|      NULL|\n+-----+----------+"], "Parameters": [["col Column or column name", "column to calculate exponential for."]], "Returns": [["Column", "exponential of the given value."]], "Category": ["Functions"], "index": 118}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.expm1.html#pyspark.sql.functions.expm1"], "Title": ["expm1"], "Feature": ["expm1"], "Description": "Computes the exponential of the given value minus one.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT id AS value FROM RANGE(5)\")\n>>> df.select(\"*\", sf.expm1(df.value)).show()\n+-----+------------------+\n|value|      EXPM1(value)|\n+-----+------------------+\n|    0|               0.0|\n|    1| 1.718281828459...|\n|    2|  6.38905609893...|\n|    3|19.085536923187...|\n|    4|53.598150033144...|\n+-----+------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.expm1(\"value\")).show()\n+-----+------------+\n|value|EXPM1(value)|\n+-----+------------+\n|  NaN|         NaN|\n| NULL|        NULL|\n+-----+------------+"], "Parameters": [["col Column or column name", "column to calculate exponential for."]], "Returns": [["Column", "exponential less one."]], "Category": ["Functions"], "index": 119}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.factorial.html#pyspark.sql.functions.factorial"], "Title": ["factorial"], "Feature": ["factorial"], "Description": "Computes the factorial of the given value.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(10).select(\"*\", sf.factorial('id')).show()\n+---+-------------+\n| id|factorial(id)|\n+---+-------------+\n|  0|            1|\n|  1|            1|\n|  2|            2|\n|  3|            6|\n|  4|           24|\n|  5|          120|\n|  6|          720|\n|  7|         5040|\n|  8|        40320|\n|  9|       362880|\n+---+-------------+"], "Parameters": [["col Column or str", "a column to calculate factorial for."]], "Returns": [["Column", "factorial of given value."]], "Category": ["Functions"], "index": 120}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.floor.html#pyspark.sql.functions.floor"], "Title": ["floor"], "Feature": ["floor"], "Description": "Computes the floor of the given value.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.floor(sf.lit(2.5))).show()\n+----------+\n|FLOOR(2.5)|\n+----------+\n|         2|\n+----------+", ">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.floor(sf.lit(2.1267), sf.lit(2))).show()\n+----------------+\n|floor(2.1267, 2)|\n+----------------+\n|            2.12|\n+----------------+"], "Parameters": [["col Column or column name", "The target column or column name to compute the floor on."], ["scale Column or int, optional", "An optional parameter to control the rounding behavior. New in version 4.0.0."]], "Returns": [["Column", "nearest integer that is less than or equal to given value."]], "Category": ["Functions"], "index": 121}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.greatest.html#pyspark.sql.functions.greatest"], "Title": ["greatest"], "Feature": ["greatest"], "Description": "Returns the greatest value of the list of column names, skipping null values.\nThis function takes at least 2 parameters. It will return null if all parameters are null.\nSee alsopyspark.sql.functions.least()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n>>> df.select(\"*\", sf.greatest(df.a, \"b\", df.c)).show()\n+---+---+---+-----------------+\n|  a|  b|  c|greatest(a, b, c)|\n+---+---+---+-----------------+\n|  1|  4|  3|                4|\n+---+---+---+-----------------+"], "Parameters": [["cols: :class:`~pyspark.sql.Column` or column name", "columns to check for greatest value."]], "Returns": [["Column", "greatest value."]], "Category": ["Functions"], "index": 122}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.hex.html#pyspark.sql.functions.hex"], "Title": ["hex"], "Feature": ["hex"], "Description": "Computes hex value of the given column, which could bepyspark.sql.types.StringType,pyspark.sql.types.BinaryType,pyspark.sql.types.IntegerTypeorpyspark.sql.types.LongType.\nSee alsopyspark.sql.functions.unhex()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('ABC', 3)], ['a', 'b'])\n>>> df.select('*', sf.hex('a'), sf.hex(df.b)).show()\n+---+---+------+------+\n|  a|  b|hex(a)|hex(b)|\n+---+---+------+------+\n|ABC|  3|414243|     3|\n+---+---+------+------+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "hexadecimal representation of given value as string."]], "Category": ["Functions"], "index": 123}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.cube.html#pyspark.sql.DataFrame.cube"], "Title": ["DataFrame.cube"], "Feature": ["DataFrame.cube"], "Description": "Create a multi-dimensional cube for the currentDataFrameusing\nthe specified columns, allowing aggregations to be performed on them.\nNotes\nA column ordinal starts from 1, which is different from the\n0-based__getitem__().", "Examples": [">>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], schema=[\"name\", \"age\"])", ">>> df.cube(\"name\").count().orderBy(\"name\").show()\n+-----+-----+\n| name|count|\n+-----+-----+\n| NULL|    2|\n|Alice|    1|\n|  Bob|    1|\n+-----+-----+", ">>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n+-----+----+-----+\n| name| age|count|\n+-----+----+-----+\n| NULL|NULL|    2|\n| NULL|   2|    1|\n| NULL|   5|    1|\n|Alice|NULL|    1|\n|Alice|   2|    1|\n|  Bob|NULL|    1|\n|  Bob|   5|    1|\n+-----+----+-----+", ">>> df.cube(1, 2).count().orderBy(1, 2).show()\n+-----+----+-----+\n| name| age|count|\n+-----+----+-----+\n| NULL|NULL|    2|\n| NULL|   2|    1|\n| NULL|   5|    1|\n|Alice|NULL|    1|\n|Alice|   2|    1|\n|  Bob|NULL|    1|\n|  Bob|   5|    1|\n+-----+----+-----+"], "Parameters": [["cols list, str, int or Column", "The columns to cube by.\nEach element should be a column name (string) or an expression ( Column )\nor a column ordinal (int, 1-based) or list of them. Changed in version 4.0.0: Supports column ordinal."]], "Returns": [["GroupedData", "Cube of the data based on the specified columns."]], "Category": ["DataFrame"], "index": 124}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.hypot.html#pyspark.sql.functions.hypot"], "Title": ["hypot"], "Feature": ["hypot"], "Description": "Computessqrt(a^2+b^2)without intermediate overflow or underflow.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.hypot(sf.lit(1), sf.lit(2))).show()\n+----------------+\n|     HYPOT(1, 2)|\n+----------------+\n|2.23606797749...|\n+----------------+"], "Parameters": [["col1 Column , column name or float", "a leg."], ["col2 Column , column name or float", "b leg."]], "Returns": [["Column", "length of the hypotenuse."]], "Category": ["Functions"], "index": 125}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.least.html#pyspark.sql.functions.least"], "Title": ["least"], "Feature": ["least"], "Description": "Returns the least value of the list of column names, skipping null values.\nThis function takes at least 2 parameters. It will return null if all parameters are null.\nSee alsopyspark.sql.functions.greatest()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n>>> df.select(\"*\", sf.least(df.a, \"b\", df.c)).show()\n+---+---+---+--------------+\n|  a|  b|  c|least(a, b, c)|\n+---+---+---+--------------+\n|  1|  4|  3|             1|\n+---+---+---+--------------+"], "Parameters": [["cols Column or column name", "column names or columns to be compared"]], "Returns": [["Column", "least value."]], "Category": ["Functions"], "index": 126}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ln.html#pyspark.sql.functions.ln"], "Title": ["ln"], "Feature": ["ln"], "Description": "Returns the natural logarithm of the argument.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(10).select(\"*\", sf.ln('id')).show()\n+---+------------------+\n| id|            ln(id)|\n+---+------------------+\n|  0|              NULL|\n|  1|               0.0|\n|  2|0.6931471805599...|\n|  3|1.0986122886681...|\n|  4|1.3862943611198...|\n|  5|1.6094379124341...|\n|  6| 1.791759469228...|\n|  7|1.9459101490553...|\n|  8|2.0794415416798...|\n|  9|2.1972245773362...|\n+---+------------------+"], "Parameters": [["col Column or str", "a column to calculate logariphm for."]], "Returns": [["Column", "natural logarithm of given value."]], "Category": ["Functions"], "index": 127}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.log.html#pyspark.sql.functions.log"], "Title": ["log"], "Feature": ["log"], "Description": "Returns the first argument-based logarithm of the second argument.\nIf there is only one argument, then this takes the natural logarithm of the argument.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1), (2), (4) AS t(value)\")\n>>> df.select(\"*\", sf.log(2.0, df.value)).show()\n+-----+---------------+\n|value|LOG(2.0, value)|\n+-----+---------------+\n|    1|            0.0|\n|    2|            1.0|\n|    4|            2.0|\n+-----+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1), (2), (0), (-1), (NULL) AS t(value)\")\n>>> df.select(\"*\", sf.log(3.0, df.value)).show()\n+-----+------------------+\n|value|   LOG(3.0, value)|\n+-----+------------------+\n|    1|               0.0|\n|    2|0.6309297535714...|\n|    0|              NULL|\n|   -1|              NULL|\n| NULL|              NULL|\n+-----+------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1), (2), (4) AS t(value)\")\n>>> df.select(\"*\", sf.log(df.value)).show()\n+-----+------------------+\n|value|         ln(value)|\n+-----+------------------+\n|    1|               0.0|\n|    2|0.6931471805599...|\n|    4|1.3862943611198...|\n+-----+------------------+"], "Parameters": [["arg1 Column , str or float", "base number or actual number (in this case base is e )"], ["arg2 Column , str or float, optional", "number to calculate logariphm for."]], "Returns": [["Column", "logariphm of given value."]], "Category": ["Functions"], "index": 128}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.log10.html#pyspark.sql.functions.log10"], "Title": ["log10"], "Feature": ["log10"], "Description": "Computes the logarithm of the given value in Base 10.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1,), (10,), (100,)], [\"value\"])\n>>> df.select(\"*\", sf.log10(df.value)).show()\n+-----+------------+\n|value|LOG10(value)|\n+-----+------------+\n|    1|         0.0|\n|   10|         1.0|\n|  100|         2.0|\n+-----+------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (-1), (0), (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.log10(\"value\")).show()\n+-----+------------+\n|value|LOG10(value)|\n+-----+------------+\n| -1.0|        NULL|\n|  0.0|        NULL|\n|  NaN|         NaN|\n| NULL|        NULL|\n+-----+------------+"], "Parameters": [["col Column or column name", "column to calculate logarithm for."]], "Returns": [["Column", "logarithm of the given value in Base 10."]], "Category": ["Functions"], "index": 129}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.log1p.html#pyspark.sql.functions.log1p"], "Title": ["log1p"], "Feature": ["log1p"], "Description": "Computes the natural logarithm of the given value plus one.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.log1p(sf.e())).show()\n+------------------+\n|        LOG1P(E())|\n+------------------+\n|1.3132616875182...|\n+------------------+", ">>> spark.range(1).select(sf.log(sf.e() + 1)).show()\n+------------------+\n|     ln((E() + 1))|\n+------------------+\n|1.3132616875182...|\n+------------------+"], "Parameters": [["col Column or column name", "column to calculate natural logarithm for."]], "Returns": [["Column", "natural logarithm of the “given value plus one”."]], "Category": ["Functions"], "index": 130}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.log2.html#pyspark.sql.functions.log2"], "Title": ["log2"], "Feature": ["log2"], "Description": "Returns the base-2 logarithm of the argument.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(10).select(\"*\", sf.log2('id')).show()\n+---+------------------+\n| id|          LOG2(id)|\n+---+------------------+\n|  0|              NULL|\n|  1|               0.0|\n|  2|               1.0|\n|  3| 1.584962500721...|\n|  4|               2.0|\n|  5| 2.321928094887...|\n|  6| 2.584962500721...|\n|  7| 2.807354922057...|\n|  8|               3.0|\n|  9|3.1699250014423...|\n+---+------------------+"], "Parameters": [["col Column or str", "a column to calculate logariphm for."]], "Returns": [["Column", "logariphm of given value."]], "Category": ["Functions"], "index": 131}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.negate.html#pyspark.sql.functions.negate"], "Title": ["negate"], "Feature": ["negate"], "Description": "Returns the negative value.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(-1,), (0,), (1,)], [\"value\"])\n>>> df.select(\"*\", sf.negative(df.value)).show()\n+-----+---------------+\n|value|negative(value)|\n+-----+---------------+\n|   -1|              1|\n|    0|              0|\n|    1|             -1|\n+-----+---------------+"], "Parameters": [["col Column or column name", "column to calculate negative value for."]], "Returns": [["Column", "negative value."]], "Category": ["Functions"], "index": 132}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.negative.html#pyspark.sql.functions.negative"], "Title": ["negative"], "Feature": ["negative"], "Description": "Returns the negative value.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(-1,), (0,), (1,)], [\"value\"])\n>>> df.select(\"*\", sf.negative(df.value)).show()\n+-----+---------------+\n|value|negative(value)|\n+-----+---------------+\n|   -1|              1|\n|    0|              0|\n|    1|             -1|\n+-----+---------------+"], "Parameters": [["col Column or column name", "column to calculate negative value for."]], "Returns": [["Column", "negative value."]], "Category": ["Functions"], "index": 133}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pi.html#pyspark.sql.functions.pi"], "Title": ["pi"], "Feature": ["pi"], "Description": "Returns Pi.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.pi()).show()\n+-----------------+\n|             PI()|\n+-----------------+\n|3.141592653589793|\n+-----------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 134}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html#pyspark.sql.DataFrame.describe"], "Title": ["DataFrame.describe"], "Feature": ["DataFrame.describe"], "Description": "Computes basic statistics for numeric and string columns.\nThis includes count, mean, stddev, min, and max. If no columns are\ngiven, this function computes statistics for all numerical or string columns.\nSee alsoDataFrame.summaryComputes summary statistics for numeric and string columns.\nNotes\nThis function is meant for exploratory data analysis, as we make no\nguarantee about the backward compatibility of the schema of the resultingDataFrame.\nUse summary for expanded statistics and control over which statistics to compute.", "Examples": [">>> df = spark.createDataFrame(\n...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n...     [\"name\", \"age\", \"weight\", \"height\"],\n... )\n>>> df.describe(['age']).show()\n+-------+----+\n|summary| age|\n+-------+----+\n|  count|   3|\n|   mean|12.0|\n| stddev| 1.0|\n|    min|  11|\n|    max|  13|\n+-------+----+", ">>> df.describe(['age', 'weight', 'height']).show()\n+-------+----+------------------+-----------------+\n|summary| age|            weight|           height|\n+-------+----+------------------+-----------------+\n|  count|   3|                 3|                3|\n|   mean|12.0| 40.73333333333333|            145.0|\n| stddev| 1.0|3.1722757341273704|4.763402145525822|\n|    min|  11|              37.8|            142.2|\n|    max|  13|              44.1|            150.5|\n+-------+----+------------------+-----------------+"], "Parameters": [["cols str, list, optional", "Column name or list of column names to describe by (default All columns)."]], "Returns": [["DataFrame", "A new DataFrame that describes (provides statistics) given DataFrame."]], "Category": ["DataFrame"], "index": 135}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pmod.html#pyspark.sql.functions.pmod"], "Title": ["pmod"], "Feature": ["pmod"], "Description": "Returns the positive value of dividend mod divisor.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (1.0, float('nan')), (float('nan'), 2.0), (10.0, 3.0),\n...     (float('nan'), float('nan')), (-3.0, 4.0), (-10.0, 3.0),\n...     (-5.0, -6.0), (7.0, -8.0), (1.0, 2.0)],\n...     (\"a\", \"b\"))\n>>> df.select(\"*\", sf.pmod(\"a\", \"b\")).show()\n+-----+----+----------+\n|    a|   b|pmod(a, b)|\n+-----+----+----------+\n|  1.0| NaN|       NaN|\n|  NaN| 2.0|       NaN|\n| 10.0| 3.0|       1.0|\n|  NaN| NaN|       NaN|\n| -3.0| 4.0|       1.0|\n|-10.0| 3.0|       2.0|\n| -5.0|-6.0|      -5.0|\n|  7.0|-8.0|       7.0|\n|  1.0| 2.0|       1.0|\n+-----+----+----------+"], "Parameters": [["dividend Column , column name or float", "the column that contains dividend, or the specified dividend value"], ["divisor Column , column name or float", "the column that contains divisor, or the specified divisor value"]], "Returns": [["Column", "positive value of dividend mod divisor."]], "Category": ["Functions"], "index": 136}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.positive.html#pyspark.sql.functions.positive"], "Title": ["positive"], "Feature": ["positive"], "Description": "Returns the value.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(-1,), (0,), (1,)], [\"value\"])\n>>> df.select(\"*\", sf.positive(df.value)).show()\n+-----+---------+\n|value|(+ value)|\n+-----+---------+\n|   -1|       -1|\n|    0|        0|\n|    1|        1|\n+-----+---------+"], "Parameters": [["col Column or column name", "input value column."]], "Returns": [["Column", "value."]], "Category": ["Functions"], "index": 137}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pow.html#pyspark.sql.functions.pow"], "Title": ["pow"], "Feature": ["pow"], "Description": "Returns the value of the first argument raised to the power of the second argument.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(5).select(\"*\", sf.pow(\"id\", 2)).show()\n+---+------------+\n| id|POWER(id, 2)|\n+---+------------+\n|  0|         0.0|\n|  1|         1.0|\n|  2|         4.0|\n|  3|         9.0|\n|  4|        16.0|\n+---+------------+"], "Parameters": [["col1 Column , column name or float", "the base number."], ["col2 Column , column name or float", "the exponent number."]], "Returns": [["Column", "the base rased to the power the argument."]], "Category": ["Functions"], "index": 138}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.power.html#pyspark.sql.functions.power"], "Title": ["power"], "Feature": ["power"], "Description": "Returns the value of the first argument raised to the power of the second argument.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(5).select(\"*\", sf.pow(\"id\", 2)).show()\n+---+------------+\n| id|POWER(id, 2)|\n+---+------------+\n|  0|         0.0|\n|  1|         1.0|\n|  2|         4.0|\n|  3|         9.0|\n|  4|        16.0|\n+---+------------+"], "Parameters": [["col1 Column , column name or float", "the base number."], ["col2 Column , column name or float", "the exponent number."]], "Returns": [["Column", "the base rased to the power the argument."]], "Category": ["Functions"], "index": 139}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.radians.html#pyspark.sql.functions.radians"], "Title": ["radians"], "Feature": ["radians"], "Description": "Converts an angle measured in degrees to an approximately equivalent angle\nmeasured in radians.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (180), (90), (45), (0) AS TAB(value)\"\n... ).select(\"*\", sf.radians(\"value\")).show()\n+-----+------------------+\n|value|    RADIANS(value)|\n+-----+------------------+\n|  180| 3.141592653589...|\n|   90|1.5707963267948...|\n|   45|0.7853981633974...|\n|    0|               0.0|\n+-----+------------------+"], "Parameters": [["col Column or column name", "angle in degrees"]], "Returns": [["Column", "angle in radians, as if computed by java.lang.Math.toRadians()"]], "Category": ["Functions"], "index": 140}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.rand.html#pyspark.sql.functions.rand"], "Title": ["rand"], "Feature": ["rand"], "Description": "Generates a random column with independent and identically distributed (i.i.d.) samples\nuniformly distributed in [0.0, 1.0).\nSee alsopyspark.sql.functions.randn()pyspark.sql.functions.randstr()pyspark.sql.functions.uniform()\nNotes\nThe function is non-deterministic in general case.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(0, 2, 1, 1).select(\"*\", sf.rand()).show() \n+---+-------------------------+\n| id|rand(-158884697681280011)|\n+---+-------------------------+\n|  0|       0.9253464547887...|\n|  1|       0.6533254118758...|\n+---+-------------------------+", ">>> spark.range(0, 2, 1, 1).select(\"*\", sf.rand(seed=42)).show()\n+---+------------------+\n| id|          rand(42)|\n+---+------------------+\n|  0| 0.619189370225...|\n|  1|0.5096018842446...|\n+---+------------------+"], "Parameters": [["seed int, optional", "Seed value for the random generator."]], "Returns": [["Column", "A column of random values."]], "Category": ["Functions"], "index": 141}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.randn.html#pyspark.sql.functions.randn"], "Title": ["randn"], "Feature": ["randn"], "Description": "Generates a random column with independent and identically distributed (i.i.d.) samples\nfrom the standard normal distribution.\nSee alsopyspark.sql.functions.rand()pyspark.sql.functions.randstr()pyspark.sql.functions.uniform()\nNotes\nThe function is non-deterministic in general case.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(0, 2, 1, 1).select(\"*\", sf.randn()).show() \n+---+--------------------------+\n| id|randn(3968742514375399317)|\n+---+--------------------------+\n|  0|      -0.47968645355788...|\n|  1|       -0.4950952457305...|\n+---+--------------------------+", ">>> spark.range(0, 2, 1, 1).select(\"*\", sf.randn(seed=42)).show()\n+---+------------------+\n| id|         randn(42)|\n+---+------------------+\n|  0| 2.384479054241...|\n|  1|0.1920934041293...|\n+---+------------------+"], "Parameters": [["seed int (default: None)", "Seed value for the random generator."]], "Returns": [["Column", "A column of random values."]], "Category": ["Functions"], "index": 142}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.rint.html#pyspark.sql.functions.rint"], "Title": ["rint"], "Feature": ["rint"], "Description": "Returns the double value that is closest in value to the argument and\nis equal to a mathematical integer.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.rint(sf.lit(10.6))).show()\n+----------+\n|rint(10.6)|\n+----------+\n|      11.0|\n+----------+", ">>> spark.range(1).select(sf.rint(sf.lit(10.3))).show()\n+----------+\n|rint(10.3)|\n+----------+\n|      10.0|\n+----------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 143}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.round.html#pyspark.sql.functions.round"], "Title": ["round"], "Feature": ["round"], "Description": "Round the given value toscaledecimal places using HALF_UP rounding mode ifscale>= 0\nor at integral part whenscale< 0.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.round(sf.lit(2.5))).show()\n+-------------+\n|round(2.5, 0)|\n+-------------+\n|          3.0|\n+-------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.round(sf.lit(2.1267), sf.lit(2))).show()\n+----------------+\n|round(2.1267, 2)|\n+----------------+\n|            2.13|\n+----------------+"], "Parameters": [["col Column or column name", "The target column or column name to compute the round on."], ["scale Column or int, optional", "An optional parameter to control the rounding behavior. Changed in version 4.0.0: Support Column type."]], "Returns": [["Column", "A column for the rounded value."]], "Category": ["Functions"], "index": 144}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sec.html#pyspark.sql.functions.sec"], "Title": ["sec"], "Feature": ["sec"], "Description": "Computes secant of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (PI() / 4), (PI() / 16) AS TAB(value)\"\n... ).select(\"*\", sf.sec(\"value\")).show()\n+-------------------+------------------+\n|              value|        SEC(value)|\n+-------------------+------------------+\n| 0.7853981633974...| 1.414213562373...|\n|0.19634954084936...|1.0195911582083...|\n+-------------------+------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.sec(\"value\")).show()\n+-----+----------+\n|value|SEC(value)|\n+-----+----------+\n|  NaN|       NaN|\n| NULL|      NULL|\n+-----+----------+"], "Parameters": [["col Column or column name", "Angle in radians"]], "Returns": [["Column", "Secant of the angle."]], "Category": ["Functions"], "index": 145}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.distinct.html#pyspark.sql.DataFrame.distinct"], "Title": ["DataFrame.distinct"], "Feature": ["DataFrame.distinct"], "Description": "Returns a newDataFramecontaining the distinct rows in thisDataFrame.\nSee alsoDataFrame.dropDuplicatesRemove duplicate rows from this DataFrame.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (23, \"Alice\")], [\"age\", \"name\"])\n>>> df.distinct().show()\n+---+-----+\n|age| name|\n+---+-----+\n| 14|  Tom|\n| 23|Alice|\n+---+-----+", ">>> df.distinct().count()\n2", ">>> df = spark.createDataFrame(\n...     [(14, \"Tom\", \"M\"), (23, \"Alice\", \"F\"), (23, \"Alice\", \"F\"), (14, \"Tom\", \"M\")],\n...     [\"age\", \"name\", \"gender\"])\n>>> df.distinct().show()\n+---+-----+------+\n|age| name|gender|\n+---+-----+------+\n| 14|  Tom|     M|\n| 23|Alice|     F|\n+---+-----+------+", ">>> df.select(\"name\").distinct().show()\n+-----+\n| name|\n+-----+\n|  Tom|\n|Alice|\n+-----+", ">>> df.select(\"name\").distinct().count()\n2", ">>> df.select(\"name\", \"gender\").distinct().show()\n+-----+------+\n| name|gender|\n+-----+------+\n|  Tom|     M|\n|Alice|     F|\n+-----+------+", ">>> df = spark.createDataFrame(\n...     [(14, \"Tom\", \"M\"), (23, \"Alice\", \"F\"), (23, \"Alice\", \"F\"), (14, \"Tom\", None)],\n...     [\"age\", \"name\", \"gender\"])\n>>> df.distinct().show()\n+---+-----+------+\n|age| name|gender|\n+---+-----+------+\n| 14|  Tom|     M|\n| 23|Alice|     F|\n| 14|  Tom|  NULL|\n+---+-----+------+", ">>> df.distinct().filter(df.gender.isNotNull()).show()\n+---+-----+------+\n|age| name|gender|\n+---+-----+------+\n| 14|  Tom|     M|\n| 23|Alice|     F|\n+---+-----+------+"], "Parameters": [], "Returns": [["DataFrame", "DataFrame with distinct records."]], "Category": ["DataFrame"], "index": 146}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sign.html#pyspark.sql.functions.sign"], "Title": ["sign"], "Feature": ["sign"], "Description": "Computes the signum of the given value.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(\n...     sf.sign(sf.lit(-5)),\n...     sf.sign(sf.lit(6)),\n...     sf.sign(sf.lit(float('nan'))),\n...     sf.sign(sf.lit(None))\n... ).show()\n+--------+-------+---------+----------+\n|sign(-5)|sign(6)|sign(NaN)|sign(NULL)|\n+--------+-------+---------+----------+\n|    -1.0|    1.0|      NaN|      NULL|\n+--------+-------+---------+----------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 147}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.signum.html#pyspark.sql.functions.signum"], "Title": ["signum"], "Feature": ["signum"], "Description": "Computes the signum of the given value.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(\n...     sf.signum(sf.lit(-5)),\n...     sf.signum(sf.lit(6)),\n...     sf.signum(sf.lit(float('nan'))),\n...     sf.signum(sf.lit(None))\n... ).show()\n+----------+---------+-----------+------------+\n|SIGNUM(-5)|SIGNUM(6)|SIGNUM(NaN)|SIGNUM(NULL)|\n+----------+---------+-----------+------------+\n|      -1.0|      1.0|        NaN|        NULL|\n+----------+---------+-----------+------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 148}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sin.html#pyspark.sql.functions.sin"], "Title": ["sin"], "Feature": ["sin"], "Description": "Computes sine of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (0.0), (PI() / 2), (PI() / 4) AS TAB(value)\"\n... ).select(\"*\", sf.sin(\"value\")).show()\n+------------------+------------------+\n|             value|        SIN(value)|\n+------------------+------------------+\n|               0.0|               0.0|\n|1.5707963267948...|               1.0|\n|0.7853981633974...|0.7071067811865...|\n+------------------+------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.sin(\"value\")).show()\n+-----+----------+\n|value|SIN(value)|\n+-----+----------+\n|  NaN|       NaN|\n| NULL|      NULL|\n+-----+----------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "sine of the angle, as if computed by java.lang.Math.sin()"]], "Category": ["Functions"], "index": 149}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sinh.html#pyspark.sql.functions.sinh"], "Title": ["sinh"], "Feature": ["sinh"], "Description": "Computes hyperbolic sine of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(-1,), (0,), (1,)], [\"value\"])\n>>> df.select(\"*\", sf.sinh(df.value)).show()\n+-----+-------------------+\n|value|        SINH(value)|\n+-----+-------------------+\n|   -1|-1.1752011936438...|\n|    0|                0.0|\n|    1| 1.1752011936438...|\n+-----+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.sinh(\"value\")).show()\n+-----+-----------+\n|value|SINH(value)|\n+-----+-----------+\n|  NaN|        NaN|\n| NULL|       NULL|\n+-----+-----------+"], "Parameters": [["col Column or column name", "hyperbolic angle."]], "Returns": [["Column", "hyperbolic sine of the given value,\nas if computed by java.lang.Math.sinh()"]], "Category": ["Functions"], "index": 150}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sqrt.html#pyspark.sql.functions.sqrt"], "Title": ["sqrt"], "Feature": ["sqrt"], "Description": "Computes the square root of the specified float value.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (-1), (0), (1), (4), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.sqrt(\"value\")).show()\n+-----+-----------+\n|value|SQRT(value)|\n+-----+-----------+\n|   -1|        NaN|\n|    0|        0.0|\n|    1|        1.0|\n|    4|        2.0|\n| NULL|       NULL|\n+-----+-----------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "column for computed results."]], "Category": ["Functions"], "index": 151}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.tan.html#pyspark.sql.functions.tan"], "Title": ["tan"], "Feature": ["tan"], "Description": "Computes tangent of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (0.0), (PI() / 4), (PI() / 6) AS TAB(value)\"\n... ).select(\"*\", sf.tan(\"value\")).show()\n+------------------+------------------+\n|             value|        TAN(value)|\n+------------------+------------------+\n|               0.0|               0.0|\n|0.7853981633974...|0.9999999999999...|\n|0.5235987755982...|0.5773502691896...|\n+------------------+------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.tan(\"value\")).show()\n+-----+----------+\n|value|TAN(value)|\n+-----+----------+\n|  NaN|       NaN|\n| NULL|      NULL|\n+-----+----------+"], "Parameters": [["col Column or column name", "angle in radians"]], "Returns": [["Column", "tangent of the given value, as if computed by java.lang.Math.tan()"]], "Category": ["Functions"], "index": 152}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.tanh.html#pyspark.sql.functions.tanh"], "Title": ["tanh"], "Feature": ["tanh"], "Description": "Computes hyperbolic tangent of the input column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(-1,), (0,), (1,)], [\"value\"])\n>>> df.select(\"*\", sf.tanh(df.value)).show()\n+-----+-------------------+\n|value|        TANH(value)|\n+-----+-------------------+\n|   -1|-0.7615941559557...|\n|    0|                0.0|\n|    1| 0.7615941559557...|\n+-----+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (FLOAT('NAN')), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.tanh(\"value\")).show()\n+-----+-----------+\n|value|TANH(value)|\n+-----+-----------+\n|  NaN|        NaN|\n| NULL|       NULL|\n+-----+-----------+"], "Parameters": [["col Column or column name", "hyperbolic angle"]], "Returns": [["Column", "hyperbolic tangent of the given value\nas if computed by java.lang.Math.tanh()"]], "Category": ["Functions"], "index": 153}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_add.html#pyspark.sql.functions.try_add"], "Title": ["try_add"], "Feature": ["try_add"], "Description": "Returns the sum ofleft`and `rightand the result is null on overflow.\nThe acceptable input types are the same with the+operator.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(1982, 15), (1990, 2)], [\"birth\", \"age\"]\n... ).select(\"*\", sf.try_add(\"birth\", \"age\")).show()\n+-----+---+-------------------+\n|birth|age|try_add(birth, age)|\n+-----+---+-------------------+\n| 1982| 15|               1997|\n| 1990|  2|               1992|\n+-----+---+-------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (DATE('2015-09-30')) AS TAB(date)\"\n... ).select(\"*\", sf.try_add(\"date\", sf.lit(1))).show()\n+----------+----------------+\n|      date|try_add(date, 1)|\n+----------+----------------+\n|2015-09-30|      2015-10-01|\n+----------+----------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (DATE('2015-09-30'), INTERVAL 1 YEAR) AS TAB(date, itvl)\"\n... ).select(\"*\", sf.try_add(\"date\", \"itvl\")).show()\n+----------+-----------------+-------------------+\n|      date|             itvl|try_add(date, itvl)|\n+----------+-----------------+-------------------+\n|2015-09-30|INTERVAL '1' YEAR|         2016-09-30|\n+----------+-----------------+-------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (INTERVAL 1 YEAR, INTERVAL 2 YEAR) AS TAB(itvl1, itvl2)\"\n... ).select(\"*\", sf.try_add(\"itvl1\", \"itvl2\")).show()\n+-----------------+-----------------+---------------------+\n|            itvl1|            itvl2|try_add(itvl1, itvl2)|\n+-----------------+-----------------+---------------------+\n|INTERVAL '1' YEAR|INTERVAL '2' YEAR|    INTERVAL '3' YEAR|\n+-----------------+-----------------+---------------------+", ">>> import pyspark.sql.functions as sf\n>>> origin = spark.conf.get(\"spark.sql.ansi.enabled\")\n>>> spark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\n>>> try:\n...     spark.range(1).select(sf.try_add(sf.lit(sys.maxsize), sf.lit(sys.maxsize))).show()\n... finally:\n...     spark.conf.set(\"spark.sql.ansi.enabled\", origin)\n+-------------------------------------------------+\n|try_add(9223372036854775807, 9223372036854775807)|\n+-------------------------------------------------+\n|                                             NULL|\n+-------------------------------------------------+"], "Parameters": [["left Column or column name", ""], ["right Column or column name", ""]], "Returns": [], "Category": ["Functions"], "index": 154}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_divide.html#pyspark.sql.functions.try_divide"], "Title": ["try_divide"], "Feature": ["try_divide"], "Description": "Returnsdividend/divisor. It always performs floating point division. Its result is\nalways null ifdivisoris 0.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(6000, 15), (1990, 2), (1234, 0)], [\"a\", \"b\"]\n... ).select(\"*\", sf.try_divide(\"a\", \"b\")).show()\n+----+---+----------------+\n|   a|  b|try_divide(a, b)|\n+----+---+----------------+\n|6000| 15|           400.0|\n|1990|  2|           995.0|\n|1234|  0|            NULL|\n+----+---+----------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.range(4).select(sf.make_interval(sf.lit(1)).alias(\"itvl\"), \"id\")\n>>> df.select(\"*\", sf.try_divide(\"itvl\", \"id\")).show()\n+-------+---+--------------------+\n|   itvl| id|try_divide(itvl, id)|\n+-------+---+--------------------+\n|1 years|  0|                NULL|\n|1 years|  1|             1 years|\n|1 years|  2|            6 months|\n|1 years|  3|            4 months|\n+-------+---+--------------------+", ">>> import pyspark.sql.functions as sf\n>>> origin = spark.conf.get(\"spark.sql.ansi.enabled\")\n>>> spark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\n>>> try:\n...     spark.range(1).select(sf.try_divide(\"id\", sf.lit(0))).show()\n... finally:\n...     spark.conf.set(\"spark.sql.ansi.enabled\", origin)\n+-----------------+\n|try_divide(id, 0)|\n+-----------------+\n|             NULL|\n+-----------------+"], "Parameters": [["left Column or column name", "dividend"], ["right Column or column name", "divisor"]], "Returns": [], "Category": ["Functions"], "index": 155}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_mod.html#pyspark.sql.functions.try_mod"], "Title": ["try_mod"], "Feature": ["try_mod"], "Description": "Returns the remainder afterdividend/divisor.  Its result is\nalways null ifdivisoris 0.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(6000, 15), (3, 2), (1234, 0)], [\"a\", \"b\"]\n... ).select(\"*\", sf.try_mod(\"a\", \"b\")).show()\n+----+---+-------------+\n|   a|  b|try_mod(a, b)|\n+----+---+-------------+\n|6000| 15|            0|\n|   3|  2|            1|\n|1234|  0|         NULL|\n+----+---+-------------+", ">>> import pyspark.sql.functions as sf\n>>> origin = spark.conf.get(\"spark.sql.ansi.enabled\")\n>>> spark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\n>>> try:\n...     spark.range(1).select(sf.try_mod(\"id\", sf.lit(0))).show()\n... finally:\n...     spark.conf.set(\"spark.sql.ansi.enabled\", origin)\n+--------------+\n|try_mod(id, 0)|\n+--------------+\n|          NULL|\n+--------------+"], "Parameters": [["left Column or column name", "dividend"], ["right Column or column name", "divisor"]], "Returns": [], "Category": ["Functions"], "index": 156}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.drop.html#pyspark.sql.DataFrame.drop"], "Title": ["DataFrame.drop"], "Feature": ["DataFrame.drop"], "Description": "Returns a newDataFramewithout specified columns.\nThis is a no-op if the schema doesn’t contain the given column name(s).\nNotes\nWhen an input is a column name, it is treated literally without further interpretation.\nOtherwise, it will try to match the equivalent expression.\nSo dropping a column by its namedrop(colName)has a different semantic\nwith directly dropping the columndrop(col(colName)).", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.drop('age').show()\n+-----+\n| name|\n+-----+\n|  Tom|\n|Alice|\n|  Bob|\n+-----+", ">>> df.drop(df.age).show()\n+-----+\n| name|\n+-----+\n|  Tom|\n|Alice|\n|  Bob|\n+-----+", ">>> df2 = spark.createDataFrame([(80, \"Tom\"), (85, \"Bob\")], [\"height\", \"name\"])\n>>> df.join(df2, df.name == df2.name).drop('name').sort('age').show()\n+---+------+\n|age|height|\n+---+------+\n| 14|    80|\n| 16|    85|\n+---+------+", ">>> df3 = df.join(df2)\n>>> df3.show()\n+---+-----+------+----+\n|age| name|height|name|\n+---+-----+------+----+\n| 14|  Tom|    80| Tom|\n| 14|  Tom|    85| Bob|\n| 23|Alice|    80| Tom|\n| 23|Alice|    85| Bob|\n| 16|  Bob|    80| Tom|\n| 16|  Bob|    85| Bob|\n+---+-----+------+----+", ">>> df3.drop(\"name\").show()\n+---+------+\n|age|height|\n+---+------+\n| 14|    80|\n| 14|    85|\n| 23|    80|\n| 23|    85|\n| 16|    80|\n| 16|    85|\n+---+------+", ">>> from pyspark.sql import functions as sf\n>>> df3.drop(sf.col(\"name\")).show()\nTraceback (most recent call last):\n...\npyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference...", ">>> from pyspark.sql import functions as sf\n>>> df4 = df.withColumn(\"a.b.c\", sf.lit(1))\n>>> df4.show()\n+---+-----+-----+\n|age| name|a.b.c|\n+---+-----+-----+\n| 14|  Tom|    1|\n| 23|Alice|    1|\n| 16|  Bob|    1|\n+---+-----+-----+", ">>> df4.drop(\"a.b.c\").show()\n+---+-----+\n|age| name|\n+---+-----+\n| 14|  Tom|\n| 23|Alice|\n| 16|  Bob|\n+---+-----+", ">>> df4.drop(sf.col(\"a.b.c\")).show()\n+---+-----+-----+\n|age| name|a.b.c|\n+---+-----+-----+\n| 14|  Tom|    1|\n| 23|Alice|    1|\n| 16|  Bob|    1|\n+---+-----+-----+"], "Parameters": [["cols: str or :class:`Column`", "A name of the column, or the Column to be dropped."]], "Returns": [["DataFrame", "A new DataFrame without the specified columns."]], "Category": ["DataFrame"], "index": 157}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_multiply.html#pyspark.sql.functions.try_multiply"], "Title": ["try_multiply"], "Feature": ["try_multiply"], "Description": "Returnsleft`*`rightand the result is null on overflow. The acceptable input types are the\nsame with the*operator.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(6000, 15), (1990, 2)], [\"a\", \"b\"]\n... ).select(\"*\", sf.try_multiply(\"a\", \"b\")).show()\n+----+---+------------------+\n|   a|  b|try_multiply(a, b)|\n+----+---+------------------+\n|6000| 15|             90000|\n|1990|  2|              3980|\n+----+---+------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.range(6).select(sf.make_interval(sf.col(\"id\"), sf.lit(3)).alias(\"itvl\"), \"id\")\n>>> df.select(\"*\", sf.try_multiply(\"itvl\", \"id\")).show()\n+----------------+---+----------------------+\n|            itvl| id|try_multiply(itvl, id)|\n+----------------+---+----------------------+\n|        3 months|  0|             0 seconds|\n|1 years 3 months|  1|      1 years 3 months|\n|2 years 3 months|  2|      4 years 6 months|\n|3 years 3 months|  3|      9 years 9 months|\n|4 years 3 months|  4|              17 years|\n|5 years 3 months|  5|     26 years 3 months|\n+----------------+---+----------------------+", ">>> import pyspark.sql.functions as sf\n>>> origin = spark.conf.get(\"spark.sql.ansi.enabled\")\n>>> spark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\n>>> try:\n...     spark.range(1).select(sf.try_multiply(sf.lit(sys.maxsize), sf.lit(sys.maxsize))).show()\n... finally:\n...     spark.conf.set(\"spark.sql.ansi.enabled\", origin)\n+------------------------------------------------------+\n|try_multiply(9223372036854775807, 9223372036854775807)|\n+------------------------------------------------------+\n|                                                  NULL|\n+------------------------------------------------------+"], "Parameters": [["left Column or column name", "multiplicand"], ["right Column or column name", "multiplier"]], "Returns": [], "Category": ["Functions"], "index": 158}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_subtract.html#pyspark.sql.functions.try_subtract"], "Title": ["try_subtract"], "Feature": ["try_subtract"], "Description": "Returnsleft-rightand the result is null on overflow. The acceptable input types are the\nsame with the-operator.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(1982, 15), (1990, 2)], [\"birth\", \"age\"]\n... ).select(\"*\", sf.try_subtract(\"birth\", \"age\")).show()\n+-----+---+------------------------+\n|birth|age|try_subtract(birth, age)|\n+-----+---+------------------------+\n| 1982| 15|                    1967|\n| 1990|  2|                    1988|\n+-----+---+------------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (DATE('2015-10-01')) AS TAB(date)\"\n... ).select(\"*\", sf.try_subtract(\"date\", sf.lit(1))).show()\n+----------+---------------------+\n|      date|try_subtract(date, 1)|\n+----------+---------------------+\n|2015-10-01|           2015-09-30|\n+----------+---------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (DATE('2015-09-30'), INTERVAL 1 YEAR) AS TAB(date, itvl)\"\n... ).select(\"*\", sf.try_subtract(\"date\", \"itvl\")).show()\n+----------+-----------------+------------------------+\n|      date|             itvl|try_subtract(date, itvl)|\n+----------+-----------------+------------------------+\n|2015-09-30|INTERVAL '1' YEAR|              2014-09-30|\n+----------+-----------------+------------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (INTERVAL 1 YEAR, INTERVAL 2 YEAR) AS TAB(itvl1, itvl2)\"\n... ).select(\"*\", sf.try_subtract(\"itvl1\", \"itvl2\")).show()\n+-----------------+-----------------+--------------------------+\n|            itvl1|            itvl2|try_subtract(itvl1, itvl2)|\n+-----------------+-----------------+--------------------------+\n|INTERVAL '1' YEAR|INTERVAL '2' YEAR|        INTERVAL '-1' YEAR|\n+-----------------+-----------------+--------------------------+", ">>> import pyspark.sql.functions as sf\n>>> origin = spark.conf.get(\"spark.sql.ansi.enabled\")\n>>> spark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\n>>> try:\n...     spark.range(1).select(sf.try_subtract(sf.lit(-sys.maxsize), sf.lit(sys.maxsize))).show()\n... finally:\n...     spark.conf.set(\"spark.sql.ansi.enabled\", origin)\n+-------------------------------------------------------+\n|try_subtract(-9223372036854775807, 9223372036854775807)|\n+-------------------------------------------------------+\n|                                                   NULL|\n+-------------------------------------------------------+"], "Parameters": [["left Column or column name", ""], ["right Column or column name", ""]], "Returns": [], "Category": ["Functions"], "index": 159}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.unhex.html#pyspark.sql.functions.unhex"], "Title": ["unhex"], "Feature": ["unhex"], "Description": "Inverse of hex. Interprets each pair of characters as a hexadecimal number\nand converts to the byte representation of number.\nSee alsopyspark.sql.functions.hex()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('414243',)], ['a'])\n>>> df.select('*', sf.unhex('a')).show()\n+------+----------+\n|     a|  unhex(a)|\n+------+----------+\n|414243|[41 42 43]|\n+------+----------+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "string representation of given hexadecimal value."]], "Category": ["Functions"], "index": 160}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.uniform.html#pyspark.sql.functions.uniform"], "Title": ["uniform"], "Feature": ["uniform"], "Description": "Returns a random value with independent and identically distributed (i.i.d.) values with the\nspecified range of numbers. The random seed is optional. The provided numbers specifying the\nminimum and maximum values of the range must be constant. If both of these numbers are integers,\nthen the result will also be an integer. Otherwise if one or both of these are floating-point\nnumbers, then the result will also be a floating-point number.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(0, 10, 1, 1).select(sf.uniform(5, 105, 3)).show()\n+------------------+\n|uniform(5, 105, 3)|\n+------------------+\n|                30|\n|                71|\n|                99|\n|                77|\n|                16|\n|                25|\n|                89|\n|                80|\n|                51|\n|                83|\n+------------------+"], "Parameters": [["min Column , int, or float", "Minimum value in the range."], ["max Column , int, or float", "Maximum value in the range."], ["seed Column or int", "Optional random number seed to use."]], "Returns": [["Column", "The generated random number within the specified range."]], "Category": ["Functions"], "index": 161}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.width_bucket.html#pyspark.sql.functions.width_bucket"], "Title": ["width_bucket"], "Feature": ["width_bucket"], "Description": "Returns the bucket number into which the value of this expression would fall\nafter being evaluated. Note that input arguments must follow conditions listed below;\notherwise, the method will return null.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (5.3, 0.2, 10.6, 5),\n...     (-2.1, 1.3, 3.4, 3),\n...     (8.1, 0.0, 5.7, 4),\n...     (-0.9, 5.2, 0.5, 2)],\n...     ['v', 'min', 'max', 'n'])\n>>> df.select(\"*\", sf.width_bucket('v', 'min', 'max', 'n')).show()\n+----+---+----+---+----------------------------+\n|   v|min| max|  n|width_bucket(v, min, max, n)|\n+----+---+----+---+----------------------------+\n| 5.3|0.2|10.6|  5|                           3|\n|-2.1|1.3| 3.4|  3|                           0|\n| 8.1|0.0| 5.7|  4|                           5|\n|-0.9|5.2| 0.5|  2|                           3|\n+----+---+----+---+----------------------------+"], "Parameters": [["v Column or column name", "value to compute a bucket number in the histogram"], ["min Column or column name", "minimum value of the histogram"], ["max Column or column name", "maximum value of the histogram"], ["numBucket Column , column name or int", "the number of buckets"]], "Returns": [["Column", "the bucket number into which the value would fall after being evaluated"]], "Category": ["Functions"], "index": 162}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ascii.html#pyspark.sql.functions.ascii"], "Title": ["ascii"], "Feature": ["ascii"], "Description": "Computes the numeric value of the first character of the string column.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n>>> df.select(\"*\", sf.ascii(\"value\")).show()\n+----------+------------+\n|     value|ascii(value)|\n+----------+------------+\n|     Spark|          83|\n|   PySpark|          80|\n|Pandas API|          80|\n+----------+------------+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "numeric value."]], "Category": ["Functions"], "index": 163}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.base64.html#pyspark.sql.functions.base64"], "Title": ["base64"], "Feature": ["base64"], "Description": "Computes the BASE64 encoding of a binary column and returns it as a string column.\nSee alsopyspark.sql.functions.unbase64()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n>>> df.select(\"*\", sf.base64(\"value\")).show()\n+----------+----------------+\n|     value|   base64(value)|\n+----------+----------------+\n|     Spark|        U3Bhcms=|\n|   PySpark|    UHlTcGFyaw==|\n|Pandas API|UGFuZGFzIEFQSQ==|\n+----------+----------------+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "BASE64 encoding of string value."]], "Category": ["Functions"], "index": 164}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bit_length.html#pyspark.sql.functions.bit_length"], "Title": ["bit_length"], "Feature": ["bit_length"], "Description": "Calculates the bit length for the specified string column.", "Examples": [">>> from pyspark.sql.functions import bit_length\n>>> spark.createDataFrame([('cat',), ( '🐈',)], ['cat']) \\\n...      .select(bit_length('cat')).collect()\n    [Row(bit_length(cat)=24), Row(bit_length(cat)=32)]"], "Parameters": [["col Column or str", "Source column or strings"]], "Returns": [["Column", "Bit length of the col"]], "Category": ["Functions"], "index": 165}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.btrim.html#pyspark.sql.functions.btrim"], "Title": ["btrim"], "Feature": ["btrim"], "Description": "Remove the leading and trailingtrimcharacters fromstr.", "Examples": [">>> df = spark.createDataFrame([(\"SSparkSQLS\", \"SL\", )], ['a', 'b'])\n>>> df.select(btrim(df.a, df.b).alias('r')).collect()\n[Row(r='parkSQ')]", ">>> df = spark.createDataFrame([(\"    SparkSQL   \",)], ['a'])\n>>> df.select(btrim(df.a).alias('r')).collect()\n[Row(r='SparkSQL')]"], "Parameters": [["str Column or str", "Input column or strings."], ["trim Column or str, optional", "The trim string characters to trim, the default value is a single space"]], "Returns": [], "Category": ["Functions"], "index": 166}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.char.html#pyspark.sql.functions.char"], "Title": ["char"], "Feature": ["char"], "Description": "Returns the ASCII character having the binary equivalent tocol. If col is larger than 256 the\nresult is equivalent to char(col % 256)", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.char(sf.lit(65))).show()\n+--------+\n|char(65)|\n+--------+\n|       A|\n+--------+"], "Parameters": [["col Column or str", "Input column or strings."]], "Returns": [], "Category": ["Functions"], "index": 167}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.dropDuplicates.html#pyspark.sql.DataFrame.dropDuplicates"], "Title": ["DataFrame.dropDuplicates"], "Feature": ["DataFrame.dropDuplicates"], "Description": "Return a newDataFramewith duplicate rows removed,\noptionally only considering certain columns.\nFor a static batchDataFrame, it just drops duplicate rows. For a streamingDataFrame, it will keep all data across triggers as intermediate state to drop\nduplicates rows. You can usewithWatermark()to limit how late the duplicate data can\nbe and the system will accordingly limit the state. In addition, data older than\nwatermark will be dropped to avoid any possibility of duplicates.\ndrop_duplicates()is an alias fordropDuplicates().", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([\n...     Row(name='Alice', age=5, height=80),\n...     Row(name='Alice', age=5, height=80),\n...     Row(name='Alice', age=10, height=80)\n... ])", ">>> df.dropDuplicates().show()\n+-----+---+------+\n| name|age|height|\n+-----+---+------+\n|Alice|  5|    80|\n|Alice| 10|    80|\n+-----+---+------+", ">>> df.dropDuplicates(['name', 'height']).show()\n+-----+---+------+\n| name|age|height|\n+-----+---+------+\n|Alice|  5|    80|\n+-----+---+------+"], "Parameters": [["subset list of column names, optional", "List of columns to use for duplicate comparison (default All columns)."]], "Returns": [["DataFrame", "DataFrame without duplicates."]], "Category": ["DataFrame"], "index": 168}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.char_length.html#pyspark.sql.functions.char_length"], "Title": ["char_length"], "Feature": ["char_length"], "Description": "Returns the character length of string data or number of bytes of binary data.\nThe length of string data includes the trailing spaces.\nThe length of binary data includes binary zeros.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.char_length(sf.lit(\"SparkSQL\"))).show()\n+---------------------+\n|char_length(SparkSQL)|\n+---------------------+\n|                    8|\n+---------------------+"], "Parameters": [["str Column or str", "Input column or strings."]], "Returns": [], "Category": ["Functions"], "index": 169}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.character_length.html#pyspark.sql.functions.character_length"], "Title": ["character_length"], "Feature": ["character_length"], "Description": "Returns the character length of string data or number of bytes of binary data.\nThe length of string data includes the trailing spaces.\nThe length of binary data includes binary zeros.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.character_length(sf.lit(\"SparkSQL\"))).show()\n+--------------------------+\n|character_length(SparkSQL)|\n+--------------------------+\n|                         8|\n+--------------------------+"], "Parameters": [["str Column or str", "Input column or strings."]], "Returns": [], "Category": ["Functions"], "index": 170}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collate.html#pyspark.sql.functions.collate"], "Title": ["collate"], "Feature": ["collate"], "Description": "Marks a given column with specified collation.", "Examples": [], "Parameters": [["col Column or str", "Target string column to work on."], ["collation str", "Target collation name."]], "Returns": [["Column", "A new column of string type, where each value has the specified collation."]], "Category": ["Functions"], "index": 171}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collation.html#pyspark.sql.functions.collation"], "Title": ["collation"], "Feature": ["collation"], "Description": "Returns the collation name of a given column.", "Examples": [">>> df = spark.createDataFrame([('name',)], ['dt'])\n>>> df.select(collation('dt').alias('collation')).show(truncate=False)\n+--------------------------+\n|collation                 |\n+--------------------------+\n|SYSTEM.BUILTIN.UTF8_BINARY|\n+--------------------------+"], "Parameters": [["col Column or str", "Target string column to work on."]], "Returns": [["Column", "collation name of a given expression."]], "Category": ["Functions"], "index": 172}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.concat_ws.html#pyspark.sql.functions.concat_ws"], "Title": ["concat_ws"], "Feature": ["concat_ws"], "Description": "Concatenates multiple input string columns together into a single string column,\nusing the given separator.\nSee alsopyspark.sql.functions.concat()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"abcd\", \"123\")], [\"s\", \"d\"])\n>>> df.select(\"*\", sf.concat_ws(\"-\", df.s, \"d\", sf.lit(\"xyz\"))).show()\n+----+---+-----------------------+\n|   s|  d|concat_ws(-, s, d, xyz)|\n+----+---+-----------------------+\n|abcd|123|           abcd-123-xyz|\n+----+---+-----------------------+"], "Parameters": [["sep literal string", "words separator."], ["cols Column or column name", "list of columns to work on."]], "Returns": [["Column", "string of concatenated words."]], "Category": ["Functions"], "index": 173}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.contains.html#pyspark.sql.functions.contains"], "Title": ["contains"], "Feature": ["contains"], "Description": "Returns a boolean. The value is True if right is found inside left.\nReturns NULL if either input expression is NULL. Otherwise, returns False.\nBoth left or right must be of STRING or BINARY type.", "Examples": [">>> df = spark.createDataFrame([(\"Spark SQL\", \"Spark\")], ['a', 'b'])\n>>> df.select(contains(df.a, df.b).alias('r')).collect()\n[Row(r=True)]", ">>> df = spark.createDataFrame([(\"414243\", \"4243\",)], [\"c\", \"d\"])\n>>> df = df.select(to_binary(\"c\").alias(\"c\"), to_binary(\"d\").alias(\"d\"))\n>>> df.printSchema()\nroot\n |-- c: binary (nullable = true)\n |-- d: binary (nullable = true)\n>>> df.select(contains(\"c\", \"d\"), contains(\"d\", \"c\")).show()\n+--------------+--------------+\n|contains(c, d)|contains(d, c)|\n+--------------+--------------+\n|          true|         false|\n+--------------+--------------+"], "Parameters": [["left Column or str", "The input column or strings to check, may be NULL."], ["right Column or str", "The input column or strings to find, may be NULL."]], "Returns": [], "Category": ["Functions"], "index": 174}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.decode.html#pyspark.sql.functions.decode"], "Title": ["decode"], "Feature": ["decode"], "Description": "Computes the first argument into a string from a binary using the provided character set\n(one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’, ‘UTF-32’).\nSee alsopyspark.sql.functions.encode()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(b\"abcd\",)], [\"a\"])\n>>> df.select(\"*\", sf.decode(\"a\", \"UTF-8\")).show()\n+-------------+----------------+\n|            a|decode(a, UTF-8)|\n+-------------+----------------+\n|[61 62 63 64]|            abcd|\n+-------------+----------------+"], "Parameters": [["col Column or column name", "target column to work on."], ["charset literal string", "charset to use to decode to."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 175}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.elt.html#pyspark.sql.functions.elt"], "Title": ["elt"], "Feature": ["elt"], "Description": "Returns then-th input, e.g., returnsinput2whennis 2.\nThe function returns NULL if the index exceeds the length of the array\nandspark.sql.ansi.enabledis set to false. Ifspark.sql.ansi.enabledis set to true,\nit throws ArrayIndexOutOfBoundsException for invalid indices.", "Examples": [">>> df = spark.createDataFrame([(1, \"scala\", \"java\")], ['a', 'b', 'c'])\n>>> df.select(elt(df.a, df.b, df.c).alias('r')).collect()\n[Row(r='scala')]"], "Parameters": [["inputs Column or str", "Input columns or strings."]], "Returns": [], "Category": ["Functions"], "index": 176}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.encode.html#pyspark.sql.functions.encode"], "Title": ["encode"], "Feature": ["encode"], "Description": "Computes the first argument into a binary from a string using the provided character set\n(one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’, ‘UTF-32’).\nSee alsopyspark.sql.functions.decode()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"abcd\",)], [\"c\"])\n>>> df.select(\"*\", sf.encode(\"c\", \"UTF-8\")).show()\n+----+----------------+\n|   c|encode(c, UTF-8)|\n+----+----------------+\n|abcd|   [61 62 63 64]|\n+----+----------------+"], "Parameters": [["col Column or column name", "target column to work on."], ["charset literal string", "charset to use to encode."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 177}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.endswith.html#pyspark.sql.functions.endswith"], "Title": ["endswith"], "Feature": ["endswith"], "Description": "Returns a boolean. The value is True if str ends with suffix.\nReturns NULL if either input expression is NULL. Otherwise, returns False.\nBoth str or suffix must be of STRING or BINARY type.", "Examples": [">>> df = spark.createDataFrame([(\"Spark SQL\", \"Spark\",)], [\"a\", \"b\"])\n>>> df.select(endswith(df.a, df.b).alias('r')).collect()\n[Row(r=False)]", ">>> df = spark.createDataFrame([(\"414243\", \"4243\",)], [\"e\", \"f\"])\n>>> df = df.select(to_binary(\"e\").alias(\"e\"), to_binary(\"f\").alias(\"f\"))\n>>> df.printSchema()\nroot\n |-- e: binary (nullable = true)\n |-- f: binary (nullable = true)\n>>> df.select(endswith(\"e\", \"f\"), endswith(\"f\", \"e\")).show()\n+--------------+--------------+\n|endswith(e, f)|endswith(f, e)|\n+--------------+--------------+\n|          true|         false|\n+--------------+--------------+"], "Parameters": [["str Column or str", "A column of string."], ["suffix Column or str", "A column of string, the suffix."]], "Returns": [], "Category": ["Functions"], "index": 178}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.dropDuplicatesWithinWatermark.html#pyspark.sql.DataFrame.dropDuplicatesWithinWatermark"], "Title": ["DataFrame.dropDuplicatesWithinWatermark"], "Feature": ["DataFrame.dropDuplicatesWithinWatermark"], "Description": "For a streamingDataFrame, this will keep all data across triggers as intermediate\nstate to drop duplicated rows. The state will be kept to guarantee the semantic, “Events\nare deduplicated as long as the time distance of earliest and latest events are smaller\nthan the delay threshold of watermark.” Users are encouraged to set the delay threshold of\nwatermark longer than max timestamp differences among duplicated events.\nNote: too late data older than watermark will be dropped.\nNotes\nSupports Spark Connect.", "Examples": [">>> from pyspark.sql import Row\n>>> from pyspark.sql.functions import timestamp_seconds\n>>> df = spark.readStream.format(\"rate\").load().selectExpr(\n...     \"value % 5 AS value\", \"timestamp\")\n>>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\nDataFrame[value: bigint, time: timestamp]", ">>> df.dropDuplicatesWithinWatermark()", ">>> df.dropDuplicatesWithinWatermark(['value'])"], "Parameters": [["subset List of column names, optional", "List of columns to use for duplicate comparison (default All columns)."]], "Returns": [["DataFrame", "DataFrame without duplicates."]], "Category": ["DataFrame"], "index": 179}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.find_in_set.html#pyspark.sql.functions.find_in_set"], "Title": ["find_in_set"], "Feature": ["find_in_set"], "Description": "Returns the index (1-based) of the given string (str) in the comma-delimited\nlist (strArray). Returns 0, if the string was not found or if the given string (str)\ncontains a comma.", "Examples": [">>> df = spark.createDataFrame([(\"ab\", \"abc,b,ab,c,def\")], ['a', 'b'])\n>>> df.select(find_in_set(df.a, df.b).alias('r')).collect()\n[Row(r=3)]"], "Parameters": [["str Column or str", "The given string to be found."], ["str_array Column or str", "The comma-delimited list."]], "Returns": [], "Category": ["Functions"], "index": 180}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.format_number.html#pyspark.sql.functions.format_number"], "Title": ["format_number"], "Feature": ["format_number"], "Description": "Formats the number X to a format like ‘#,–#,–#.–’, rounded to d decimal places\nwith HALF_EVEN round mode, and returns the result as a string.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(5,)], [\"a\"])\n>>> df.select(\"*\", sf.format_number(\"a\", 4), sf.format_number(df.a, 6)).show()\n+---+-------------------+-------------------+\n|  a|format_number(a, 4)|format_number(a, 6)|\n+---+-------------------+-------------------+\n|  5|             5.0000|           5.000000|\n+---+-------------------+-------------------+"], "Parameters": [["col Column or column name", "the column name of the numeric value to be formatted"], ["d int", "the N decimal places"]], "Returns": [["Column", "the column of formatted results."]], "Category": ["Functions"], "index": 181}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.format_string.html#pyspark.sql.functions.format_string"], "Title": ["format_string"], "Feature": ["format_string"], "Description": "Formats the arguments in printf-style and returns the result as a string column.\nSee alsopyspark.sql.functions.printf()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(5, \"hello\")], [\"a\", \"b\"])\n>>> df.select(\"*\", sf.format_string('%d %s', \"a\", df.b)).show()\n+---+-----+--------------------------+\n|  a|    b|format_string(%d %s, a, b)|\n+---+-----+--------------------------+\n|  5|hello|                   5 hello|\n+---+-----+--------------------------+"], "Parameters": [["format literal string", "string that can contain embedded format tags and used as result column’s value"], ["cols Column or column name", "column names or Column s to be used in formatting"]], "Returns": [["Column", "the column of formatted results."]], "Category": ["Functions"], "index": 182}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.initcap.html#pyspark.sql.functions.initcap"], "Title": ["initcap"], "Feature": ["initcap"], "Description": "Translate the first letter of each word to upper case in the sentence.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('ab cd',)], ['a'])\n>>> df.select(\"*\", sf.initcap(\"a\")).show()\n+-----+----------+\n|    a|initcap(a)|\n+-----+----------+\n|ab cd|     Ab Cd|\n+-----+----------+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "string with all first letters are uppercase in each word."]], "Category": ["Functions"], "index": 183}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.instr.html#pyspark.sql.functions.instr"], "Title": ["instr"], "Feature": ["instr"], "Description": "Locate the position of the first occurrence of substr column in the given string.\nReturns null if either of the arguments are null.\nSee alsopyspark.sql.functions.locate()pyspark.sql.functions.substr()pyspark.sql.functions.substring()pyspark.sql.functions.substring_index()\nNotes\nThe position is not zero based, but 1 based index. Returns 0 if substr\ncould not be found in str.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"abcd\",), (\"xyz\",)], [\"s\",])\n>>> df.select(\"*\", sf.instr(df.s, \"b\")).show()\n+----+-----------+\n|   s|instr(s, b)|\n+----+-----------+\n|abcd|          2|\n| xyz|          0|\n+----+-----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"abcd\",), (\"xyz\",)], [\"s\",])\n>>> df.select(\"*\", sf.instr(\"s\", sf.lit(\"abc\").substr(0, 2))).show()\n+----+---------------------------+\n|   s|instr(s, substr(abc, 0, 2))|\n+----+---------------------------+\n|abcd|                          1|\n| xyz|                          0|\n+----+---------------------------+"], "Parameters": [["str Column or column name", "target column to work on."], ["substr Column or literal string", "substring to look for. Changed in version 4.0.0: substr now accepts column."]], "Returns": [["Column", "location of the first occurrence of the substring as integer."]], "Category": ["Functions"], "index": 184}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.is_valid_utf8.html#pyspark.sql.functions.is_valid_utf8"], "Title": ["is_valid_utf8"], "Feature": ["is_valid_utf8"], "Description": "Returns true if the input is a valid UTF-8 string, otherwise returns false.\nSee alsopyspark.sql.functions.make_valid_utf8()pyspark.sql.functions.validate_utf8()pyspark.sql.functions.try_validate_utf8()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.is_valid_utf8(sf.lit(\"SparkSQL\"))).show()\n+-----------------------+\n|is_valid_utf8(SparkSQL)|\n+-----------------------+\n|                   true|\n+-----------------------+"], "Parameters": [["str Column or column name", "A column of strings, each representing a UTF-8 byte sequence."]], "Returns": [["Column", "whether the input string is a valid UTF-8 string."]], "Category": ["Functions"], "index": 185}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.lcase.html#pyspark.sql.functions.lcase"], "Title": ["lcase"], "Feature": ["lcase"], "Description": "Returnsstrwith all characters changed to lowercase.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.lcase(sf.lit(\"Spark\"))).show()\n+------------+\n|lcase(Spark)|\n+------------+\n|       spark|\n+------------+"], "Parameters": [["str Column or str", "Input column or strings."]], "Returns": [], "Category": ["Functions"], "index": 186}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.left.html#pyspark.sql.functions.left"], "Title": ["left"], "Feature": ["left"], "Description": "Returns the leftmostlen`(`lencan be string type) characters from the stringstr,\niflenis less or equal than 0 the result is an empty string.", "Examples": [">>> df = spark.createDataFrame([(\"Spark SQL\", 3,)], ['a', 'b'])\n>>> df.select(left(df.a, df.b).alias('r')).collect()\n[Row(r='Spa')]"], "Parameters": [["str Column or str", "Input column or strings."], ["len Column or str", "Input column or strings, the leftmost len ."]], "Returns": [], "Category": ["Functions"], "index": 187}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.length.html#pyspark.sql.functions.length"], "Title": ["length"], "Feature": ["length"], "Description": "Computes the character length of string data or number of bytes of binary data.\nThe length of character data includes the trailing spaces. The length of binary data\nincludes binary zeros.", "Examples": [">>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n[Row(length=4)]"], "Parameters": [["col Column or str", "target column to work on."]], "Returns": [["Column", "length of the value."]], "Category": ["Functions"], "index": 188}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.levenshtein.html#pyspark.sql.functions.levenshtein"], "Title": ["levenshtein"], "Feature": ["levenshtein"], "Description": "Computes the Levenshtein distance of the two given strings.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n>>> df.select('*', sf.levenshtein('l', 'r')).show()\n+------+-------+-----------------+\n|     l|      r|levenshtein(l, r)|\n+------+-------+-----------------+\n|kitten|sitting|                3|\n+------+-------+-----------------+", ">>> df.select('*', sf.levenshtein(df.l, df.r, 2)).show()\n+------+-------+--------------------+\n|     l|      r|levenshtein(l, r, 2)|\n+------+-------+--------------------+\n|kitten|sitting|                  -1|\n+------+-------+--------------------+"], "Parameters": [["left Column or column name", "first column value."], ["right Column or column name", "second column value."], ["threshold int, optional", "if set when the levenshtein distance of the two given strings\nless than or equal to a given threshold then return result distance, or -1"]], "Returns": [["Column", "Levenshtein distance as integer value."]], "Category": ["Functions"], "index": 189}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.drop_duplicates.html#pyspark.sql.DataFrame.drop_duplicates"], "Title": ["DataFrame.drop_duplicates"], "Feature": ["DataFrame.drop_duplicates"], "Description": "drop_duplicates()is an alias fordropDuplicates().", "Examples": [], "Parameters": [], "Returns": [], "Category": ["DataFrame"], "index": 190}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.locate.html#pyspark.sql.functions.locate"], "Title": ["locate"], "Feature": ["locate"], "Description": "Locate the position of the first occurrence of substr in a string column, after position pos.\nSee alsopyspark.sql.functions.instr()pyspark.sql.functions.substr()pyspark.sql.functions.substring()pyspark.sql.functions.substring_index()pyspark.sql.Column.substr()\nNotes\nThe position is not zero based, but 1 based index. Returns 0 if substr\ncould not be found in str.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('abcd',)], ['s',])\n>>> df.select('*', sf.locate('b', 's', 1)).show()\n+----+---------------+\n|   s|locate(b, s, 1)|\n+----+---------------+\n|abcd|              2|\n+----+---------------+", ">>> df.select('*', sf.locate('b', df.s, 3)).show()\n+----+---------------+\n|   s|locate(b, s, 3)|\n+----+---------------+\n|abcd|              0|\n+----+---------------+"], "Parameters": [["substr literal string", "a string"], ["str Column or column name", "a Column of pyspark.sql.types.StringType"], ["pos int, optional", "start position (zero based)"]], "Returns": [["Column", "position of the substring."]], "Category": ["Functions"], "index": 191}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.lower.html#pyspark.sql.functions.lower"], "Title": ["lower"], "Feature": ["lower"], "Description": "Converts a string expression to lower case.\nSee alsopyspark.sql.functions.upper()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n>>> df.select(\"*\", sf.lower(\"value\")).show()\n+----------+------------+\n|     value|lower(value)|\n+----------+------------+\n|     Spark|       spark|\n|   PySpark|     pyspark|\n|Pandas API|  pandas api|\n+----------+------------+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "lower case values."]], "Category": ["Functions"], "index": 192}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.lpad.html#pyspark.sql.functions.lpad"], "Title": ["lpad"], "Feature": ["lpad"], "Description": "Left-pad the string column to widthlenwithpad.\nSee alsopyspark.sql.functions.rpad()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('abcd',), ('xyz',), ('12',)], ['s',])\n>>> df.select(\"*\", sf.lpad(df.s, 6, '#')).show()\n+----+-------------+\n|   s|lpad(s, 6, #)|\n+----+-------------+\n|abcd|       ##abcd|\n| xyz|       ###xyz|\n|  12|       ####12|\n+----+-------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('abcd',), ('xyz',), ('12',)], ['s',])\n>>> df.select(\"*\", sf.lpad(df.s, 6, sf.lit(b\"uv\"))).show()\n+----+-------------------+\n|   s|lpad(s, 6, X'7576')|\n+----+-------------------+\n|abcd|             uvabcd|\n| xyz|             uvuxyz|\n|  12|             uvuv12|\n+----+-------------------+"], "Parameters": [["col Column or column name", "target column to work on."], ["len Column or int", "length of the final string. Changed in version 4.0.0: pattern now accepts column."], ["pad Column or literal string", "chars to prepend. Changed in version 4.0.0: pattern now accepts column."]], "Returns": [["Column", "left padded result."]], "Category": ["Functions"], "index": 193}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ltrim.html#pyspark.sql.functions.ltrim"], "Title": ["ltrim"], "Feature": ["ltrim"], "Description": "Trim the spaces from left end for the specified string value.\nSee alsopyspark.sql.functions.trim()pyspark.sql.functions.rtrim()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n>>> df.select(\"*\", sf.ltrim(\"value\")).show()\n+--------+------------+\n|   value|ltrim(value)|\n+--------+------------+\n|   Spark|       Spark|\n| Spark  |     Spark  |\n|   Spark|       Spark|\n+--------+------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\"***Spark\", \"Spark**\", \"*Spark\"], \"STRING\")\n>>> df.select(\"*\", sf.ltrim(\"value\", sf.lit(\"*\"))).show()\n+--------+--------------------------+\n|   value|TRIM(LEADING * FROM value)|\n+--------+--------------------------+\n|***Spark|                     Spark|\n| Spark**|                   Spark**|\n|  *Spark|                     Spark|\n+--------+--------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"**Spark*\", \"*\"), (\"==Spark=\", \"=\")], [\"value\", \"t\"])\n>>> df.select(\"*\", sf.ltrim(\"value\", \"t\")).show()\n+--------+---+--------------------------+\n|   value|  t|TRIM(LEADING t FROM value)|\n+--------+---+--------------------------+\n|**Spark*|  *|                    Spark*|\n|==Spark=|  =|                    Spark=|\n+--------+---+--------------------------+"], "Parameters": [["col Column or column name", "target column to work on."], ["trim Column or column name, optional", "The trim string characters to trim, the default value is a single space New in version 4.0.0."]], "Returns": [["Column", "left trimmed values."]], "Category": ["Functions"], "index": 194}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.make_valid_utf8.html#pyspark.sql.functions.make_valid_utf8"], "Title": ["make_valid_utf8"], "Feature": ["make_valid_utf8"], "Description": "Returns a new string in which all invalid UTF-8 byte sequences, if any, are replaced by the\nUnicode replacement character (U+FFFD).\nSee alsopyspark.sql.functions.is_valid_utf8()pyspark.sql.functions.validate_utf8()pyspark.sql.functions.try_validate_utf8()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.make_valid_utf8(sf.lit(\"SparkSQL\"))).show()\n+-------------------------+\n|make_valid_utf8(SparkSQL)|\n+-------------------------+\n|                 SparkSQL|\n+-------------------------+"], "Parameters": [["str Column or column name", "A column of strings, each representing a UTF-8 byte sequence."]], "Returns": [["Column", "the valid UTF-8 version of the given input string."]], "Category": ["Functions"], "index": 195}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.mask.html#pyspark.sql.functions.mask"], "Title": ["mask"], "Feature": ["mask"], "Description": "Masks the given string value. This can be useful for creating copies of tables with sensitive\ninformation removed.", "Examples": [">>> df = spark.createDataFrame([(\"AbCD123-@$#\",), (\"abcd-EFGH-8765-4321\",)], ['data'])\n>>> df.select(mask(df.data).alias('r')).collect()\n[Row(r='XxXXnnn-@$#'), Row(r='xxxx-XXXX-nnnn-nnnn')]\n>>> df.select(mask(df.data, lit('Y')).alias('r')).collect()\n[Row(r='YxYYnnn-@$#'), Row(r='xxxx-YYYY-nnnn-nnnn')]\n>>> df.select(mask(df.data, lit('Y'), lit('y')).alias('r')).collect()\n[Row(r='YyYYnnn-@$#'), Row(r='yyyy-YYYY-nnnn-nnnn')]\n>>> df.select(mask(df.data, lit('Y'), lit('y'), lit('d')).alias('r')).collect()\n[Row(r='YyYYddd-@$#'), Row(r='yyyy-YYYY-dddd-dddd')]\n>>> df.select(mask(df.data, lit('Y'), lit('y'), lit('d'), lit('*')).alias('r')).collect()\n[Row(r='YyYYddd****'), Row(r='yyyy*YYYY*dddd*dddd')]"], "Parameters": [["col: :class:`~pyspark.sql.Column` or str", "target column to compute on."], ["upperChar: :class:`~pyspark.sql.Column` or str, optional", "character to replace upper-case characters with. Specify NULL to retain original character."], ["lowerChar: :class:`~pyspark.sql.Column` or str, optional", "character to replace lower-case characters with. Specify NULL to retain original character."], ["digitChar: :class:`~pyspark.sql.Column` or str, optional", "character to replace digit characters with. Specify NULL to retain original character."], ["otherChar: :class:`~pyspark.sql.Column` or str, optional", "character to replace all other characters with. Specify NULL to retain original character."]], "Returns": [["Column", ""]], "Category": ["Functions"], "index": 196}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.octet_length.html#pyspark.sql.functions.octet_length"], "Title": ["octet_length"], "Feature": ["octet_length"], "Description": "Calculates the byte length for the specified string column.", "Examples": [">>> from pyspark.sql.functions import octet_length\n>>> spark.createDataFrame([('cat',), ( '🐈',)], ['cat']) \\\n...      .select(octet_length('cat')).collect()\n    [Row(octet_length(cat)=3), Row(octet_length(cat)=4)]"], "Parameters": [["col Column or str", "Source column or strings"]], "Returns": [["Column", "Byte length of the col"]], "Category": ["Functions"], "index": 197}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.overlay.html#pyspark.sql.functions.overlay"], "Title": ["overlay"], "Feature": ["overlay"], "Description": "Overlay the specified portion ofsrcwithreplace,\nstarting from byte positionposofsrcand proceeding forlenbytes.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"SPARK_SQL\", \"CORE\")], (\"x\", \"y\"))\n>>> df.select(\"*\", sf.overlay(\"x\", df.y, 7)).show()\n+---------+----+--------------------+\n|        x|   y|overlay(x, y, 7, -1)|\n+---------+----+--------------------+\n|SPARK_SQL|CORE|          SPARK_CORE|\n+---------+----+--------------------+", ">>> df.select(\"*\", sf.overlay(\"x\", df.y, 7, 0)).show()\n+---------+----+-------------------+\n|        x|   y|overlay(x, y, 7, 0)|\n+---------+----+-------------------+\n|SPARK_SQL|CORE|      SPARK_CORESQL|\n+---------+----+-------------------+", ">>> df.select(\"*\", sf.overlay(\"x\", \"y\", 7, 2)).show()\n+---------+----+-------------------+\n|        x|   y|overlay(x, y, 7, 2)|\n+---------+----+-------------------+\n|SPARK_SQL|CORE|        SPARK_COREL|\n+---------+----+-------------------+"], "Parameters": [["src Column or column name", "the string that will be replaced"], ["replace Column or column name", "the substitution string"], ["pos Column or column name or int", "the starting position in src"], ["len Column or column name or int, optional", "the number of bytes to replace in src\nstring by ‘replace’ defaults to -1, which represents the length of the ‘replace’ string"]], "Returns": [["Column", "string with replaced values."]], "Category": ["Functions"], "index": 198}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.position.html#pyspark.sql.functions.position"], "Title": ["position"], "Feature": ["position"], "Description": "Returns the position of the first occurrence ofsubstrinstrafter positionstart.\nThe givenstartand return value are 1-based.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(\"bar\", \"foobarbar\", 5,)], [\"a\", \"b\", \"c\"]\n... ).select(sf.position(\"a\", \"b\", \"c\")).show()\n+-----------------+\n|position(a, b, c)|\n+-----------------+\n|                7|\n+-----------------+", ">>> spark.createDataFrame(\n...     [(\"bar\", \"foobarbar\", 5,)], [\"a\", \"b\", \"c\"]\n... ).select(sf.position(\"a\", \"b\")).show()\n+-----------------+\n|position(a, b, 1)|\n+-----------------+\n|                4|\n+-----------------+"], "Parameters": [["substr Column or str", "A column of string, substring."], ["str Column or str", "A column of string."], ["start Column or str, optional", "A column of string, start position."]], "Returns": [], "Category": ["Functions"], "index": 199}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.printf.html#pyspark.sql.functions.printf"], "Title": ["printf"], "Feature": ["printf"], "Description": "Formats the arguments in printf-style and returns the result as a string column.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(\"aa%d%s\", 123, \"cc\",)], [\"a\", \"b\", \"c\"]\n... ).select(sf.printf(\"a\", \"b\", \"c\")).show()\n+---------------+\n|printf(a, b, c)|\n+---------------+\n|        aa123cc|\n+---------------+"], "Parameters": [["format Column or str", "string that can contain embedded format tags and used as result column’s value"], ["cols Column or str", "column names or Column s to be used in formatting"]], "Returns": [], "Category": ["Functions"], "index": 200}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.dropna.html#pyspark.sql.DataFrame.dropna"], "Title": ["DataFrame.dropna"], "Feature": ["DataFrame.dropna"], "Description": "Returns a newDataFrameomitting rows with null or NaN values.DataFrame.dropna()andDataFrameNaFunctions.drop()are\naliases of each other.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([\n...     Row(age=10, height=80.0, name=\"Alice\"),\n...     Row(age=5, height=float(\"nan\"), name=\"Bob\"),\n...     Row(age=None, height=None, name=\"Tom\"),\n...     Row(age=None, height=float(\"nan\"), name=None),\n... ])", ">>> df.na.drop().show()\n+---+------+-----+\n|age|height| name|\n+---+------+-----+\n| 10|  80.0|Alice|\n+---+------+-----+", ">>> df.na.drop(how='all').show()\n+----+------+-----+\n| age|height| name|\n+----+------+-----+\n|  10|  80.0|Alice|\n|   5|   NaN|  Bob|\n|NULL|  NULL|  Tom|\n+----+------+-----+", ">>> df.na.drop(thresh=2).show()\n+---+------+-----+\n|age|height| name|\n+---+------+-----+\n| 10|  80.0|Alice|\n|  5|   NaN|  Bob|\n+---+------+-----+", ">>> df.na.drop(subset=['age', 'name']).show()\n+---+------+-----+\n|age|height| name|\n+---+------+-----+\n| 10|  80.0|Alice|\n|  5|   NaN|  Bob|\n+---+------+-----+"], "Parameters": [["how str, optional, the values that can be ‘any’ or ‘all’, default ‘any’.", "If ‘any’, drop a row if it contains any nulls.\nIf ‘all’, drop a row only if all its values are null."], ["thresh: int, optional, default None.", "If specified, drop rows that have less than thresh non-null values.\nThis overwrites the how parameter."], ["subset str, tuple or list, optional", "optional list of column names to consider."]], "Returns": [["DataFrame", "DataFrame with null only rows excluded."]], "Category": ["DataFrame"], "index": 201}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.randstr.html#pyspark.sql.functions.randstr"], "Title": ["randstr"], "Feature": ["randstr"], "Description": "Returns a string of the specified length whose characters are chosen uniformly at random from\nthe following pool of characters: 0-9, a-z, A-Z. The random seed is optional. The string length\nmust be a constant two-byte or four-byte integer (SMALLINT or INT, respectively).\nSee alsopyspark.sql.functions.rand()pyspark.sql.functions.randn()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(0, 10, 1, 1).select(sf.randstr(16, 3)).show()\n+----------------+\n|  randstr(16, 3)|\n+----------------+\n|nurJIpH4cmmMnsCG|\n|fl9YtT5m01trZtIt|\n|PD19rAgscTHS7qQZ|\n|2CuAICF5UJOruVv4|\n|kNZEs8nDpJEoz3Rl|\n|OXiU0KN5eaXfjXFs|\n|qfnTM1BZAHtN0gBV|\n|1p8XiSKwg33KnRPK|\n|od5y5MucayQq1bKK|\n|tklYPmKmc5sIppWM|\n+----------------+"], "Parameters": [["length Column or int", "Number of characters in the string to generate."], ["seed Column or int", "Optional random number seed to use."]], "Returns": [["Column", "The generated random string with the specified length."]], "Category": ["Functions"], "index": 202}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_count.html#pyspark.sql.functions.regexp_count"], "Title": ["regexp_count"], "Feature": ["regexp_count"], "Description": "Returns a count of the number of times that the Java regex patternregexpis matched\nin the stringstr.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+\")], [\"str\", \"regexp\"])\n>>> df.select('*', sf.regexp_count('str', sf.lit(r'\\d+'))).show()\n+---------+------+----------------------+\n|      str|regexp|regexp_count(str, \\d+)|\n+---------+------+----------------------+\n|1a 2b 14m|   \\d+|                     3|\n+---------+------+----------------------+", ">>> df.select('*', sf.regexp_count('str', sf.lit(r'mmm'))).show()\n+---------+------+----------------------+\n|      str|regexp|regexp_count(str, mmm)|\n+---------+------+----------------------+\n|1a 2b 14m|   \\d+|                     0|\n+---------+------+----------------------+", ">>> df.select('*', sf.regexp_count(\"str\", sf.col(\"regexp\"))).show()\n+---------+------+-------------------------+\n|      str|regexp|regexp_count(str, regexp)|\n+---------+------+-------------------------+\n|1a 2b 14m|   \\d+|                        3|\n+---------+------+-------------------------+", ">>> df.select('*', sf.regexp_count(sf.col('str'), \"regexp\")).show()\n+---------+------+-------------------------+\n|      str|regexp|regexp_count(str, regexp)|\n+---------+------+-------------------------+\n|1a 2b 14m|   \\d+|                        3|\n+---------+------+-------------------------+"], "Parameters": [["str Column or column name", "target column to work on."], ["regexp Column or column name", "regex pattern to apply."]], "Returns": [["Column", "the number of times that a Java regex pattern is matched in the string."]], "Category": ["Functions"], "index": 203}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_extract.html#pyspark.sql.functions.regexp_extract"], "Title": ["regexp_extract"], "Feature": ["regexp_extract"], "Description": "Extract a specific group matched by the Java regexregexp, from the specified string column.\nIf the regex did not match, or the specified group did not match, an empty string is returned.\nSee alsopyspark.sql.functions.regexp_extract_all()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('100-200',)], ['str'])\n>>> df.select('*', sf.regexp_extract('str', r'(\\d+)-(\\d+)', 1)).show()\n+-------+-----------------------------------+\n|    str|regexp_extract(str, (\\d+)-(\\d+), 1)|\n+-------+-----------------------------------+\n|100-200|                                100|\n+-------+-----------------------------------+", ">>> df = spark.createDataFrame([('foo',)], ['str'])\n>>> df.select('*', sf.regexp_extract('str', r'(\\d+)', 1)).show()\n+---+-----------------------------+\n|str|regexp_extract(str, (\\d+), 1)|\n+---+-----------------------------+\n|foo|                             |\n+---+-----------------------------+", ">>> df = spark.createDataFrame([('aaaac',)], ['str'])\n>>> df.select('*', sf.regexp_extract(sf.col('str'), '(a+)(b)?(c)', 2)).show()\n+-----+-----------------------------------+\n|  str|regexp_extract(str, (a+)(b)?(c), 2)|\n+-----+-----------------------------------+\n|aaaac|                                   |\n+-----+-----------------------------------+"], "Parameters": [["str Column or column name", "target column to work on."], ["pattern str", "regex pattern to apply."], ["idx int", "matched group id."]], "Returns": [["Column", "matched value specified by idx group id."]], "Category": ["Functions"], "index": 204}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_extract_all.html#pyspark.sql.functions.regexp_extract_all"], "Title": ["regexp_extract_all"], "Feature": ["regexp_extract_all"], "Description": "Extract all strings in thestrthat match the Java regexregexpand corresponding to the regex group index.\nSee alsopyspark.sql.functions.regexp_extract()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"100-200, 300-400\", r\"(\\d+)-(\\d+)\")], [\"str\", \"regexp\"])\n>>> df.select('*', sf.regexp_extract_all('str', sf.lit(r'(\\d+)-(\\d+)'))).show()\n+----------------+-----------+---------------------------------------+\n|             str|     regexp|regexp_extract_all(str, (\\d+)-(\\d+), 1)|\n+----------------+-----------+---------------------------------------+\n|100-200, 300-400|(\\d+)-(\\d+)|                             [100, 300]|\n+----------------+-----------+---------------------------------------+", ">>> df.select('*', sf.regexp_extract_all('str', sf.lit(r'(\\d+)-(\\d+)'), sf.lit(1))).show()\n+----------------+-----------+---------------------------------------+\n|             str|     regexp|regexp_extract_all(str, (\\d+)-(\\d+), 1)|\n+----------------+-----------+---------------------------------------+\n|100-200, 300-400|(\\d+)-(\\d+)|                             [100, 300]|\n+----------------+-----------+---------------------------------------+", ">>> df.select('*', sf.regexp_extract_all('str', sf.lit(r'(\\d+)-(\\d+)'), 2)).show()\n+----------------+-----------+---------------------------------------+\n|             str|     regexp|regexp_extract_all(str, (\\d+)-(\\d+), 2)|\n+----------------+-----------+---------------------------------------+\n|100-200, 300-400|(\\d+)-(\\d+)|                             [200, 400]|\n+----------------+-----------+---------------------------------------+", ">>> df.select('*', sf.regexp_extract_all('str', sf.col(\"regexp\"))).show()\n+----------------+-----------+----------------------------------+\n|             str|     regexp|regexp_extract_all(str, regexp, 1)|\n+----------------+-----------+----------------------------------+\n|100-200, 300-400|(\\d+)-(\\d+)|                        [100, 300]|\n+----------------+-----------+----------------------------------+", ">>> df.select('*', sf.regexp_extract_all(sf.col('str'), \"regexp\")).show()\n+----------------+-----------+----------------------------------+\n|             str|     regexp|regexp_extract_all(str, regexp, 1)|\n+----------------+-----------+----------------------------------+\n|100-200, 300-400|(\\d+)-(\\d+)|                        [100, 300]|\n+----------------+-----------+----------------------------------+"], "Parameters": [["str Column or column name", "target column to work on."], ["regexp Column or column name", "regex pattern to apply."], ["idx Column or int, optional", "matched group id."]], "Returns": [["Column", "all strings in the str that match a Java regex and corresponding to the regex group index."]], "Category": ["Functions"], "index": 205}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_instr.html#pyspark.sql.functions.regexp_instr"], "Title": ["regexp_instr"], "Feature": ["regexp_instr"], "Description": "Returns the position of the first substring in thestrthat match the Java regexregexpand corresponding to the regex group index.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+(a|b|m)\")], [\"str\", \"regexp\"])", ">>> df.select('*', sf.regexp_instr('str', sf.lit(r'\\d+(a|b|m)'))).show()\n+---------+----------+--------------------------------+\n|      str|    regexp|regexp_instr(str, \\d+(a|b|m), 0)|\n+---------+----------+--------------------------------+\n|1a 2b 14m|\\d+(a|b|m)|                               1|\n+---------+----------+--------------------------------+", ">>> df.select('*', sf.regexp_instr('str', sf.lit(r'\\d+(a|b|m)'), sf.lit(1))).show()\n+---------+----------+--------------------------------+\n|      str|    regexp|regexp_instr(str, \\d+(a|b|m), 1)|\n+---------+----------+--------------------------------+\n|1a 2b 14m|\\d+(a|b|m)|                               1|\n+---------+----------+--------------------------------+", ">>> df.select('*', sf.regexp_instr('str', sf.col(\"regexp\"))).show()\n+---------+----------+----------------------------+\n|      str|    regexp|regexp_instr(str, regexp, 0)|\n+---------+----------+----------------------------+\n|1a 2b 14m|\\d+(a|b|m)|                           1|\n+---------+----------+----------------------------+", ">>> df.select('*', sf.regexp_instr(sf.col(\"str\"), \"regexp\")).show()\n+---------+----------+----------------------------+\n|      str|    regexp|regexp_instr(str, regexp, 0)|\n+---------+----------+----------------------------+\n|1a 2b 14m|\\d+(a|b|m)|                           1|\n+---------+----------+----------------------------+"], "Parameters": [["str Column or column name", "target column to work on."], ["regexp Column or column name", "regex pattern to apply."], ["idx Column or int, optional", "matched group id."]], "Returns": [["Column", "the position of the first substring in the str that match a Java regex and corresponding\nto the regex group index."]], "Category": ["Functions"], "index": 206}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_replace.html#pyspark.sql.functions.regexp_replace"], "Title": ["regexp_replace"], "Feature": ["regexp_replace"], "Description": "Replace all substrings of the specified string value that match regexp with replacement.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...      [(\"100-200\", r\"(\\d+)\", \"--\")],\n...      [\"str\", \"pattern\", \"replacement\"]\n... )", ">>> df.select('*', sf.regexp_replace('str', r'(\\d+)', '--')).show()\n+-------+-------+-----------+---------------------------------+\n|    str|pattern|replacement|regexp_replace(str, (\\d+), --, 1)|\n+-------+-------+-----------+---------------------------------+\n|100-200|  (\\d+)|         --|                            -----|\n+-------+-------+-----------+---------------------------------+", ">>> df.select('*', \\\n...     sf.regexp_replace(sf.col(\"str\"), sf.col(\"pattern\"), sf.col(\"replacement\")) \\\n... ).show()\n+-------+-------+-----------+--------------------------------------------+\n|    str|pattern|replacement|regexp_replace(str, pattern, replacement, 1)|\n+-------+-------+-----------+--------------------------------------------+\n|100-200|  (\\d+)|         --|                                       -----|\n+-------+-------+-----------+--------------------------------------------+"], "Parameters": [["string Column or str", "column name or column containing the string value"], ["pattern Column or str", "column object or str containing the regexp pattern"], ["replacement Column or str", "column object or str containing the replacement"]], "Returns": [["Column", "string with all substrings replaced."]], "Category": ["Functions"], "index": 207}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_substr.html#pyspark.sql.functions.regexp_substr"], "Title": ["regexp_substr"], "Feature": ["regexp_substr"], "Description": "Returns the first substring that matches the Java regexregexpwithin the stringstr.\nIf the regular expression is not found, the result is null.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+\")], [\"str\", \"regexp\"])", ">>> df.select('*', sf.regexp_substr('str', sf.lit(r'\\d+'))).show()\n+---------+------+-----------------------+\n|      str|regexp|regexp_substr(str, \\d+)|\n+---------+------+-----------------------+\n|1a 2b 14m|   \\d+|                      1|\n+---------+------+-----------------------+", ">>> df.select('*', sf.regexp_substr('str', sf.lit(r'mmm'))).show()\n+---------+------+-----------------------+\n|      str|regexp|regexp_substr(str, mmm)|\n+---------+------+-----------------------+\n|1a 2b 14m|   \\d+|                   NULL|\n+---------+------+-----------------------+", ">>> df.select('*', sf.regexp_substr(\"str\", sf.col(\"regexp\"))).show()\n+---------+------+--------------------------+\n|      str|regexp|regexp_substr(str, regexp)|\n+---------+------+--------------------------+\n|1a 2b 14m|   \\d+|                         1|\n+---------+------+--------------------------+", ">>> df.select('*', sf.regexp_substr(sf.col(\"str\"), \"regexp\")).show()\n+---------+------+--------------------------+\n|      str|regexp|regexp_substr(str, regexp)|\n+---------+------+--------------------------+\n|1a 2b 14m|   \\d+|                         1|\n+---------+------+--------------------------+"], "Parameters": [["str Column or column name", "target column to work on."], ["regexp Column or column name", "regex pattern to apply."]], "Returns": [["Column", "the first substring that matches a Java regex within the string str ."]], "Category": ["Functions"], "index": 208}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.repeat.html#pyspark.sql.functions.repeat"], "Title": ["repeat"], "Feature": ["repeat"], "Description": "Repeats a string column n times, and returns it as a new string column.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('ab',)], ['s',])\n>>> df.select(\"*\", sf.repeat(\"s\", 3)).show()\n+---+------------+\n|  s|repeat(s, 3)|\n+---+------------+\n| ab|      ababab|\n+---+------------+", ">>> df.select(\"*\", sf.repeat(df.s, sf.lit(4))).show()\n+---+------------+\n|  s|repeat(s, 4)|\n+---+------------+\n| ab|    abababab|\n+---+------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('ab', 5,), ('abc', 6,)], ['s', 't'])\n>>> df.select(\"*\", sf.repeat(\"s\", \"t\")).show()\n+---+---+------------------+\n|  s|  t|      repeat(s, t)|\n+---+---+------------------+\n| ab|  5|        ababababab|\n|abc|  6|abcabcabcabcabcabc|\n+---+---+------------------+"], "Parameters": [["col Column or column name", "target column to work on."], ["n Column or column name or int", "number of times to repeat value. Changed in version 4.0.0: n now accepts column and column name."]], "Returns": [["Column", "string with repeated values."]], "Category": ["Functions"], "index": 209}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.replace.html#pyspark.sql.functions.replace"], "Title": ["replace"], "Feature": ["replace"], "Description": "Replaces all occurrences ofsearchwithreplace.", "Examples": [">>> df = spark.createDataFrame([(\"ABCabc\", \"abc\", \"DEF\",)], [\"a\", \"b\", \"c\"])\n>>> df.select(replace(df.a, df.b, df.c).alias('r')).collect()\n[Row(r='ABCDEF')]", ">>> df.select(replace(df.a, df.b).alias('r')).collect()\n[Row(r='ABC')]"], "Parameters": [["src Column or str", "A column of string to be replaced."], ["search Column or str", "A column of string, If search is not found in str , str is returned unchanged."], ["replace Column or str, optional", "A column of string, If replace is not specified or is an empty string,\nnothing replaces the string that is removed from str ."]], "Returns": [], "Category": ["Functions"], "index": 210}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.right.html#pyspark.sql.functions.right"], "Title": ["right"], "Feature": ["right"], "Description": "Returns the rightmostlen`(`lencan be string type) characters from the stringstr,\niflenis less or equal than 0 the result is an empty string.", "Examples": [">>> df = spark.createDataFrame([(\"Spark SQL\", 3,)], ['a', 'b'])\n>>> df.select(right(df.a, df.b).alias('r')).collect()\n[Row(r='SQL')]"], "Parameters": [["str Column or str", "Input column or strings."], ["len Column or str", "Input column or strings, the rightmost len ."]], "Returns": [], "Category": ["Functions"], "index": 211}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.dtypes.html#pyspark.sql.DataFrame.dtypes"], "Title": ["DataFrame.dtypes"], "Feature": ["DataFrame.dtypes"], "Description": "Returns all column names and their data types as a list.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.dtypes\n[('age', 'bigint'), ('name', 'string')]"], "Parameters": [], "Returns": [["list", "List of columns as tuple pairs."]], "Category": ["DataFrame"], "index": 212}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.rpad.html#pyspark.sql.functions.rpad"], "Title": ["rpad"], "Feature": ["rpad"], "Description": "Right-pad the string column to widthlenwithpad.\nSee alsopyspark.sql.functions.lpad()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('abcd',), ('xyz',), ('12',)], ['s',])\n>>> df.select(\"*\", sf.rpad(df.s, 6, '#')).show()\n+----+-------------+\n|   s|rpad(s, 6, #)|\n+----+-------------+\n|abcd|       abcd##|\n| xyz|       xyz###|\n|  12|       12####|\n+----+-------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('abcd',), ('xyz',), ('12',)], ['s',])\n>>> df.select(\"*\", sf.rpad(df.s, 6, sf.lit(b\"uv\"))).show()\n+----+-------------------+\n|   s|rpad(s, 6, X'7576')|\n+----+-------------------+\n|abcd|             abcduv|\n| xyz|             xyzuvu|\n|  12|             12uvuv|\n+----+-------------------+"], "Parameters": [["col Column or str", "target column to work on."], ["len Column or int", "length of the final string. Changed in version 4.0.0: pattern now accepts column."], ["pad Column or literal string", "chars to prepend. Changed in version 4.0.0: pattern now accepts column."]], "Returns": [["Column", "right padded result."]], "Category": ["Functions"], "index": 213}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.rtrim.html#pyspark.sql.functions.rtrim"], "Title": ["rtrim"], "Feature": ["rtrim"], "Description": "Trim the spaces from right end for the specified string value.\nSee alsopyspark.sql.functions.trim()pyspark.sql.functions.ltrim()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n>>> df.select(\"*\", sf.rtrim(\"value\")).show()\n+--------+------------+\n|   value|rtrim(value)|\n+--------+------------+\n|   Spark|       Spark|\n| Spark  |       Spark|\n|   Spark|       Spark|\n+--------+------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\"***Spark\", \"Spark**\", \"*Spark\"], \"STRING\")\n>>> df.select(\"*\", sf.rtrim(\"value\", sf.lit(\"*\"))).show()\n+--------+---------------------------+\n|   value|TRIM(TRAILING * FROM value)|\n+--------+---------------------------+\n|***Spark|                   ***Spark|\n| Spark**|                      Spark|\n|  *Spark|                     *Spark|\n+--------+---------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"**Spark*\", \"*\"), (\"==Spark=\", \"=\")], [\"value\", \"t\"])\n>>> df.select(\"*\", sf.rtrim(\"value\", \"t\")).show()\n+--------+---+---------------------------+\n|   value|  t|TRIM(TRAILING t FROM value)|\n+--------+---+---------------------------+\n|**Spark*|  *|                    **Spark|\n|==Spark=|  =|                    ==Spark|\n+--------+---+---------------------------+"], "Parameters": [["col Column or column name", "target column to work on."], ["trim Column or column name, optional", "The trim string characters to trim, the default value is a single space New in version 4.0.0."]], "Returns": [["Column", "right trimmed values."]], "Category": ["Functions"], "index": 214}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sentences.html#pyspark.sql.functions.sentences"], "Title": ["sentences"], "Feature": ["sentences"], "Description": "Splits a string into arrays of sentences, where each sentence is an array of words.\nThelanguageandcountryarguments are optional,\nWhen they are omitted:\n1.If they are both omitted, theLocale.ROOT - locale(language=’’, country=’’)is used.\nTheLocale.ROOTis regarded as the base locale of all locales, and is used as the\nlanguage/country neutral locale for the locale sensitive operations.\n2.If thecountryis omitted, thelocale(language, country=’’)is used.\nWhen they are null:\n1.If they are bothnull, theLocale.US - locale(language=’en’, country=’US’)is used.\n2.If thelanguageis null and thecountryis not null,\ntheLocale.US - locale(language=’en’, country=’US’)is used.\n3.If thelanguageis not null and thecountryis null, thelocale(language)is used.\n4.If neither isnull, thelocale(language, country)is used.\nSee alsopyspark.sql.functions.split()pyspark.sql.functions.split_part()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"This is an example sentence.\", )], [\"s\"])\n>>> df.select(\"*\", sf.sentences(df.s, sf.lit(\"en\"), sf.lit(\"US\"))).show(truncate=False)\n+----------------------------+-----------------------------------+\n|s                           |sentences(s, en, US)               |\n+----------------------------+-----------------------------------+\n|This is an example sentence.|[[This, is, an, example, sentence]]|\n+----------------------------+-----------------------------------+", ">>> df.select(\"*\", sf.sentences(df.s, sf.lit(\"en\"))).show(truncate=False)\n+----------------------------+-----------------------------------+\n|s                           |sentences(s, en, )                 |\n+----------------------------+-----------------------------------+\n|This is an example sentence.|[[This, is, an, example, sentence]]|\n+----------------------------+-----------------------------------+", ">>> df.select(\"*\", sf.sentences(df.s)).show(truncate=False)\n+----------------------------+-----------------------------------+\n|s                           |sentences(s, , )                   |\n+----------------------------+-----------------------------------+\n|This is an example sentence.|[[This, is, an, example, sentence]]|\n+----------------------------+-----------------------------------+"], "Parameters": [["string Column or column name", "a string to be split"], ["language Column or column name, optional", "a language of the locale"], ["country Column or column name, optional", "a country of the locale"]], "Returns": [["Column", "arrays of split sentences."]], "Category": ["Functions"], "index": 215}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.soundex.html#pyspark.sql.functions.soundex"], "Title": ["soundex"], "Feature": ["soundex"], "Description": "Returns the SoundEx encoding for a string", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], [\"s\"])\n>>> df.select(\"*\", sf.soundex(\"s\")).show()\n+-------+----------+\n|      s|soundex(s)|\n+-------+----------+\n| Peters|      P362|\n|Uhrbach|      U612|\n+-------+----------+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "SoundEx encoded string."]], "Category": ["Functions"], "index": 216}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.split.html#pyspark.sql.functions.split"], "Title": ["split"], "Feature": ["split"], "Description": "Splits str around matches of the given pattern.\nSee alsopyspark.sql.functions.sentences()pyspark.sql.functions.split_part()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n>>> df.select('*', sf.split(df.s, '[ABC]')).show()\n+--------------+-------------------+\n|             s|split(s, [ABC], -1)|\n+--------------+-------------------+\n|oneAtwoBthreeC|[one, two, three, ]|\n+--------------+-------------------+", ">>> df.select('*', sf.split(df.s, '[ABC]', 2)).show()\n+--------------+------------------+\n|             s|split(s, [ABC], 2)|\n+--------------+------------------+\n|oneAtwoBthreeC| [one, twoBthreeC]|\n+--------------+------------------+", ">>> df.select('*', sf.split('s', '[ABC]', -2)).show()\n+--------------+-------------------+\n|             s|split(s, [ABC], -2)|\n+--------------+-------------------+\n|oneAtwoBthreeC|[one, two, three, ]|\n+--------------+-------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([\n...     ('oneAtwoBthreeC', '[ABC]', 2),\n...     ('1A2B3C', '[1-9]+', 1),\n...     ('aa2bb3cc4', '[1-9]+', -1)], ['s', 'p', 'l'])\n>>> df.select('*', sf.split(df.s, df.p)).show()\n+--------------+------+---+-------------------+\n|             s|     p|  l|    split(s, p, -1)|\n+--------------+------+---+-------------------+\n|oneAtwoBthreeC| [ABC]|  2|[one, two, three, ]|\n|        1A2B3C|[1-9]+|  1|        [, A, B, C]|\n|     aa2bb3cc4|[1-9]+| -1|     [aa, bb, cc, ]|\n+--------------+------+---+-------------------+", ">>> df.select(sf.split('s', df.p, 'l')).show()\n+-----------------+\n|   split(s, p, l)|\n+-----------------+\n|[one, twoBthreeC]|\n|         [1A2B3C]|\n|   [aa, bb, cc, ]|\n+-----------------+"], "Parameters": [["str Column or column name", "a string expression to split"], ["pattern Column or literal string", "a string representing a regular expression. The regex string should be\na Java regular expression. Changed in version 4.0.0: pattern now accepts column. Does not accept column name since string type remain\naccepted as a regular expression representation, for backwards compatibility.\nIn addition to int, limit now accepts column and column name."], ["limit Column or column name or int", "an integer which controls the number of times pattern is applied. limit > 0 : The resulting array’s length will not be more than limit , and the resulting array’s last entry will contain all input beyond the last\nmatched pattern. limit <= 0 : pattern will be applied as many times as possible, and the resulting array can be of any size. Changed in version 3.0: split now takes an optional limit field. If not provided, default limit value is -1."], ["limit > 0 : The resulting array’s length will not be more than limit , and the", "resulting array’s last entry will contain all input beyond the last\nmatched pattern."], ["limit <= 0 : pattern will be applied as many times as possible, and the resulting", "array can be of any size."]], "Returns": [["Column", "array of separated strings."]], "Category": ["Functions"], "index": 217}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.split_part.html#pyspark.sql.functions.split_part"], "Title": ["split_part"], "Feature": ["split_part"], "Description": "Splitsstrby delimiter and return requested part of the split (1-based).\nIf any input is null, returns null. ifpartNumis out of range of split parts,\nreturns empty string. IfpartNumis 0, throws an error. IfpartNumis negative,\nthe parts are counted backward from the end of the string.\nIf thedelimiteris an empty string, thestris not split.\nSee alsopyspark.sql.functions.sentences()pyspark.sql.functions.split()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"11.12.13\", \".\", 3,)], [\"a\", \"b\", \"c\"])\n>>> df.select(\"*\", sf.split_part(\"a\", \"b\", \"c\")).show()\n+--------+---+---+-------------------+\n|       a|  b|  c|split_part(a, b, c)|\n+--------+---+---+-------------------+\n|11.12.13|  .|  3|                 13|\n+--------+---+---+-------------------+", ">>> df.select(\"*\", sf.split_part(df.a, df.b, sf.lit(-2))).show()\n+--------+---+---+--------------------+\n|       a|  b|  c|split_part(a, b, -2)|\n+--------+---+---+--------------------+\n|11.12.13|  .|  3|                  12|\n+--------+---+---+--------------------+"], "Parameters": [["src Column or column name", "A column of string to be split."], ["delimiter Column or column name", "A column of string, the delimiter used for split."], ["partNum Column or column name", "A column of string, requested part of the split (1-based)."]], "Returns": [], "Category": ["Functions"], "index": 218}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.startswith.html#pyspark.sql.functions.startswith"], "Title": ["startswith"], "Feature": ["startswith"], "Description": "Returns a boolean. The value is True if str starts with prefix.\nReturns NULL if either input expression is NULL. Otherwise, returns False.\nBoth str or prefix must be of STRING or BINARY type.", "Examples": [">>> df = spark.createDataFrame([(\"Spark SQL\", \"Spark\",)], [\"a\", \"b\"])\n>>> df.select(startswith(df.a, df.b).alias('r')).collect()\n[Row(r=True)]", ">>> df = spark.createDataFrame([(\"414243\", \"4142\",)], [\"e\", \"f\"])\n>>> df = df.select(to_binary(\"e\").alias(\"e\"), to_binary(\"f\").alias(\"f\"))\n>>> df.printSchema()\nroot\n |-- e: binary (nullable = true)\n |-- f: binary (nullable = true)\n>>> df.select(startswith(\"e\", \"f\"), startswith(\"f\", \"e\")).show()\n+----------------+----------------+\n|startswith(e, f)|startswith(f, e)|\n+----------------+----------------+\n|            true|           false|\n+----------------+----------------+"], "Parameters": [["str Column or str", "A column of string."], ["prefix Column or str", "A column of string, the prefix."]], "Returns": [], "Category": ["Functions"], "index": 219}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.substr.html#pyspark.sql.functions.substr"], "Title": ["substr"], "Feature": ["substr"], "Description": "Returns the substring ofstrthat starts atposand is of lengthlen,\nor the slice of byte array that starts atposand is of lengthlen.\nSee alsopyspark.sql.functions.instr()pyspark.sql.functions.substring()pyspark.sql.functions.substring_index()pyspark.sql.Column.substr()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Spark SQL\", 5, 1,)], [\"a\", \"b\", \"c\"])\n>>> df.select(\"*\", sf.substr(\"a\", \"b\", \"c\")).show()\n+---------+---+---+---------------+\n|        a|  b|  c|substr(a, b, c)|\n+---------+---+---+---------------+\n|Spark SQL|  5|  1|              k|\n+---------+---+---+---------------+", ">>> df.select(\"*\", sf.substr(df.a, df.b)).show()\n+---------+---+---+------------------------+\n|        a|  b|  c|substr(a, b, 2147483647)|\n+---------+---+---+------------------------+\n|Spark SQL|  5|  1|                   k SQL|\n+---------+---+---+------------------------+"], "Parameters": [["str Column or column name", "A column of string."], ["pos Column or column name", "A column of string, the substring of str that starts at pos ."], ["len Column or column name, optional", "A column of string, the substring of str is of length len ."]], "Returns": [["Column", "substring of given value."]], "Category": ["Functions"], "index": 220}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.substring.html#pyspark.sql.functions.substring"], "Title": ["substring"], "Feature": ["substring"], "Description": "Substring starts atposand is of lengthlenwhen str is String type or\nreturns the slice of byte array that starts atposin byte and is of lengthlenwhen str is Binary type.\nSee alsopyspark.sql.functions.instr()pyspark.sql.functions.locate()pyspark.sql.functions.substr()pyspark.sql.functions.substring_index()pyspark.sql.Column.substr()\nNotes\nThe position is not zero based, but 1 based index.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('abcd',)], ['s',])\n>>> df.select('*', sf.substring(df.s, 1, 2)).show()\n+----+------------------+\n|   s|substring(s, 1, 2)|\n+----+------------------+\n|abcd|                ab|\n+----+------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('Spark', 2, 3)], ['s', 'p', 'l'])\n>>> df.select('*', sf.substring(df.s, 2, df.l)).show()\n+-----+---+---+------------------+\n|    s|  p|  l|substring(s, 2, l)|\n+-----+---+---+------------------+\n|Spark|  2|  3|               par|\n+-----+---+---+------------------+", ">>> df.select('*', sf.substring(df.s, df.p, 3)).show()\n+-----+---+---+------------------+\n|    s|  p|  l|substring(s, p, 3)|\n+-----+---+---+------------------+\n|Spark|  2|  3|               par|\n+-----+---+---+------------------+", ">>> df.select('*', sf.substring(df.s, df.p, df.l)).show()\n+-----+---+---+------------------+\n|    s|  p|  l|substring(s, p, l)|\n+-----+---+---+------------------+\n|Spark|  2|  3|               par|\n+-----+---+---+------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('Spark', 2, 3)], ['s', 'p', 'l'])\n>>> df.select('*', sf.substring(df.s, 2, 'l')).show()\n+-----+---+---+------------------+\n|    s|  p|  l|substring(s, 2, l)|\n+-----+---+---+------------------+\n|Spark|  2|  3|               par|\n+-----+---+---+------------------+", ">>> df.select('*', sf.substring('s', 'p', 'l')).show()\n+-----+---+---+------------------+\n|    s|  p|  l|substring(s, p, l)|\n+-----+---+---+------------------+\n|Spark|  2|  3|               par|\n+-----+---+---+------------------+"], "Parameters": [["str Column or column name", "target column to work on."], ["pos Column or column name or int", "starting position in str. Changed in version 4.0.0: pos now accepts column and column name."], ["len Column or column name or int", "length of chars. Changed in version 4.0.0: len now accepts column and column name."]], "Returns": [["Column", "substring of given value."]], "Category": ["Functions"], "index": 221}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.substring_index.html#pyspark.sql.functions.substring_index"], "Title": ["substring_index"], "Feature": ["substring_index"], "Description": "Returns the substring from string str before count occurrences of the delimiter delim.\nIf count is positive, everything the left of the final delimiter (counting from left) is\nreturned. If count is negative, every to the right of the final delimiter (counting from the\nright) is returned. substring_index performs a case-sensitive match when searching for delim.\nSee alsopyspark.sql.functions.instr()pyspark.sql.functions.locate()pyspark.sql.functions.substr()pyspark.sql.functions.substring()pyspark.sql.Column.substr()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n>>> df.select('*', sf.substring_index(df.s, '.', 2)).show()\n+-------+------------------------+\n|      s|substring_index(s, ., 2)|\n+-------+------------------------+\n|a.b.c.d|                     a.b|\n+-------+------------------------+", ">>> df.select('*', sf.substring_index('s', '.', -3)).show()\n+-------+-------------------------+\n|      s|substring_index(s, ., -3)|\n+-------+-------------------------+\n|a.b.c.d|                    b.c.d|\n+-------+-------------------------+"], "Parameters": [["str Column or column name", "target column to work on."], ["delim literal string", "delimiter of values."], ["count int", "number of occurrences."]], "Returns": [["Column", "substring of given value."]], "Category": ["Functions"], "index": 222}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.alias.html#pyspark.sql.DataFrame.alias"], "Title": ["DataFrame.alias"], "Feature": ["DataFrame.alias"], "Description": "Returns a newDataFramewith an alias set.", "Examples": [">>> from pyspark.sql.functions import col, desc\n>>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df_as1 = df.alias(\"df_as1\")\n>>> df_as2 = df.alias(\"df_as2\")\n>>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n>>> joined_df.select(\n...     \"df_as1.name\", \"df_as2.name\", \"df_as2.age\").sort(desc(\"df_as1.name\")).show()\n+-----+-----+---+\n| name| name|age|\n+-----+-----+---+\n|  Tom|  Tom| 14|\n|  Bob|  Bob| 16|\n|Alice|Alice| 23|\n+-----+-----+---+"], "Parameters": [["alias str", "an alias name to be set for the DataFrame ."]], "Returns": [["DataFrame", "Aliased DataFrame."]], "Category": ["DataFrame"], "index": 223}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.exceptAll.html#pyspark.sql.DataFrame.exceptAll"], "Title": ["DataFrame.exceptAll"], "Feature": ["DataFrame.exceptAll"], "Description": "Return a newDataFramecontaining rows in thisDataFramebut\nnot in anotherDataFramewhile preserving duplicates.\nThis is equivalent toEXCEPT ALLin SQL.\nAs standard in SQL, this function resolves columns by position (not by name).\nSee alsoDataFrame.subtractSimilar toexceptAll, but eliminates duplicates.", "Examples": [">>> df1 = spark.createDataFrame(\n...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n>>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n>>> df1.exceptAll(df2).show()\n+---+---+\n| C1| C2|\n+---+---+\n|  a|  1|\n|  a|  1|\n|  a|  2|\n|  c|  4|\n+---+---+"], "Parameters": [["other DataFrame", "The other DataFrame to compare to."]], "Returns": [["DataFrame", ""]], "Category": ["DataFrame"], "index": 224}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_binary.html#pyspark.sql.functions.to_binary"], "Title": ["to_binary"], "Feature": ["to_binary"], "Description": "Converts the inputcolto a binary value based on the suppliedformat.\nTheformatcan be a case-insensitive string literal of “hex”, “utf-8”, “utf8”,\nor “base64”. By default, the binary format for conversion is “hex” ifformatis omitted. The function returns NULL if at least one of the\ninput parameters is NULL.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\"abc\",)], [\"e\"])\n>>> df.select(sf.try_to_binary(df.e, sf.lit(\"utf-8\")).alias('r')).collect()\n[Row(r=bytearray(b'abc'))]", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\"414243\",)], [\"e\"])\n>>> df.select(sf.try_to_binary(df.e).alias('r')).collect()\n[Row(r=bytearray(b'ABC'))]"], "Parameters": [["col Column or str", "Input column or strings."], ["format Column or str, optional", "format to use to convert binary values."]], "Returns": [], "Category": ["Functions"], "index": 225}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_char.html#pyspark.sql.functions.to_char"], "Title": ["to_char"], "Feature": ["to_char"], "Description": "Convertcolto a string based on theformat.\nThrows an exception if the conversion fails. The format can consist of the following\ncharacters, case insensitive:\n‘0’ or ‘9’: Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the\nformat string matches a sequence of digits in the input value, generating a result\nstring of the same length as the corresponding sequence in the format string.\nThe result string is left-padded with zeros if the 0/9 sequence comprises more digits\nthan the matching part of the decimal value, starts with 0, and is before the decimal\npoint. Otherwise, it is padded with spaces.\n‘.’ or ‘D’: Specifies the position of the decimal point (optional, only allowed once).\n‘,’ or ‘G’: Specifies the position of the grouping (thousands) separator (,).\nThere must be a 0 or 9 to the left and right of each grouping separator.\n‘$’: Specifies the location of the $ currency sign. This character may only be specified once.\n‘S’ or ‘MI’: Specifies the position of a ‘-’ or ‘+’ sign (optional, only allowed once at\nthe beginning or end of the format string). Note that ‘S’ prints ‘+’ for positive\nvalues but ‘MI’ prints a space.\n‘PR’: Only allowed at the end of the format string; specifies that the result string\nwill be wrapped by angle brackets if the input value is negative.\nIfcolis a datetime,formatshall be a valid datetime pattern, see\n<a href=”https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html”>Patterns</a>.\nIfcolis a binary, it is converted to a string in one of the formats:\n‘base64’: a base 64 string.\n‘hex’: a string in the hexadecimal format.\n‘utf-8’: the input binary is decoded to UTF-8 string.", "Examples": [">>> df = spark.createDataFrame([(78.12,)], [\"e\"])\n>>> df.select(to_char(df.e, lit(\"$99.99\")).alias('r')).collect()\n[Row(r='$78.12')]"], "Parameters": [["col Column or str", "Input column or strings."], ["format Column or str, optional", "format to use to convert char values."]], "Returns": [], "Category": ["Functions"], "index": 226}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_number.html#pyspark.sql.functions.to_number"], "Title": ["to_number"], "Feature": ["to_number"], "Description": "Convert string ‘col’ to a number based on the string format ‘format’.\nThrows an exception if the conversion fails. The format can consist of the following\ncharacters, case insensitive:\n‘0’ or ‘9’: Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the\nformat string matches a sequence of digits in the input string. If the 0/9\nsequence starts with 0 and is before the decimal point, it can only match a digit\nsequence of the same size. Otherwise, if the sequence starts with 9 or is after\nthe decimal point, it can match a digit sequence that has the same or smaller size.\n‘.’ or ‘D’: Specifies the position of the decimal point (optional, only allowed once).\n‘,’ or ‘G’: Specifies the position of the grouping (thousands) separator (,).\nThere must be a 0 or 9 to the left and right of each grouping separator.\n‘col’ must match the grouping separator relevant for the size of the number.\n‘$’: Specifies the location of the $ currency sign. This character may only be\nspecified once.\n‘S’ or ‘MI’: Specifies the position of a ‘-’ or ‘+’ sign (optional, only allowed\nonce at the beginning or end of the format string). Note that ‘S’ allows ‘-’\nbut ‘MI’ does not.\n‘PR’: Only allowed at the end of the format string; specifies that ‘col’ indicates a\nnegative number with wrapping angled brackets.", "Examples": [">>> df = spark.createDataFrame([(\"$78.12\",)], [\"e\"])\n>>> df.select(to_number(df.e, lit(\"$99.99\")).alias('r')).collect()\n[Row(r=Decimal('78.12'))]"], "Parameters": [["col Column or str", "Input column or strings."], ["format Column or str, optional", "format to use to convert number values."]], "Returns": [], "Category": ["Functions"], "index": 227}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_varchar.html#pyspark.sql.functions.to_varchar"], "Title": ["to_varchar"], "Feature": ["to_varchar"], "Description": "Convertcolto a string based on theformat.\nThrows an exception if the conversion fails. The format can consist of the following\ncharacters, case insensitive:\n‘0’ or ‘9’: Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the\nformat string matches a sequence of digits in the input value, generating a result\nstring of the same length as the corresponding sequence in the format string.\nThe result string is left-padded with zeros if the 0/9 sequence comprises more digits\nthan the matching part of the decimal value, starts with 0, and is before the decimal\npoint. Otherwise, it is padded with spaces.\n‘.’ or ‘D’: Specifies the position of the decimal point (optional, only allowed once).\n‘,’ or ‘G’: Specifies the position of the grouping (thousands) separator (,).\nThere must be a 0 or 9 to the left and right of each grouping separator.\n‘$’: Specifies the location of the $ currency sign. This character may only be specified once.\n‘S’ or ‘MI’: Specifies the position of a ‘-’ or ‘+’ sign (optional, only allowed once at\nthe beginning or end of the format string). Note that ‘S’ prints ‘+’ for positive\nvalues but ‘MI’ prints a space.\n‘PR’: Only allowed at the end of the format string; specifies that the result string\nwill be wrapped by angle brackets if the input value is negative.\nIfcolis a datetime,formatshall be a valid datetime pattern, see\n<a href=”https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html”>Patterns</a>.\nIfcolis a binary, it is converted to a string in one of the formats:\n‘base64’: a base 64 string.\n‘hex’: a string in the hexadecimal format.\n‘utf-8’: the input binary is decoded to UTF-8 string.", "Examples": [">>> df = spark.createDataFrame([(78.12,)], [\"e\"])\n>>> df.select(to_varchar(df.e, lit(\"$99.99\")).alias('r')).collect()\n[Row(r='$78.12')]"], "Parameters": [["col Column or str", "Input column or strings."], ["format Column or str, optional", "format to use to convert char values."]], "Returns": [], "Category": ["Functions"], "index": 228}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.translate.html#pyspark.sql.functions.translate"], "Title": ["translate"], "Feature": ["translate"], "Description": "A function translate any character in thesrcColby a character inmatching.\nThe characters inreplaceis corresponding to the characters inmatching.\nTranslation will happen whenever any character in the string is matching with the character\nin thematching.", "Examples": [">>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n...     .alias('r')).collect()\n[Row(r='1a2s3ae')]"], "Parameters": [["srcCol Column or str", "Source column or strings"], ["matching str", "matching characters."], ["replace str", "characters for replacement. If this is shorter than matching string then\nthose chars that don’t have replacement will be dropped."]], "Returns": [["Column", "replaced value."]], "Category": ["Functions"], "index": 229}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.trim.html#pyspark.sql.functions.trim"], "Title": ["trim"], "Feature": ["trim"], "Description": "Trim the spaces from both ends for the specified string column.\nSee alsopyspark.sql.functions.ltrim()pyspark.sql.functions.rtrim()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n>>> df.select(\"*\", sf.trim(\"value\")).show()\n+--------+-----------+\n|   value|trim(value)|\n+--------+-----------+\n|   Spark|      Spark|\n| Spark  |      Spark|\n|   Spark|      Spark|\n+--------+-----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\"***Spark\", \"Spark**\", \"*Spark\"], \"STRING\")\n>>> df.select(\"*\", sf.trim(\"value\", sf.lit(\"*\"))).show()\n+--------+-----------------------+\n|   value|TRIM(BOTH * FROM value)|\n+--------+-----------------------+\n|***Spark|                  Spark|\n| Spark**|                  Spark|\n|  *Spark|                  Spark|\n+--------+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"**Spark*\", \"*\"), (\"==Spark=\", \"=\")], [\"value\", \"t\"])\n>>> df.select(\"*\", sf.trim(\"value\", \"t\")).show()\n+--------+---+-----------------------+\n|   value|  t|TRIM(BOTH t FROM value)|\n+--------+---+-----------------------+\n|**Spark*|  *|                  Spark|\n|==Spark=|  =|                  Spark|\n+--------+---+-----------------------+"], "Parameters": [["col Column or column name", "target column to work on."], ["trim Column or column name, optional", "The trim string characters to trim, the default value is a single space New in version 4.0.0."]], "Returns": [["Column", "trimmed values from both sides."]], "Category": ["Functions"], "index": 230}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_to_binary.html#pyspark.sql.functions.try_to_binary"], "Title": ["try_to_binary"], "Feature": ["try_to_binary"], "Description": "This is a special version ofto_binarythat performs the same operation, but returns a NULL\nvalue instead of raising an error if the conversion cannot be performed.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\"abc\",)], [\"e\"])\n>>> df.select(sf.try_to_binary(df.e, sf.lit(\"utf-8\")).alias('r')).collect()\n[Row(r=bytearray(b'abc'))]", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\"414243\",)], [\"e\"])\n>>> df.select(sf.try_to_binary(df.e).alias('r')).collect()\n[Row(r=bytearray(b'ABC'))]", ">>> import pyspark.sql.functions as sf\n>>> origin = spark.conf.get(\"spark.sql.ansi.enabled\")\n>>> spark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\n>>> try:\n...     df = spark.range(1)\n...     df.select(sf.try_to_binary(sf.lit(\"malformed\"), sf.lit(\"hex\"))).show()\n... finally:\n...     spark.conf.set(\"spark.sql.ansi.enabled\", origin)\n+-----------------------------+\n|try_to_binary(malformed, hex)|\n+-----------------------------+\n|                         NULL|\n+-----------------------------+"], "Parameters": [["col Column or str", "Input column or strings."], ["format Column or str, optional", "format to use to convert binary values."]], "Returns": [], "Category": ["Functions"], "index": 231}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_to_number.html#pyspark.sql.functions.try_to_number"], "Title": ["try_to_number"], "Feature": ["try_to_number"], "Description": "Convert string ‘col’ to a number based on the string formatformat. Returns NULL if the\nstring ‘col’ does not match the expected format. The format follows the same semantics as the\nto_number function.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\"$78.12\",)], [\"e\"])\n>>> df.select(sf.try_to_number(df.e, sf.lit(\"$99.99\")).alias('r')).show()\n+-----+\n|    r|\n+-----+\n|78.12|\n+-----+", ">>> import pyspark.sql.functions as sf\n>>> origin = spark.conf.get(\"spark.sql.ansi.enabled\")\n>>> spark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\n>>> try:\n...     df = spark.range(1)\n...     df.select(sf.try_to_number(sf.lit(\"77\"), sf.lit(\"$99.99\")).alias('r')).show()\n... finally:\n...     spark.conf.set(\"spark.sql.ansi.enabled\", origin)\n+----+\n|   r|\n+----+\n|NULL|\n+----+"], "Parameters": [["col Column or str", "Input column or strings."], ["format Column or str, optional", "format to use to convert number values."]], "Returns": [], "Category": ["Functions"], "index": 232}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_validate_utf8.html#pyspark.sql.functions.try_validate_utf8"], "Title": ["try_validate_utf8"], "Feature": ["try_validate_utf8"], "Description": "Returns the input value if it corresponds to a valid UTF-8 string, or NULL otherwise.\nSee alsopyspark.sql.functions.is_valid_utf8()pyspark.sql.functions.make_valid_utf8()pyspark.sql.functions.validate_utf8()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.try_validate_utf8(sf.lit(\"SparkSQL\"))).show()\n+---------------------------+\n|try_validate_utf8(SparkSQL)|\n+---------------------------+\n|                   SparkSQL|\n+---------------------------+"], "Parameters": [["str Column or column name", "A column of strings, each representing a UTF-8 byte sequence."]], "Returns": [["Column", "the input string if it is a valid UTF-8 string, null otherwise."]], "Category": ["Functions"], "index": 233}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ucase.html#pyspark.sql.functions.ucase"], "Title": ["ucase"], "Feature": ["ucase"], "Description": "Returnsstrwith all characters changed to uppercase.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.ucase(sf.lit(\"Spark\"))).show()\n+------------+\n|ucase(Spark)|\n+------------+\n|       SPARK|\n+------------+"], "Parameters": [["str Column or str", "Input column or strings."]], "Returns": [], "Category": ["Functions"], "index": 234}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.executionInfo.html#pyspark.sql.DataFrame.executionInfo"], "Title": ["DataFrame.executionInfo"], "Feature": ["DataFrame.executionInfo"], "Description": "Returns a ExecutionInfo object after the query was executed.\nThe executionInfo method allows to introspect information about the actual\nquery execution after the successful execution. Accessing this member before\nthe query execution will return None.\nIf the same DataFrame is executed multiple times, the execution info will be\noverwritten by the latest operation.\nNotes\nThis is an API dedicated to Spark Connect client only. With regular Spark Session, it throws\nan exception.", "Examples": [], "Parameters": [], "Returns": [["An instance of ExecutionInfo or None when the value is not set yet.", ""]], "Category": ["DataFrame"], "index": 235}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.unbase64.html#pyspark.sql.functions.unbase64"], "Title": ["unbase64"], "Feature": ["unbase64"], "Description": "Decodes a BASE64 encoded string column and returns it as a binary column.\nSee alsopyspark.sql.functions.base64()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\"U3Bhcms=\", \"UHlTcGFyaw==\", \"UGFuZGFzIEFQSQ==\"], \"STRING\")\n>>> df.select(\"*\", sf.unbase64(\"value\")).show(truncate=False)\n+----------------+-------------------------------+\n|value           |unbase64(value)                |\n+----------------+-------------------------------+\n|U3Bhcms=        |[53 70 61 72 6B]               |\n|UHlTcGFyaw==    |[50 79 53 70 61 72 6B]         |\n|UGFuZGFzIEFQSQ==|[50 61 6E 64 61 73 20 41 50 49]|\n+----------------+-------------------------------+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "encoded string value."]], "Category": ["Functions"], "index": 236}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.upper.html#pyspark.sql.functions.upper"], "Title": ["upper"], "Feature": ["upper"], "Description": "Converts a string expression to upper case.\nSee alsopyspark.sql.functions.lower()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n>>> df.select(\"*\", sf.upper(\"value\")).show()\n+----------+------------+\n|     value|upper(value)|\n+----------+------------+\n|     Spark|       SPARK|\n|   PySpark|     PYSPARK|\n|Pandas API|  PANDAS API|\n+----------+------------+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "upper case values."]], "Category": ["Functions"], "index": 237}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.validate_utf8.html#pyspark.sql.functions.validate_utf8"], "Title": ["validate_utf8"], "Feature": ["validate_utf8"], "Description": "Returns the input value if it corresponds to a valid UTF-8 string, or emits an error otherwise.\nSee alsopyspark.sql.functions.is_valid_utf8()pyspark.sql.functions.make_valid_utf8()pyspark.sql.functions.try_validate_utf8()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.validate_utf8(sf.lit(\"SparkSQL\"))).show()\n+-----------------------+\n|validate_utf8(SparkSQL)|\n+-----------------------+\n|               SparkSQL|\n+-----------------------+"], "Parameters": [["str Column or column name", "A column of strings, each representing a UTF-8 byte sequence."]], "Returns": [["Column", "the input string if it is a valid UTF-8 string, error otherwise."]], "Category": ["Functions"], "index": 238}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bit_count.html#pyspark.sql.functions.bit_count"], "Title": ["bit_count"], "Feature": ["bit_count"], "Description": "Returns the number of bits that are set in the argument expr as an unsigned 64-bit integer,\nor NULL if the argument is NULL.\nSee alsopyspark.sql.functions.bit_get()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (0), (1), (2), (3), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.bit_count(\"value\")).show()\n+-----+----------------+\n|value|bit_count(value)|\n+-----+----------------+\n|    0|               0|\n|    1|               1|\n|    2|               1|\n|    3|               2|\n| NULL|            NULL|\n+-----+----------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the number of bits that are set in the argument expr as an unsigned 64-bit integer,\nor NULL if the argument is NULL."]], "Category": ["Functions"], "index": 239}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bit_get.html#pyspark.sql.functions.bit_get"], "Title": ["bit_get"], "Feature": ["bit_get"], "Description": "Returns the value of the bit (0 or 1) at the specified position.\nThe positions are numbered from right to left, starting at zero.\nThe position argument cannot be negative.\nSee alsopyspark.sql.functions.bit_count()pyspark.sql.functions.getbit()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1],[2],[3],[None]], [\"value\"])\n>>> df.select(\"*\", sf.bit_get(\"value\", sf.lit(1))).show()\n+-----+-----------------+\n|value|bit_get(value, 1)|\n+-----+-----------------+\n|    1|                0|\n|    2|                1|\n|    3|                1|\n| NULL|             NULL|\n+-----+-----------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1,2],[2,1],[3,None],[None,1]], [\"value\", \"pos\"])\n>>> df.select(\"*\", sf.bit_get(df.value, \"pos\")).show()\n+-----+----+-------------------+\n|value| pos|bit_get(value, pos)|\n+-----+----+-------------------+\n|    1|   2|                  0|\n|    2|   1|                  1|\n|    3|NULL|               NULL|\n| NULL|   1|               NULL|\n+-----+----+-------------------+"], "Parameters": [["col Column or column name", "target column to compute on."], ["pos Column or column name", "The positions are numbered from right to left, starting at zero."]], "Returns": [["Column", "the value of the bit (0 or 1) at the specified position."]], "Category": ["Functions"], "index": 240}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bitwise_not.html#pyspark.sql.functions.bitwise_not"], "Title": ["bitwise_not"], "Feature": ["bitwise_not"], "Description": "Computes bitwise not.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.sql(\n...     \"SELECT * FROM VALUES (0), (1), (2), (3), (NULL) AS TAB(value)\"\n... ).select(\"*\", sf.bitwise_not(\"value\")).show()\n+-----+------+\n|value|~value|\n+-----+------+\n|    0|    -1|\n|    1|    -2|\n|    2|    -3|\n|    3|    -4|\n| NULL|  NULL|\n+-----+------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 241}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.getbit.html#pyspark.sql.functions.getbit"], "Title": ["getbit"], "Feature": ["getbit"], "Description": "Returns the value of the bit (0 or 1) at the specified position.\nThe positions are numbered from right to left, starting at zero.\nThe position argument cannot be negative.\nSee alsopyspark.sql.functions.bit_get()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [[1], [2], [3], [None]], [\"value\"]\n... ).select(\"*\", sf.getbit(\"value\", sf.lit(1))).show()\n+-----+----------------+\n|value|getbit(value, 1)|\n+-----+----------------+\n|    1|               0|\n|    2|               1|\n|    3|               1|\n| NULL|            NULL|\n+-----+----------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1,2],[2,1],[3,None],[None,1]], [\"value\", \"pos\"])\n>>> df.select(\"*\", sf.getbit(df.value, \"pos\")).show()\n+-----+----+------------------+\n|value| pos|getbit(value, pos)|\n+-----+----+------------------+\n|    1|   2|                 0|\n|    2|   1|                 1|\n|    3|NULL|              NULL|\n| NULL|   1|              NULL|\n+-----+----+------------------+"], "Parameters": [["col Column or column name", "target column to compute on."], ["pos Column or column name", "The positions are numbered from right to left, starting at zero."]], "Returns": [["Column", "the value of the bit (0 or 1) at the specified position."]], "Category": ["Functions"], "index": 242}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.shiftleft.html#pyspark.sql.functions.shiftleft"], "Title": ["shiftleft"], "Feature": ["shiftleft"], "Description": "Shift the given value numBits left.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(4).select(\"*\", sf.shiftleft('id', 1)).show()\n+---+----------------+\n| id|shiftleft(id, 1)|\n+---+----------------+\n|  0|               0|\n|  1|               2|\n|  2|               4|\n|  3|               6|\n+---+----------------+"], "Parameters": [["col Column or column name", "input column of values to shift."], ["numBits int", "number of bits to shift."]], "Returns": [["Column", "shifted value."]], "Category": ["Functions"], "index": 243}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.shiftright.html#pyspark.sql.functions.shiftright"], "Title": ["shiftright"], "Feature": ["shiftright"], "Description": "(Signed) shift the given value numBits right.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(4).select(\"*\", sf.shiftright('id', 1)).show()\n+---+-----------------+\n| id|shiftright(id, 1)|\n+---+-----------------+\n|  0|                0|\n|  1|                0|\n|  2|                1|\n|  3|                1|\n+---+-----------------+"], "Parameters": [["col Column or column name", "input column of values to shift."], ["numBits int", "number of bits to shift."]], "Returns": [["Column", "shifted values."]], "Category": ["Functions"], "index": 244}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.shiftrightunsigned.html#pyspark.sql.functions.shiftrightunsigned"], "Title": ["shiftrightunsigned"], "Feature": ["shiftrightunsigned"], "Description": "Unsigned shift the given value numBits right.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(4).select(\"*\", sf.shiftrightunsigned(sf.col('id') - 2, 1)).show()\n+---+-------------------------------+\n| id|shiftrightunsigned((id - 2), 1)|\n+---+-------------------------------+\n|  0|            9223372036854775807|\n|  1|            9223372036854775807|\n|  2|                              0|\n|  3|                              0|\n+---+-------------------------------+"], "Parameters": [["col Column or column name", "input column of values to shift."], ["numBits int", "number of bits to shift."]], "Returns": [["Column", "shifted value."]], "Category": ["Functions"], "index": 245}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.exists.html#pyspark.sql.DataFrame.exists"], "Title": ["DataFrame.exists"], "Feature": ["DataFrame.exists"], "Description": "Return aColumnobject for an EXISTS Subquery.\nTheexistsmethod provides a way to create a boolean column that checks for the presence\nof related records in a subquery. When applied within aDataFrame, this method allows you\nto filter rows based on whether matching records exist in the related dataset. The resultingColumnobject can be used directly in filtering conditions or as a computed column.", "Examples": [">>> data_customers = [\n...     (101, \"Alice\", \"USA\"), (102, \"Bob\", \"Canada\"), (103, \"Charlie\", \"USA\"),\n...     (104, \"David\", \"Australia\")\n... ]\n>>> data_orders = [\n...     (1, 101, \"2023-01-15\", 250), (2, 102, \"2023-01-20\", 300),\n...     (3, 103, \"2023-01-25\", 400), (4, 101, \"2023-02-05\", 150)\n... ]\n>>> customers = spark.createDataFrame(\n...     data_customers, [\"customer_id\", \"customer_name\", \"country\"])\n>>> orders = spark.createDataFrame(\n...     data_orders, [\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"])", ">>> from pyspark.sql import functions as sf\n>>> customers.alias(\"c\").where(\n...     orders.alias(\"o\").where(\n...         sf.col(\"o.customer_id\") == sf.col(\"c.customer_id\").outer()\n...     ).exists()\n... ).orderBy(\"customer_id\").show()\n+-----------+-------------+-------+\n|customer_id|customer_name|country|\n+-----------+-------------+-------+\n|        101|        Alice|    USA|\n|        102|          Bob| Canada|\n|        103|      Charlie|    USA|\n+-----------+-------------+-------+", ">>> from pyspark.sql import functions as sf\n>>> customers.alias(\"c\").where(\n...     ~orders.alias(\"o\").where(\n...         sf.col(\"o.customer_id\") == sf.col(\"c.customer_id\").outer()\n...     ).exists()\n... ).orderBy(\"customer_id\").show()\n+-----------+-------------+---------+\n|customer_id|customer_name|  country|\n+-----------+-------------+---------+\n|        104|        David|Australia|\n+-----------+-------------+---------+", ">>> from pyspark.sql import functions as sf\n>>> orders.alias(\"o\").where(\n...     customers.alias(\"c\").where(\n...         (sf.col(\"c.customer_id\") == sf.col(\"o.customer_id\").outer())\n...         & (sf.col(\"country\") == \"USA\")\n...     ).exists()\n... ).orderBy(\"order_id\").show()\n+--------+-----------+----------+------------+\n|order_id|customer_id|order_date|total_amount|\n+--------+-----------+----------+------------+\n|       1|        101|2023-01-15|         250|\n|       3|        103|2023-01-25|         400|\n|       4|        101|2023-02-05|         150|\n+--------+-----------+----------+------------+"], "Parameters": [], "Returns": [["Column", "A Column object representing an EXISTS subquery"]], "Category": ["DataFrame"], "index": 246}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.add_months.html#pyspark.sql.functions.add_months"], "Title": ["add_months"], "Feature": ["add_months"], "Description": "Returns the date that ismonthsmonths afterstart. Ifmonthsis a negative value\nthen these amount of months will be deducted from thestart.\nSee alsopyspark.sql.functions.dateadd()pyspark.sql.functions.date_add()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08', 2,)], 'struct<dt:string,a:int>')\n>>> df.select('*', sf.add_months(df.dt, 1)).show()\n+----------+---+-----------------+\n|        dt|  a|add_months(dt, 1)|\n+----------+---+-----------------+\n|2015-04-08|  2|       2015-05-08|\n+----------+---+-----------------+", ">>> df.select('*', sf.add_months('dt', 'a')).show()\n+----------+---+-----------------+\n|        dt|  a|add_months(dt, a)|\n+----------+---+-----------------+\n|2015-04-08|  2|       2015-06-08|\n+----------+---+-----------------+", ">>> df.select('*', sf.add_months('dt', sf.lit(-1))).show()\n+----------+---+------------------+\n|        dt|  a|add_months(dt, -1)|\n+----------+---+------------------+\n|2015-04-08|  2|        2015-03-08|\n+----------+---+------------------+"], "Parameters": [["start Column or column name", "date column to work on."], ["months Column or column name or int", "how many months after the given date to calculate.\nAccepts negative value as well to calculate backwards."]], "Returns": [["Column", "a date after/before given number of months."]], "Category": ["Functions"], "index": 247}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.convert_timezone.html#pyspark.sql.functions.convert_timezone"], "Title": ["convert_timezone"], "Feature": ["convert_timezone"], "Description": "Converts the timestamp without time zonesourceTsfrom thesourceTztime zone totargetTz.\nSee alsopyspark.sql.functions.current_timezone()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 00:00:00',)], ['ts'])\n>>> df.select(\n...     '*',\n...     sf.convert_timezone(None, sf.lit('Asia/Hong_Kong'), 'ts')\n... ).show() \n+-------------------+--------------------------------------------------------+\n|                 ts|convert_timezone(current_timezone(), Asia/Hong_Kong, ts)|\n+-------------------+--------------------------------------------------------+\n|2015-04-08 00:00:00|                                     2015-04-08 15:00:00|\n+-------------------+--------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 15:00:00',)], ['ts'])\n>>> df.select(\n...     '*',\n...     sf.convert_timezone(sf.lit('Asia/Hong_Kong'), sf.lit('America/Los_Angeles'), df.ts)\n... ).show()\n+-------------------+---------------------------------------------------------+\n|                 ts|convert_timezone(Asia/Hong_Kong, America/Los_Angeles, ts)|\n+-------------------+---------------------------------------------------------+\n|2015-04-08 15:00:00|                                      2015-04-08 00:00:00|\n+-------------------+---------------------------------------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["sourceTz Column , optional", "The time zone for the input timestamp. If it is missed,\nthe current session time zone is used as the source time zone."], ["targetTz Column", "The time zone to which the input timestamp should be converted."], ["sourceTs Column or column name", "A timestamp without time zone."]], "Returns": [["Column", "A new column that contains a timestamp for converted time zone."]], "Category": ["Functions"], "index": 248}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.curdate.html#pyspark.sql.functions.curdate"], "Title": ["curdate"], "Feature": ["curdate"], "Description": "Returns the current date at the start of query evaluation as aDateTypecolumn.\nAll calls of current_date within the same query return the same value.\nSee alsopyspark.sql.functions.now()pyspark.sql.functions.current_date()pyspark.sql.functions.current_timestamp()pyspark.sql.functions.localtimestamp()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.curdate()).show() \n+--------------+\n|current_date()|\n+--------------+\n|    2022-08-26|\n+--------------+"], "Parameters": [], "Returns": [["Column", "current date."]], "Category": ["Functions"], "index": 249}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.current_date.html#pyspark.sql.functions.current_date"], "Title": ["current_date"], "Feature": ["current_date"], "Description": "Returns the current date at the start of query evaluation as aDateTypecolumn.\nAll calls of current_date within the same query return the same value.\nSee alsopyspark.sql.functions.now()pyspark.sql.functions.curdate()pyspark.sql.functions.current_timestamp()pyspark.sql.functions.localtimestamp()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.current_date()).show() \n+--------------+\n|current_date()|\n+--------------+\n|    2022-08-26|\n+--------------+"], "Parameters": [], "Returns": [["Column", "current date."]], "Category": ["Functions"], "index": 250}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.current_timestamp.html#pyspark.sql.functions.current_timestamp"], "Title": ["current_timestamp"], "Feature": ["current_timestamp"], "Description": "Returns the current timestamp at the start of query evaluation as aTimestampTypecolumn. All calls of current_timestamp within the same query return the same value.\nSee alsopyspark.sql.functions.now()pyspark.sql.functions.curdate()pyspark.sql.functions.current_date()pyspark.sql.functions.localtimestamp()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.current_timestamp()).show(truncate=False) \n+-----------------------+\n|current_timestamp()    |\n+-----------------------+\n|2022-08-26 21:23:22.716|\n+-----------------------+"], "Parameters": [], "Returns": [["Column", "current date and time."]], "Category": ["Functions"], "index": 251}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.current_timezone.html#pyspark.sql.functions.current_timezone"], "Title": ["current_timezone"], "Feature": ["current_timezone"], "Description": "Returns the current session local timezone.\nSee alsopyspark.sql.functions.convert_timezone()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.current_timezone()).show()\n+-------------------+\n| current_timezone()|\n+-------------------+\n|America/Los_Angeles|\n+-------------------+", ">>> spark.conf.set(\"spark.sql.session.timeZone\", \"Asia/Shanghai\")\n>>> spark.range(1).select(sf.current_timezone()).show()\n+------------------+\n|current_timezone()|\n+------------------+\n|     Asia/Shanghai|\n+------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [], "Returns": [["Column", "current session local timezone."]], "Category": ["Functions"], "index": 252}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.date_add.html#pyspark.sql.functions.date_add"], "Title": ["date_add"], "Feature": ["date_add"], "Description": "Returns the date that isdaysdays afterstart. Ifdaysis a negative value\nthen these amount of days will be deducted fromstart.\nSee alsopyspark.sql.functions.dateadd()pyspark.sql.functions.date_sub()pyspark.sql.functions.datediff()pyspark.sql.functions.date_diff()pyspark.sql.functions.timestamp_add()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08', 2,)], 'struct<dt:string,a:int>')\n>>> df.select('*', sf.date_add(df.dt, 1)).show()\n+----------+---+---------------+\n|        dt|  a|date_add(dt, 1)|\n+----------+---+---------------+\n|2015-04-08|  2|     2015-04-09|\n+----------+---+---------------+", ">>> df.select('*', sf.date_add('dt', 'a')).show()\n+----------+---+---------------+\n|        dt|  a|date_add(dt, a)|\n+----------+---+---------------+\n|2015-04-08|  2|     2015-04-10|\n+----------+---+---------------+", ">>> df.select('*', sf.date_add('dt', sf.lit(-1))).show()\n+----------+---+----------------+\n|        dt|  a|date_add(dt, -1)|\n+----------+---+----------------+\n|2015-04-08|  2|      2015-04-07|\n+----------+---+----------------+"], "Parameters": [["start Column or column name", "date column to work on."], ["days Column or column name or int", "how many days after the given date to calculate.\nAccepts negative value as well to calculate backwards in time."]], "Returns": [["Column", "a date after/before given number of days."]], "Category": ["Functions"], "index": 253}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.date_diff.html#pyspark.sql.functions.date_diff"], "Title": ["date_diff"], "Feature": ["date_diff"], "Description": "Returns the number of days fromstarttoend.\nSee alsopyspark.sql.functions.dateadd()pyspark.sql.functions.date_add()pyspark.sql.functions.date_sub()pyspark.sql.functions.datediff()pyspark.sql.functions.timestamp_diff()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n>>> df.select('*', sf.date_diff('d1', 'd2')).show()\n+----------+----------+-----------------+\n|        d1|        d2|date_diff(d1, d2)|\n+----------+----------+-----------------+\n|2015-04-08|2015-05-10|              -32|\n+----------+----------+-----------------+", ">>> df.select('*', sf.date_diff(df.d2, df.d1)).show()\n+----------+----------+-----------------+\n|        d1|        d2|date_diff(d2, d1)|\n+----------+----------+-----------------+\n|2015-04-08|2015-05-10|               32|\n+----------+----------+-----------------+"], "Parameters": [["end Column or column name", "to date column to work on."], ["start Column or column name", "from date column to work on."]], "Returns": [["Column", "difference in days between two dates."]], "Category": ["Functions"], "index": 254}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.date_format.html#pyspark.sql.functions.date_format"], "Title": ["date_format"], "Feature": ["date_format"], "Description": "Converts a date/timestamp/string to a value of string in the format specified by the date\nformat given by the second argument.\nA pattern could be for instancedd.MM.yyyyand could return a string like ‘18.03.1993’. All\npattern letters ofdatetime pattern. can be used.\nSee alsopyspark.sql.functions.to_date()pyspark.sql.functions.to_timestamp()pyspark.sql.functions.to_timestamp_ltz()pyspark.sql.functions.to_timestamp_ntz()pyspark.sql.functions.to_utc_timestamp()pyspark.sql.functions.try_to_timestamp()\nNotes\nWhenever possible, use specialized functions likeyear.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.date_format('dt', 'MM/dd/yyyy')).show()\n+----------+----------+---------------------------+\n|        dt|typeof(dt)|date_format(dt, MM/dd/yyyy)|\n+----------+----------+---------------------------+\n|2015-04-08|    string|                 04/08/2015|\n|2024-10-31|    string|                 10/31/2024|\n+----------+----------+---------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.date_format('ts', 'yy=MM=dd HH=mm=ss')).show()\n+-------------------+----------+----------------------------------+\n|                 ts|typeof(ts)|date_format(ts, yy=MM=dd HH=mm=ss)|\n+-------------------+----------+----------------------------------+\n|2015-04-08 13:08:15|    string|                 15=04=08 13=08=15|\n|2024-10-31 10:09:16|    string|                 24=10=31 10=09=16|\n+-------------------+----------+----------------------------------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.date(2015, 4, 8),),\n...     (datetime.date(2024, 10, 31),)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.date_format('dt', 'yy--MM--dd')).show()\n+----------+----------+---------------------------+\n|        dt|typeof(dt)|date_format(dt, yy--MM--dd)|\n+----------+----------+---------------------------+\n|2015-04-08|      date|                 15--04--08|\n|2024-10-31|      date|                 24--10--31|\n+----------+----------+---------------------------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.date_format('ts', 'yy=MM=dd HH=mm=ss')).show()\n+-------------------+----------+----------------------------------+\n|                 ts|typeof(ts)|date_format(ts, yy=MM=dd HH=mm=ss)|\n+-------------------+----------+----------------------------------+\n|2015-04-08 13:08:15| timestamp|                 15=04=08 13=08=15|\n|2024-10-31 10:09:16| timestamp|                 24=10=31 10=09=16|\n+-------------------+----------+----------------------------------+"], "Parameters": [["date Column or column name", "input column of values to format."], ["format: literal string", "format to use to represent datetime values."]], "Returns": [["Column", "string value representing formatted datetime."]], "Category": ["Functions"], "index": 255}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.date_from_unix_date.html#pyspark.sql.functions.date_from_unix_date"], "Title": ["date_from_unix_date"], "Feature": ["date_from_unix_date"], "Description": "Create date from the number ofdayssince 1970-01-01.\nSee alsopyspark.sql.functions.from_unixtime()pyspark.sql.functions.unix_date()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(4).select('*', sf.date_from_unix_date('id')).show()\n+---+-----------------------+\n| id|date_from_unix_date(id)|\n+---+-----------------------+\n|  0|             1970-01-01|\n|  1|             1970-01-02|\n|  2|             1970-01-03|\n|  3|             1970-01-04|\n+---+-----------------------+"], "Parameters": [["days Column or column name", "the target column to work on."]], "Returns": [["Column", "the date from the number of days since 1970-01-01."]], "Category": ["Functions"], "index": 256}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.explain.html#pyspark.sql.DataFrame.explain"], "Title": ["DataFrame.explain"], "Feature": ["DataFrame.explain"], "Description": "Prints the (logical and physical) plans to the console for debugging purposes.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.explain()  \n== Physical Plan ==\n*(1) Scan ExistingRDD[age...,name...]", ">>> df.explain(extended=True)\n== Parsed Logical Plan ==\n...\n== Analyzed Logical Plan ==\n...\n== Optimized Logical Plan ==\n...\n== Physical Plan ==\n...", ">>> df.explain(mode=\"formatted\")  \n== Physical Plan ==\n* Scan ExistingRDD (...)\n(1) Scan ExistingRDD [codegen id : ...]\nOutput [2]: [age..., name...]\n...", ">>> df.explain(mode=\"cost\")\n== Optimized Logical Plan ==\n...Statistics...\n..."], "Parameters": [["extended bool, optional", "default False . If False , prints only the physical plan.\nWhen this is a string without specifying the mode , it works as the mode is\nspecified."], ["mode str, optional", "specifies the expected output format of plans. simple : Print only a physical plan. extended : Print both logical and physical plans. codegen : Print a physical plan and generated codes if they are available. cost : Print a logical plan and statistics if they are available. formatted : Split explain output into two sections: a physical plan outline                 and node details. Changed in version 3.0.0: Added optional argument mode to specify the expected output format of plans."]], "Returns": [], "Category": ["DataFrame"], "index": 257}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.date_part.html#pyspark.sql.functions.date_part"], "Title": ["date_part"], "Feature": ["date_part"], "Description": "Extracts a part of the date/timestamp or interval source.\nSee alsopyspark.sql.functions.year()pyspark.sql.functions.quarter()pyspark.sql.functions.month()pyspark.sql.functions.day()pyspark.sql.functions.hour()pyspark.sql.functions.minute()pyspark.sql.functions.second()pyspark.sql.functions.datepart()pyspark.sql.functions.extract()", "Examples": [">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n>>> df.select(\n...     '*',\n...     sf.date_part(sf.lit('YEAR'), 'ts').alias('year'),\n...     sf.date_part(sf.lit('month'), 'ts').alias('month'),\n...     sf.date_part(sf.lit('WEEK'), 'ts').alias('week'),\n...     sf.date_part(sf.lit('D'), df.ts).alias('day'),\n...     sf.date_part(sf.lit('M'), df.ts).alias('minute'),\n...     sf.date_part(sf.lit('S'), df.ts).alias('second')\n... ).show()\n+-------------------+----+-----+----+---+------+---------+\n|                 ts|year|month|week|day|minute|   second|\n+-------------------+----+-----+----+---+------+---------+\n|2015-04-08 13:08:15|2015|    4|  15|  8|     8|15.000000|\n+-------------------+----+-----+----+---+------+---------+"], "Parameters": [["field Column", "selects which part of the source should be extracted, and supported string values\nare as same as the fields of the equivalent function extract ."], ["source Column or column name", "a date/timestamp or interval column from where field should be extracted."]], "Returns": [["Column", "a part of the date/timestamp or interval source."]], "Category": ["Functions"], "index": 258}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.date_sub.html#pyspark.sql.functions.date_sub"], "Title": ["date_sub"], "Feature": ["date_sub"], "Description": "Returns the date that isdaysdays beforestart. Ifdaysis a negative value\nthen these amount of days will be added tostart.\nSee alsopyspark.sql.functions.dateadd()pyspark.sql.functions.date_add()pyspark.sql.functions.datediff()pyspark.sql.functions.date_diff()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08', 2,)], 'struct<dt:string,a:int>')\n>>> df.select('*', sf.date_sub(df.dt, 1)).show()\n+----------+---+---------------+\n|        dt|  a|date_sub(dt, 1)|\n+----------+---+---------------+\n|2015-04-08|  2|     2015-04-07|\n+----------+---+---------------+", ">>> df.select('*', sf.date_sub('dt', 'a')).show()\n+----------+---+---------------+\n|        dt|  a|date_sub(dt, a)|\n+----------+---+---------------+\n|2015-04-08|  2|     2015-04-06|\n+----------+---+---------------+", ">>> df.select('*', sf.date_sub('dt', sf.lit(-1))).show()\n+----------+---+----------------+\n|        dt|  a|date_sub(dt, -1)|\n+----------+---+----------------+\n|2015-04-08|  2|      2015-04-09|\n+----------+---+----------------+"], "Parameters": [["start Column or column name", "date column to work on."], ["days Column or column name or int", "how many days before the given date to calculate.\nAccepts negative value as well to calculate forward in time."]], "Returns": [["Column", "a date before/after given number of days."]], "Category": ["Functions"], "index": 259}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.date_trunc.html#pyspark.sql.functions.date_trunc"], "Title": ["date_trunc"], "Feature": ["date_trunc"], "Description": "Returns timestamp truncated to the unit specified by the format.\nSee alsopyspark.sql.functions.trunc()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['ts'])\n>>> df.select('*', sf.date_trunc('year', df.ts)).show()\n+-------------------+--------------------+\n|                 ts|date_trunc(year, ts)|\n+-------------------+--------------------+\n|1997-02-28 05:02:11| 1997-01-01 00:00:00|\n+-------------------+--------------------+", ">>> df.select('*', sf.date_trunc('mon', 'ts')).show()\n+-------------------+-------------------+\n|                 ts|date_trunc(mon, ts)|\n+-------------------+-------------------+\n|1997-02-28 05:02:11|1997-02-01 00:00:00|\n+-------------------+-------------------+"], "Parameters": [["format literal string", "‘year’, ‘yyyy’, ‘yy’ to truncate by year,\n‘month’, ‘mon’, ‘mm’ to truncate by month,\n‘day’, ‘dd’ to truncate by day,\nOther options are:\n‘microsecond’, ‘millisecond’, ‘second’, ‘minute’, ‘hour’, ‘week’, ‘quarter’"], ["timestamp Column or column name", "input column of values to truncate."]], "Returns": [["Column", "truncated timestamp."]], "Category": ["Functions"], "index": 260}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.dateadd.html#pyspark.sql.functions.dateadd"], "Title": ["dateadd"], "Feature": ["dateadd"], "Description": "Returns the date that isdaysdays afterstart. Ifdaysis a negative value\nthen these amount of days will be deducted fromstart.\nSee alsopyspark.sql.functions.date_add()pyspark.sql.functions.date_sub()pyspark.sql.functions.datediff()pyspark.sql.functions.date_diff()pyspark.sql.functions.timestamp_add()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08', 2,)], 'struct<dt:string,a:int>')\n>>> df.select('*', sf.dateadd(df.dt, 1)).show()\n+----------+---+---------------+\n|        dt|  a|date_add(dt, 1)|\n+----------+---+---------------+\n|2015-04-08|  2|     2015-04-09|\n+----------+---+---------------+", ">>> df.select('*', sf.dateadd('dt', 'a')).show()\n+----------+---+---------------+\n|        dt|  a|date_add(dt, a)|\n+----------+---+---------------+\n|2015-04-08|  2|     2015-04-10|\n+----------+---+---------------+", ">>> df.select('*', sf.dateadd('dt', sf.lit(-1))).show()\n+----------+---+----------------+\n|        dt|  a|date_add(dt, -1)|\n+----------+---+----------------+\n|2015-04-08|  2|      2015-04-07|\n+----------+---+----------------+"], "Parameters": [["start Column or column name", "date column to work on."], ["days Column or column name or int", "how many days after the given date to calculate.\nAccepts negative value as well to calculate backwards in time."]], "Returns": [["Column", "a date after/before given number of days."]], "Category": ["Functions"], "index": 261}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.datediff.html#pyspark.sql.functions.datediff"], "Title": ["datediff"], "Feature": ["datediff"], "Description": "Returns the number of days fromstarttoend.\nSee alsopyspark.sql.functions.dateadd()pyspark.sql.functions.date_add()pyspark.sql.functions.date_sub()pyspark.sql.functions.date_diff()pyspark.sql.functions.timestamp_diff()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n>>> df.select('*', sf.datediff('d1', 'd2')).show()\n+----------+----------+----------------+\n|        d1|        d2|datediff(d1, d2)|\n+----------+----------+----------------+\n|2015-04-08|2015-05-10|             -32|\n+----------+----------+----------------+", ">>> df.select('*', sf.datediff(df.d2, df.d1)).show()\n+----------+----------+----------------+\n|        d1|        d2|datediff(d2, d1)|\n+----------+----------+----------------+\n|2015-04-08|2015-05-10|              32|\n+----------+----------+----------------+"], "Parameters": [["end Column or column name", "to date column to work on."], ["start Column or column name", "from date column to work on."]], "Returns": [["Column", "difference in days between two dates."]], "Category": ["Functions"], "index": 262}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.datepart.html#pyspark.sql.functions.datepart"], "Title": ["datepart"], "Feature": ["datepart"], "Description": "Extracts a part of the date/timestamp or interval source.\nSee alsopyspark.sql.functions.year()pyspark.sql.functions.quarter()pyspark.sql.functions.month()pyspark.sql.functions.day()pyspark.sql.functions.hour()pyspark.sql.functions.minute()pyspark.sql.functions.second()pyspark.sql.functions.date_part()pyspark.sql.functions.extract()", "Examples": [">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n>>> df.select(\n...     '*',\n...     sf.datepart(sf.lit('YEAR'), 'ts').alias('year'),\n...     sf.datepart(sf.lit('month'), 'ts').alias('month'),\n...     sf.datepart(sf.lit('WEEK'), 'ts').alias('week'),\n...     sf.datepart(sf.lit('D'), df.ts).alias('day'),\n...     sf.datepart(sf.lit('M'), df.ts).alias('minute'),\n...     sf.datepart(sf.lit('S'), df.ts).alias('second')\n... ).show()\n+-------------------+----+-----+----+---+------+---------+\n|                 ts|year|month|week|day|minute|   second|\n+-------------------+----+-----+----+---+------+---------+\n|2015-04-08 13:08:15|2015|    4|  15|  8|     8|15.000000|\n+-------------------+----+-----+----+---+------+---------+"], "Parameters": [["field Column", "selects which part of the source should be extracted, and supported string values\nare as same as the fields of the equivalent function extract ."], ["source Column or column name", "a date/timestamp or interval column from where field should be extracted."]], "Returns": [["Column", "a part of the date/timestamp or interval source."]], "Category": ["Functions"], "index": 263}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.day.html#pyspark.sql.functions.day"], "Title": ["day"], "Feature": ["day"], "Description": "Extract the day of the month of a given date/timestamp as integer.\nSee alsopyspark.sql.functions.year()pyspark.sql.functions.quarter()pyspark.sql.functions.month()pyspark.sql.functions.hour()pyspark.sql.functions.minute()pyspark.sql.functions.second()pyspark.sql.functions.dayname()pyspark.sql.functions.dayofyear()pyspark.sql.functions.dayofmonth()pyspark.sql.functions.dayofweek()pyspark.sql.functions.extract()pyspark.sql.functions.datepart()pyspark.sql.functions.date_part()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.day('dt')).show()\n+----------+----------+-------+\n|        dt|typeof(dt)|day(dt)|\n+----------+----------+-------+\n|2015-04-08|    string|      8|\n|2024-10-31|    string|     31|\n+----------+----------+-------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.day('ts')).show()\n+-------------------+----------+-------+\n|                 ts|typeof(ts)|day(ts)|\n+-------------------+----------+-------+\n|2015-04-08 13:08:15|    string|      8|\n|2024-10-31 10:09:16|    string|     31|\n+-------------------+----------+-------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.date(2015, 4, 8),),\n...     (datetime.date(2024, 10, 31),)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.day('dt')).show()\n+----------+----------+-------+\n|        dt|typeof(dt)|day(dt)|\n+----------+----------+-------+\n|2015-04-08|      date|      8|\n|2024-10-31|      date|     31|\n+----------+----------+-------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.day('ts')).show()\n+-------------------+----------+-------+\n|                 ts|typeof(ts)|day(ts)|\n+-------------------+----------+-------+\n|2015-04-08 13:08:15| timestamp|      8|\n|2024-10-31 10:09:16| timestamp|     31|\n+-------------------+----------+-------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "day of the month for given date/timestamp as integer."]], "Category": ["Functions"], "index": 264}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.dayname.html#pyspark.sql.functions.dayname"], "Title": ["dayname"], "Feature": ["dayname"], "Description": "Date and Timestamp Function: Returns the three-letter abbreviated day name from the given date.\nSee alsopyspark.sql.functions.day()pyspark.sql.functions.monthname()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.dayname('dt')).show()\n+----------+----------+-----------+\n|        dt|typeof(dt)|dayname(dt)|\n+----------+----------+-----------+\n|2015-04-08|    string|        Wed|\n|2024-10-31|    string|        Thu|\n+----------+----------+-----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.dayname('ts')).show()\n+-------------------+----------+-----------+\n|                 ts|typeof(ts)|dayname(ts)|\n+-------------------+----------+-----------+\n|2015-04-08 13:08:15|    string|        Wed|\n|2024-10-31 10:09:16|    string|        Thu|\n+-------------------+----------+-----------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.date(2015, 4, 8),),\n...     (datetime.date(2024, 10, 31),)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.dayname('dt')).show()\n+----------+----------+-----------+\n|        dt|typeof(dt)|dayname(dt)|\n+----------+----------+-----------+\n|2015-04-08|      date|        Wed|\n|2024-10-31|      date|        Thu|\n+----------+----------+-----------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.dayname('ts')).show()\n+-------------------+----------+-----------+\n|                 ts|typeof(ts)|dayname(ts)|\n+-------------------+----------+-----------+\n|2015-04-08 13:08:15| timestamp|        Wed|\n|2024-10-31 10:09:16| timestamp|        Thu|\n+-------------------+----------+-----------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "the three-letter abbreviation of day name for date/timestamp (Mon, Tue, Wed…)"]], "Category": ["Functions"], "index": 265}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.dayofmonth.html#pyspark.sql.functions.dayofmonth"], "Title": ["dayofmonth"], "Feature": ["dayofmonth"], "Description": "Extract the day of the month of a given date/timestamp as integer.\nSee alsopyspark.sql.functions.day()pyspark.sql.functions.dayofyear()pyspark.sql.functions.dayofweek()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.dayofmonth('dt')).show()\n+----------+----------+--------------+\n|        dt|typeof(dt)|dayofmonth(dt)|\n+----------+----------+--------------+\n|2015-04-08|    string|             8|\n|2024-10-31|    string|            31|\n+----------+----------+--------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.dayofmonth('ts')).show()\n+-------------------+----------+--------------+\n|                 ts|typeof(ts)|dayofmonth(ts)|\n+-------------------+----------+--------------+\n|2015-04-08 13:08:15|    string|             8|\n|2024-10-31 10:09:16|    string|            31|\n+-------------------+----------+--------------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.date(2015, 4, 8),),\n...     (datetime.date(2024, 10, 31),)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.dayofmonth('dt')).show()\n+----------+----------+--------------+\n|        dt|typeof(dt)|dayofmonth(dt)|\n+----------+----------+--------------+\n|2015-04-08|      date|             8|\n|2024-10-31|      date|            31|\n+----------+----------+--------------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.dayofmonth('ts')).show()\n+-------------------+----------+--------------+\n|                 ts|typeof(ts)|dayofmonth(ts)|\n+-------------------+----------+--------------+\n|2015-04-08 13:08:15| timestamp|             8|\n|2024-10-31 10:09:16| timestamp|            31|\n+-------------------+----------+--------------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "day of the month for given date/timestamp as integer."]], "Category": ["Functions"], "index": 266}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.dayofweek.html#pyspark.sql.functions.dayofweek"], "Title": ["dayofweek"], "Feature": ["dayofweek"], "Description": "Extract the day of the week of a given date/timestamp as integer.\nRanges from 1 for a Sunday through to 7 for a Saturday\nSee alsopyspark.sql.functions.day()pyspark.sql.functions.dayofyear()pyspark.sql.functions.dayofmonth()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.dayofweek('dt')).show()\n+----------+----------+-------------+\n|        dt|typeof(dt)|dayofweek(dt)|\n+----------+----------+-------------+\n|2015-04-08|    string|            4|\n|2024-10-31|    string|            5|\n+----------+----------+-------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.dayofweek('ts')).show()\n+-------------------+----------+-------------+\n|                 ts|typeof(ts)|dayofweek(ts)|\n+-------------------+----------+-------------+\n|2015-04-08 13:08:15|    string|            4|\n|2024-10-31 10:09:16|    string|            5|\n+-------------------+----------+-------------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.date(2015, 4, 8),),\n...     (datetime.date(2024, 10, 31),)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.dayofweek('dt')).show()\n+----------+----------+-------------+\n|        dt|typeof(dt)|dayofweek(dt)|\n+----------+----------+-------------+\n|2015-04-08|      date|            4|\n|2024-10-31|      date|            5|\n+----------+----------+-------------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.dayofweek('ts')).show()\n+-------------------+----------+-------------+\n|                 ts|typeof(ts)|dayofweek(ts)|\n+-------------------+----------+-------------+\n|2015-04-08 13:08:15| timestamp|            4|\n|2024-10-31 10:09:16| timestamp|            5|\n+-------------------+----------+-------------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "day of the week for given date/timestamp as integer."]], "Category": ["Functions"], "index": 267}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.fillna.html#pyspark.sql.DataFrame.fillna"], "Title": ["DataFrame.fillna"], "Feature": ["DataFrame.fillna"], "Description": "Returns a newDataFramewhich null values are filled with new value.DataFrame.fillna()andDataFrameNaFunctions.fill()are\naliases of each other.", "Examples": [">>> df = spark.createDataFrame([\n...     (10, 80.5, \"Alice\", None),\n...     (5, None, \"Bob\", None),\n...     (None, None, \"Tom\", None),\n...     (None, None, None, True)],\n...     schema=[\"age\", \"height\", \"name\", \"bool\"])", ">>> df.na.fill(50).show()\n+---+------+-----+----+\n|age|height| name|bool|\n+---+------+-----+----+\n| 10|  80.5|Alice|NULL|\n|  5|  50.0|  Bob|NULL|\n| 50|  50.0|  Tom|NULL|\n| 50|  50.0| NULL|true|\n+---+------+-----+----+", ">>> df.na.fill(False).show()\n+----+------+-----+-----+\n| age|height| name| bool|\n+----+------+-----+-----+\n|  10|  80.5|Alice|false|\n|   5|  NULL|  Bob|false|\n|NULL|  NULL|  Tom|false|\n|NULL|  NULL| NULL| true|\n+----+------+-----+-----+", ">>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n+---+------+-------+----+\n|age|height|   name|bool|\n+---+------+-------+----+\n| 10|  80.5|  Alice|NULL|\n|  5|  NULL|    Bob|NULL|\n| 50|  NULL|    Tom|NULL|\n| 50|  NULL|unknown|true|\n+---+------+-------+----+", ">>> df.na.fill(value = 'Spark', subset = 'name').show()\n+----+------+-----+----+\n| age|height| name|bool|\n+----+------+-----+----+\n|  10|  80.5|Alice|NULL|\n|   5|  NULL|  Bob|NULL|\n|NULL|  NULL|  Tom|NULL|\n|NULL|  NULL|Spark|true|\n+----+------+-----+----+"], "Parameters": [["value int, float, string, bool or dict, the value to replace null values with.", "If the value is a dict, then subset is ignored and value must be a mapping\nfrom column name (string) to replacement value. The replacement value must be\nan int, float, boolean, or string."], ["subset str, tuple or list, optional", "optional list of column names to consider.\nColumns specified in subset that do not have matching data types are ignored.\nFor example, if value is a string, and subset contains a non-string column,\nthen the non-string column is simply ignored."]], "Returns": [["DataFrame", "DataFrame with replaced null values."]], "Category": ["DataFrame"], "index": 268}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.dayofyear.html#pyspark.sql.functions.dayofyear"], "Title": ["dayofyear"], "Feature": ["dayofyear"], "Description": "Extract the day of the year of a given date/timestamp as integer.\nSee alsopyspark.sql.functions.day()pyspark.sql.functions.dayofyear()pyspark.sql.functions.dayofmonth()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.dayofyear('dt')).show()\n+----------+----------+-------------+\n|        dt|typeof(dt)|dayofyear(dt)|\n+----------+----------+-------------+\n|2015-04-08|    string|           98|\n|2024-10-31|    string|          305|\n+----------+----------+-------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.dayofyear('ts')).show()\n+-------------------+----------+-------------+\n|                 ts|typeof(ts)|dayofyear(ts)|\n+-------------------+----------+-------------+\n|2015-04-08 13:08:15|    string|           98|\n|2024-10-31 10:09:16|    string|          305|\n+-------------------+----------+-------------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.date(2015, 4, 8),),\n...     (datetime.date(2024, 10, 31),)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.dayofyear('dt')).show()\n+----------+----------+-------------+\n|        dt|typeof(dt)|dayofyear(dt)|\n+----------+----------+-------------+\n|2015-04-08|      date|           98|\n|2024-10-31|      date|          305|\n+----------+----------+-------------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.dayofyear('ts')).show()\n+-------------------+----------+-------------+\n|                 ts|typeof(ts)|dayofyear(ts)|\n+-------------------+----------+-------------+\n|2015-04-08 13:08:15| timestamp|           98|\n|2024-10-31 10:09:16| timestamp|          305|\n+-------------------+----------+-------------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "day of the year for given date/timestamp as integer."]], "Category": ["Functions"], "index": 269}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.extract.html#pyspark.sql.functions.extract"], "Title": ["extract"], "Feature": ["extract"], "Description": "Extracts a part of the date/timestamp or interval source.\nSee alsopyspark.sql.functions.year()pyspark.sql.functions.quarter()pyspark.sql.functions.month()pyspark.sql.functions.day()pyspark.sql.functions.hour()pyspark.sql.functions.minute()pyspark.sql.functions.second()pyspark.sql.functions.datepart()pyspark.sql.functions.date_part()", "Examples": [">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n>>> df.select(\n...     '*',\n...     sf.extract(sf.lit('YEAR'), 'ts').alias('year'),\n...     sf.extract(sf.lit('month'), 'ts').alias('month'),\n...     sf.extract(sf.lit('WEEK'), 'ts').alias('week'),\n...     sf.extract(sf.lit('D'), df.ts).alias('day'),\n...     sf.extract(sf.lit('M'), df.ts).alias('minute'),\n...     sf.extract(sf.lit('S'), df.ts).alias('second')\n... ).show()\n+-------------------+----+-----+----+---+------+---------+\n|                 ts|year|month|week|day|minute|   second|\n+-------------------+----+-----+----+---+------+---------+\n|2015-04-08 13:08:15|2015|    4|  15|  8|     8|15.000000|\n+-------------------+----+-----+----+---+------+---------+\n"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 270}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.from_unixtime.html#pyspark.sql.functions.from_unixtime"], "Title": ["from_unixtime"], "Feature": ["from_unixtime"], "Description": "Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\nrepresenting the timestamp of that moment in the current system time zone in the given\nformat.\nSee alsopyspark.sql.functions.date_from_unix_date()pyspark.sql.functions.unix_seconds()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n>>> df.select('*', sf.from_unixtime('unix_time')).show()\n+----------+---------------------------------------------+\n| unix_time|from_unixtime(unix_time, yyyy-MM-dd HH:mm:ss)|\n+----------+---------------------------------------------+\n|1428476400|                          2015-04-08 00:00:00|\n+----------+---------------------------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["timestamp Column or column name", "column of unix time values."], ["format literal string, optional", "format to use to convert to (default: yyyy-MM-dd HH:mm:ss)"]], "Returns": [["Column", "formatted timestamp as string."]], "Category": ["Functions"], "index": 271}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.from_utc_timestamp.html#pyspark.sql.functions.from_utc_timestamp"], "Title": ["from_utc_timestamp"], "Feature": ["from_utc_timestamp"], "Description": "This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\ntakes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\nrenders that timestamp as a timestamp in the given time zone.\nHowever, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\ntimezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\nthe given timezone.\nThis function may return confusing result if the input is a string with timezone, e.g.\n‘2018-03-13T06:18:23+00:00’. The reason is that, Spark firstly cast the string to timestamp\naccording to the timezone in the string, and finally display the result by converting the\ntimestamp to string according to the session local timezone.\nSee alsopyspark.sql.functions.to_utc_timestamp()pyspark.sql.functions.to_timestamp()pyspark.sql.functions.to_timestamp_ltz()pyspark.sql.functions.to_timestamp_ntz()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n>>> df.select('*', sf.from_utc_timestamp('ts', 'PST')).show()\n+-------------------+---+---------------------------+\n|                 ts| tz|from_utc_timestamp(ts, PST)|\n+-------------------+---+---------------------------+\n|1997-02-28 10:30:00|JST|        1997-02-28 02:30:00|\n+-------------------+---+---------------------------+", ">>> df.select('*', sf.from_utc_timestamp(df.ts, df.tz)).show()\n+-------------------+---+--------------------------+\n|                 ts| tz|from_utc_timestamp(ts, tz)|\n+-------------------+---+--------------------------+\n|1997-02-28 10:30:00|JST|       1997-02-28 19:30:00|\n+-------------------+---+--------------------------+"], "Parameters": [["timestamp Column or column name", "the column that contains timestamps"], ["tz Column or literal string", "A string detailing the time zone ID that the input should be adjusted to. It should\nbe in the format of either region-based zone IDs or zone offsets. Region IDs must\nhave the form ‘area/city’, such as ‘America/Los_Angeles’. Zone offsets must be in\nthe format ‘(+|-)HH:mm’, for example ‘-08:00’ or ‘+01:00’. Also ‘UTC’ and ‘Z’ are\nsupported as aliases of ‘+00:00’. Other short names are not recommended to use\nbecause they can be ambiguous. Changed in version 2.4: tz can take a Column containing timezone ID strings."]], "Returns": [["Column", "timestamp value represented in given timezone."]], "Category": ["Functions"], "index": 272}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.hour.html#pyspark.sql.functions.hour"], "Title": ["hour"], "Feature": ["hour"], "Description": "Extract the hours of a given timestamp as integer.\nSee alsopyspark.sql.functions.year()pyspark.sql.functions.quarter()pyspark.sql.functions.month()pyspark.sql.functions.day()pyspark.sql.functions.minute()pyspark.sql.functions.second()pyspark.sql.functions.extract()pyspark.sql.functions.datepart()pyspark.sql.functions.date_part()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.hour('ts')).show()\n+-------------------+----------+--------+\n|                 ts|typeof(ts)|hour(ts)|\n+-------------------+----------+--------+\n|2015-04-08 13:08:15|    string|      13|\n|2024-10-31 10:09:16|    string|      10|\n+-------------------+----------+--------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.hour('ts')).show()\n+-------------------+----------+--------+\n|                 ts|typeof(ts)|hour(ts)|\n+-------------------+----------+--------+\n|2015-04-08 13:08:15| timestamp|      13|\n|2024-10-31 10:09:16| timestamp|      10|\n+-------------------+----------+--------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "hour part of the timestamp as integer."]], "Category": ["Functions"], "index": 273}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.last_day.html#pyspark.sql.functions.last_day"], "Title": ["last_day"], "Feature": ["last_day"], "Description": "Returns the last day of the month which the given date belongs to.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('1997-02-10',)], ['dt'])\n>>> df.select('*', sf.last_day(df.dt)).show()\n+----------+------------+\n|        dt|last_day(dt)|\n+----------+------------+\n|1997-02-10|  1997-02-28|\n+----------+------------+", ">>> df.select('*', sf.last_day('dt')).show()\n+----------+------------+\n|        dt|last_day(dt)|\n+----------+------------+\n|1997-02-10|  1997-02-28|\n+----------+------------+"], "Parameters": [["date Column or column name", "target column to compute on."]], "Returns": [["Column", "last day of the month."]], "Category": ["Functions"], "index": 274}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.localtimestamp.html#pyspark.sql.functions.localtimestamp"], "Title": ["localtimestamp"], "Feature": ["localtimestamp"], "Description": "Returns the current timestamp without time zone at the start of query evaluation\nas a timestamp without time zone column. All calls of localtimestamp within the\nsame query return the same value.\nSee alsopyspark.sql.functions.now()pyspark.sql.functions.curdate()pyspark.sql.functions.current_date()pyspark.sql.functions.current_timestamp()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.localtimestamp()).show(truncate=False) \n+-----------------------+\n|localtimestamp()       |\n+-----------------------+\n|2022-08-26 21:28:34.639|\n+-----------------------+"], "Parameters": [], "Returns": [["Column", "current local date and time."]], "Category": ["Functions"], "index": 275}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.make_date.html#pyspark.sql.functions.make_date"], "Title": ["make_date"], "Feature": ["make_date"], "Description": "Returns a column with a date built from the year, month and day columns.\nSee alsopyspark.sql.functions.make_timestamp()pyspark.sql.functions.make_timestamp_ltz()pyspark.sql.functions.make_timestamp_ntz()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(2020, 6, 26)], ['Y', 'M', 'D'])\n>>> df.select('*', sf.make_date(df.Y, 'M', df.D)).show()\n+----+---+---+------------------+\n|   Y|  M|  D|make_date(Y, M, D)|\n+----+---+---+------------------+\n|2020|  6| 26|        2020-06-26|\n+----+---+---+------------------+"], "Parameters": [["year Column or column name", "The year to build the date"], ["month Column or column name", "The month to build the date"], ["day Column or column name", "The day to build the date"]], "Returns": [["Column", "a date built from given parts."]], "Category": ["Functions"], "index": 276}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.make_dt_interval.html#pyspark.sql.functions.make_dt_interval"], "Title": ["make_dt_interval"], "Feature": ["make_dt_interval"], "Description": "Make DayTimeIntervalType duration from days, hours, mins and secs.\nSee alsopyspark.sql.functions.make_interval()pyspark.sql.functions.make_ym_interval()pyspark.sql.functions.try_make_interval()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[1, 12, 30, 01.001001]], ['day', 'hour', 'min', 'sec'])\n>>> df.select('*', sf.make_dt_interval(df.day, df.hour, df.min, df.sec)).show(truncate=False)\n+---+----+---+--------+------------------------------------------+\n|day|hour|min|sec     |make_dt_interval(day, hour, min, sec)     |\n+---+----+---+--------+------------------------------------------+\n|1  |12  |30 |1.001001|INTERVAL '1 12:30:01.001001' DAY TO SECOND|\n+---+----+---+--------+------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[1, 12, 30, 01.001001]], ['day', 'hour', 'min', 'sec'])\n>>> df.select('*', sf.make_dt_interval(df.day, 'hour', df.min)).show(truncate=False)\n+---+----+---+--------+-----------------------------------+\n|day|hour|min|sec     |make_dt_interval(day, hour, min, 0)|\n+---+----+---+--------+-----------------------------------+\n|1  |12  |30 |1.001001|INTERVAL '1 12:30:00' DAY TO SECOND|\n+---+----+---+--------+-----------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[1, 12, 30, 01.001001]], ['day', 'hour', 'min', 'sec'])\n>>> df.select('*', sf.make_dt_interval(df.day, df.hour)).show(truncate=False)\n+---+----+---+--------+-----------------------------------+\n|day|hour|min|sec     |make_dt_interval(day, hour, 0, 0)  |\n+---+----+---+--------+-----------------------------------+\n|1  |12  |30 |1.001001|INTERVAL '1 12:00:00' DAY TO SECOND|\n+---+----+---+--------+-----------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[1, 12, 30, 01.001001]], ['day', 'hour', 'min', 'sec'])\n>>> df.select('*', sf.make_dt_interval('day')).show(truncate=False)\n+---+----+---+--------+-----------------------------------+\n|day|hour|min|sec     |make_dt_interval(day, 0, 0, 0)     |\n+---+----+---+--------+-----------------------------------+\n|1  |12  |30 |1.001001|INTERVAL '1 00:00:00' DAY TO SECOND|\n+---+----+---+--------+-----------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.make_dt_interval()).show(truncate=False)\n+-----------------------------------+\n|make_dt_interval(0, 0, 0, 0)       |\n+-----------------------------------+\n|INTERVAL '0 00:00:00' DAY TO SECOND|\n+-----------------------------------+"], "Parameters": [["days Column or column name, optional", "The number of days, positive or negative."], ["hours Column or column name, optional", "The number of hours, positive or negative."], ["mins Column or column name, optional", "The number of minutes, positive or negative."], ["secs Column or column name, optional", "The number of seconds with the fractional part in microsecond precision."]], "Returns": [["Column", "A new column that contains a DayTimeIntervalType duration."]], "Category": ["Functions"], "index": 277}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.make_interval.html#pyspark.sql.functions.make_interval"], "Title": ["make_interval"], "Feature": ["make_interval"], "Description": "Make interval from years, months, weeks, days, hours, mins and secs.\nSee alsopyspark.sql.functions.make_dt_interval()pyspark.sql.functions.make_ym_interval()pyspark.sql.functions.try_make_interval()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(\n...     sf.make_interval(df.year, df.month, 'week', df.day, df.hour, df.min, df.sec)\n... ).show(truncate=False)\n+---------------------------------------------------------------+\n|make_interval(year, month, week, day, hour, min, sec)          |\n+---------------------------------------------------------------+\n|100 years 11 months 8 days 12 hours 30 minutes 1.001001 seconds|\n+---------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(\n...     sf.make_interval(df.year, df.month, 'week', df.day, df.hour, df.min)\n... ).show(truncate=False)\n+---------------------------------------------------+\n|make_interval(year, month, week, day, hour, min, 0)|\n+---------------------------------------------------+\n|100 years 11 months 8 days 12 hours 30 minutes     |\n+---------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(\n...     sf.make_interval(df.year, df.month, 'week', df.day, df.hour)\n... ).show(truncate=False)\n+-------------------------------------------------+\n|make_interval(year, month, week, day, hour, 0, 0)|\n+-------------------------------------------------+\n|100 years 11 months 8 days 12 hours              |\n+-------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(sf.make_interval(df.year, df.month, 'week', df.day)).show(truncate=False)\n+----------------------------------------------+\n|make_interval(year, month, week, day, 0, 0, 0)|\n+----------------------------------------------+\n|100 years 11 months 8 days                    |\n+----------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(sf.make_interval(df.year, df.month, 'week')).show(truncate=False)\n+--------------------------------------------+\n|make_interval(year, month, week, 0, 0, 0, 0)|\n+--------------------------------------------+\n|100 years 11 months 7 days                  |\n+--------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(sf.make_interval(df.year, df.month)).show(truncate=False)\n+-----------------------------------------+\n|make_interval(year, month, 0, 0, 0, 0, 0)|\n+-----------------------------------------+\n|100 years 11 months                      |\n+-----------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(sf.make_interval(df.year)).show(truncate=False)\n+-------------------------------------+\n|make_interval(year, 0, 0, 0, 0, 0, 0)|\n+-------------------------------------+\n|100 years                            |\n+-------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.make_interval()).show(truncate=False)\n+----------------------------------+\n|make_interval(0, 0, 0, 0, 0, 0, 0)|\n+----------------------------------+\n|0 seconds                         |\n+----------------------------------+"], "Parameters": [["years Column or column name, optional", "The number of years, positive or negative."], ["months Column or column name, optional", "The number of months, positive or negative."], ["weeks Column or column name, optional", "The number of weeks, positive or negative."], ["days Column or column name, optional", "The number of days, positive or negative."], ["hours Column or column name, optional", "The number of hours, positive or negative."], ["mins Column or column name, optional", "The number of minutes, positive or negative."], ["secs Column or column name, optional", "The number of seconds with the fractional part in microsecond precision."]], "Returns": [["Column", "A new column that contains an interval."]], "Category": ["Functions"], "index": 278}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.first.html#pyspark.sql.DataFrame.first"], "Title": ["DataFrame.first"], "Feature": ["DataFrame.first"], "Description": "Returns the first row as aRow.", "Examples": [">>> df = spark.createDataFrame([\n...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.first()\nRow(age=2, name='Alice')"], "Parameters": [], "Returns": [["Row", "First row if DataFrame is not empty, otherwise None ."]], "Category": ["DataFrame"], "index": 279}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.make_timestamp.html#pyspark.sql.functions.make_timestamp"], "Title": ["make_timestamp"], "Feature": ["make_timestamp"], "Description": "Create timestamp from years, months, days, hours, mins, secs and timezone fields.\nThe result data type is consistent with the value of configurationspark.sql.timestampType.\nIf the configurationspark.sql.ansi.enabledis false, the function returns NULL\non invalid inputs. Otherwise, it will throw an error instead.\nSee alsopyspark.sql.functions.make_timestamp_ltz()pyspark.sql.functions.make_timestamp_ntz()pyspark.sql.functions.try_make_timestamp()pyspark.sql.functions.try_make_timestamp_ltz()pyspark.sql.functions.try_make_timestamp_ntz()pyspark.sql.functions.make_interval()pyspark.sql.functions.try_make_interval()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887, 'CET']],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec', 'tz'])\n>>> df.select(\n...     sf.make_timestamp(df.year, df.month, df.day, 'hour', df.min, df.sec, 'tz')\n... ).show(truncate=False)\n+----------------------------------------------------+\n|make_timestamp(year, month, day, hour, min, sec, tz)|\n+----------------------------------------------------+\n|2014-12-27 21:30:45.887                             |\n+----------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887, 'CET']],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec', 'tz'])\n>>> df.select(\n...     sf.make_timestamp(df.year, df.month, df.day, 'hour', df.min, df.sec)\n... ).show(truncate=False)\n+------------------------------------------------+\n|make_timestamp(year, month, day, hour, min, sec)|\n+------------------------------------------------+\n|2014-12-28 06:30:45.887                         |\n+------------------------------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["years Column or column name", "The year to represent, from 1 to 9999"], ["months Column or column name", "The month-of-year to represent, from 1 (January) to 12 (December)"], ["days Column or column name", "The day-of-month to represent, from 1 to 31"], ["hours Column or column name", "The hour-of-day to represent, from 0 to 23"], ["mins Column or column name", "The minute-of-hour to represent, from 0 to 59"], ["secs Column or column name", "The second-of-minute and its micro-fraction to represent, from 0 to 60.\nThe value can be either an integer like 13 , or a fraction like 13.123.\nIf the sec argument equals to 60, the seconds field is set\nto 0 and 1 minute is added to the final timestamp."], ["timezone Column or column name, optional", "The time zone identifier. For example, CET, UTC and etc."]], "Returns": [["Column", "A new column that contains a timestamp."]], "Category": ["Functions"], "index": 280}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.make_timestamp_ltz.html#pyspark.sql.functions.make_timestamp_ltz"], "Title": ["make_timestamp_ltz"], "Feature": ["make_timestamp_ltz"], "Description": "Create the current timestamp with local time zone from years, months, days, hours, mins,\nsecs and timezone fields. If the configurationspark.sql.ansi.enabledis false,\nthe function returns NULL on invalid inputs. Otherwise, it will throw an error instead.\nSee alsopyspark.sql.functions.make_timestamp()pyspark.sql.functions.make_timestamp_ntz()pyspark.sql.functions.try_make_timestamp()pyspark.sql.functions.try_make_timestamp_ltz()pyspark.sql.functions.try_make_timestamp_ntz()pyspark.sql.functions.make_interval()pyspark.sql.functions.try_make_interval()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887, 'CET']],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec', 'tz'])\n>>> df.select(\n...     sf.make_timestamp_ltz(df.year, df.month, 'day', df.hour, df.min, df.sec, 'tz')\n... ).show(truncate=False)\n+--------------------------------------------------------+\n|make_timestamp_ltz(year, month, day, hour, min, sec, tz)|\n+--------------------------------------------------------+\n|2014-12-27 21:30:45.887                                 |\n+--------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887, 'CET']],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec', 'tz'])\n>>> df.select(\n...     sf.make_timestamp_ltz(df.year, df.month, 'day', df.hour, df.min, df.sec)\n... ).show(truncate=False)\n+----------------------------------------------------+\n|make_timestamp_ltz(year, month, day, hour, min, sec)|\n+----------------------------------------------------+\n|2014-12-28 06:30:45.887                             |\n+----------------------------------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["years Column or str", "The year to represent, from 1 to 9999"], ["months Column or str", "The month-of-year to represent, from 1 (January) to 12 (December)"], ["days Column or str", "The day-of-month to represent, from 1 to 31"], ["hours Column or str", "The hour-of-day to represent, from 0 to 23"], ["mins Column or str", "The minute-of-hour to represent, from 0 to 59"], ["secs Column or str", "The second-of-minute and its micro-fraction to represent, from 0 to 60.\nThe value can be either an integer like 13 , or a fraction like 13.123.\nIf the sec argument equals to 60, the seconds field is set\nto 0 and 1 minute is added to the final timestamp."], ["timezone Column or str, optional", "The time zone identifier. For example, CET, UTC and etc."]], "Returns": [["Column", "A new column that contains a current timestamp."]], "Category": ["Functions"], "index": 281}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.make_timestamp_ntz.html#pyspark.sql.functions.make_timestamp_ntz"], "Title": ["make_timestamp_ntz"], "Feature": ["make_timestamp_ntz"], "Description": "Create local date-time from years, months, days, hours, mins, secs fields.\nIf the configurationspark.sql.ansi.enabledis false, the function returns NULL\non invalid inputs. Otherwise, it will throw an error instead.\nSee alsopyspark.sql.functions.make_timestamp()pyspark.sql.functions.make_timestamp_ltz()pyspark.sql.functions.try_make_timestamp()pyspark.sql.functions.try_make_timestamp_ltz()pyspark.sql.functions.try_make_timestamp_ntz()pyspark.sql.functions.make_interval()pyspark.sql.functions.try_make_interval()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887]],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec'])\n>>> df.select(\n...     sf.make_timestamp_ntz('year', 'month', df.day, df.hour, df.min, df.sec)\n... ).show(truncate=False)\n+----------------------------------------------------+\n|make_timestamp_ntz(year, month, day, hour, min, sec)|\n+----------------------------------------------------+\n|2014-12-28 06:30:45.887                             |\n+----------------------------------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["years Column or column name", "The year to represent, from 1 to 9999"], ["months Column or column name", "The month-of-year to represent, from 1 (January) to 12 (December)"], ["days Column or column name", "The day-of-month to represent, from 1 to 31"], ["hours Column or column name", "The hour-of-day to represent, from 0 to 23"], ["mins Column or column name", "The minute-of-hour to represent, from 0 to 59"], ["secs Column or column name", "The second-of-minute and its micro-fraction to represent, from 0 to 60.\nThe value can be either an integer like 13 , or a fraction like 13.123.\nIf the sec argument equals to 60, the seconds field is set\nto 0 and 1 minute is added to the final timestamp."]], "Returns": [["Column", "A new column that contains a local date-time."]], "Category": ["Functions"], "index": 282}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.make_ym_interval.html#pyspark.sql.functions.make_ym_interval"], "Title": ["make_ym_interval"], "Feature": ["make_ym_interval"], "Description": "Make year-month interval from years, months.\nSee alsopyspark.sql.functions.make_interval()pyspark.sql.functions.make_dt_interval()pyspark.sql.functions.try_make_interval()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 12]], ['year', 'month'])\n>>> df.select('*', sf.make_ym_interval('year', df.month)).show(truncate=False)\n+----+-----+-------------------------------+\n|year|month|make_ym_interval(year, month)  |\n+----+-----+-------------------------------+\n|2014|12   |INTERVAL '2015-0' YEAR TO MONTH|\n+----+-----+-------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 12]], ['year', 'month'])\n>>> df.select('*', sf.make_ym_interval(df.year)).show(truncate=False)\n+----+-----+-------------------------------+\n|year|month|make_ym_interval(year, 0)      |\n+----+-----+-------------------------------+\n|2014|12   |INTERVAL '2014-0' YEAR TO MONTH|\n+----+-----+-------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.make_ym_interval()).show(truncate=False)\n+----------------------------+\n|make_ym_interval(0, 0)      |\n+----------------------------+\n|INTERVAL '0-0' YEAR TO MONTH|\n+----------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["years Column or column name, optional", "The number of years, positive or negative"], ["months Column or column name, optional", "The number of months, positive or negative"]], "Returns": [["Column", "A new column that contains a year-month interval."]], "Category": ["Functions"], "index": 283}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.minute.html#pyspark.sql.functions.minute"], "Title": ["minute"], "Feature": ["minute"], "Description": "Extract the minutes of a given timestamp as integer.\nSee alsopyspark.sql.functions.year()pyspark.sql.functions.quarter()pyspark.sql.functions.month()pyspark.sql.functions.day()pyspark.sql.functions.hour()pyspark.sql.functions.second()pyspark.sql.functions.extract()pyspark.sql.functions.datepart()pyspark.sql.functions.date_part()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.minute('ts')).show()\n+-------------------+----------+----------+\n|                 ts|typeof(ts)|minute(ts)|\n+-------------------+----------+----------+\n|2015-04-08 13:08:15|    string|         8|\n|2024-10-31 10:09:16|    string|         9|\n+-------------------+----------+----------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.minute('ts')).show()\n+-------------------+----------+----------+\n|                 ts|typeof(ts)|minute(ts)|\n+-------------------+----------+----------+\n|2015-04-08 13:08:15| timestamp|         8|\n|2024-10-31 10:09:16| timestamp|         9|\n+-------------------+----------+----------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "minutes part of the timestamp as integer."]], "Category": ["Functions"], "index": 284}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.month.html#pyspark.sql.functions.month"], "Title": ["month"], "Feature": ["month"], "Description": "Extract the month of a given date/timestamp as integer.\nSee alsopyspark.sql.functions.year()pyspark.sql.functions.quarter()pyspark.sql.functions.day()pyspark.sql.functions.hour()pyspark.sql.functions.minute()pyspark.sql.functions.second()pyspark.sql.functions.monthname()pyspark.sql.functions.extract()pyspark.sql.functions.datepart()pyspark.sql.functions.date_part()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.month('dt')).show()\n+----------+----------+---------+\n|        dt|typeof(dt)|month(dt)|\n+----------+----------+---------+\n|2015-04-08|    string|        4|\n|2024-10-31|    string|       10|\n+----------+----------+---------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.month('ts')).show()\n+-------------------+----------+---------+\n|                 ts|typeof(ts)|month(ts)|\n+-------------------+----------+---------+\n|2015-04-08 13:08:15|    string|        4|\n|2024-10-31 10:09:16|    string|       10|\n+-------------------+----------+---------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.date(2015, 4, 8),),\n...     (datetime.date(2024, 10, 31),)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.month('dt')).show()\n+----------+----------+---------+\n|        dt|typeof(dt)|month(dt)|\n+----------+----------+---------+\n|2015-04-08|      date|        4|\n|2024-10-31|      date|       10|\n+----------+----------+---------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.month('ts')).show()\n+-------------------+----------+---------+\n|                 ts|typeof(ts)|month(ts)|\n+-------------------+----------+---------+\n|2015-04-08 13:08:15| timestamp|        4|\n|2024-10-31 10:09:16| timestamp|       10|\n+-------------------+----------+---------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "month part of the date/timestamp as integer."]], "Category": ["Functions"], "index": 285}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.monthname.html#pyspark.sql.functions.monthname"], "Title": ["monthname"], "Feature": ["monthname"], "Description": "Returns the three-letter abbreviated month name from the given date.\nSee alsopyspark.sql.functions.month()pyspark.sql.functions.dayname()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.monthname('dt')).show()\n+----------+----------+-------------+\n|        dt|typeof(dt)|monthname(dt)|\n+----------+----------+-------------+\n|2015-04-08|    string|          Apr|\n|2024-10-31|    string|          Oct|\n+----------+----------+-------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.monthname('ts')).show()\n+-------------------+----------+-------------+\n|                 ts|typeof(ts)|monthname(ts)|\n+-------------------+----------+-------------+\n|2015-04-08 13:08:15|    string|          Apr|\n|2024-10-31 10:09:16|    string|          Oct|\n+-------------------+----------+-------------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.date(2015, 4, 8),),\n...     (datetime.date(2024, 10, 31),)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.monthname('dt')).show()\n+----------+----------+-------------+\n|        dt|typeof(dt)|monthname(dt)|\n+----------+----------+-------------+\n|2015-04-08|      date|          Apr|\n|2024-10-31|      date|          Oct|\n+----------+----------+-------------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.monthname('ts')).show()\n+-------------------+----------+-------------+\n|                 ts|typeof(ts)|monthname(ts)|\n+-------------------+----------+-------------+\n|2015-04-08 13:08:15| timestamp|          Apr|\n|2024-10-31 10:09:16| timestamp|          Oct|\n+-------------------+----------+-------------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "the three-letter abbreviation of month name for date/timestamp (Jan, Feb, Mar…)"]], "Category": ["Functions"], "index": 286}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.months_between.html#pyspark.sql.functions.months_between"], "Title": ["months_between"], "Feature": ["months_between"], "Description": "Returns number of months between dates date1 and date2.\nIf date1 is later than date2, then the result is positive.\nA whole number is returned if both inputs have the same day of month or both are the last day\nof their respective months. Otherwise, the difference is calculated assuming 31 days per month.\nThe result is rounded off to 8 digits unlessroundOffis set toFalse.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['d1', 'd2'])\n>>> df.select('*', sf.months_between(df.d1, df.d2)).show()\n+-------------------+----------+----------------------------+\n|                 d1|        d2|months_between(d1, d2, true)|\n+-------------------+----------+----------------------------+\n|1997-02-28 10:30:00|1996-10-30|                  3.94959677|\n+-------------------+----------+----------------------------+", ">>> df.select('*', sf.months_between('d2', 'd1')).show()\n+-------------------+----------+----------------------------+\n|                 d1|        d2|months_between(d2, d1, true)|\n+-------------------+----------+----------------------------+\n|1997-02-28 10:30:00|1996-10-30|                 -3.94959677|\n+-------------------+----------+----------------------------+", ">>> df.select('*', sf.months_between('d1', df.d2, False)).show()\n+-------------------+----------+-----------------------------+\n|                 d1|        d2|months_between(d1, d2, false)|\n+-------------------+----------+-----------------------------+\n|1997-02-28 10:30:00|1996-10-30|           3.9495967741935...|\n+-------------------+----------+-----------------------------+"], "Parameters": [["date1 Column or column name", "first date column."], ["date2 Column or column name", "second date column."], ["roundOff bool, optional", "whether to round (to 8 digits) the final value or not (default: True)."]], "Returns": [["Column", "number of months between two dates."]], "Category": ["Functions"], "index": 287}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.next_day.html#pyspark.sql.functions.next_day"], "Title": ["next_day"], "Feature": ["next_day"], "Description": "Returns the first date which is later than the value of the date column\nbased on secondweek dayargument.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-07-27',)], ['dt'])\n>>> df.select('*', sf.next_day(df.dt, 'Sun')).show()\n+----------+-----------------+\n|        dt|next_day(dt, Sun)|\n+----------+-----------------+\n|2015-07-27|       2015-08-02|\n+----------+-----------------+", ">>> df.select('*', sf.next_day('dt', 'Sat')).show()\n+----------+-----------------+\n|        dt|next_day(dt, Sat)|\n+----------+-----------------+\n|2015-07-27|       2015-08-01|\n+----------+-----------------+"], "Parameters": [["date Column or column name", "target column to compute on."], ["dayOfWeek literal string", "day of the week, case-insensitive, accepts: “Mon”, “Tue”, “Wed”, “Thu”, “Fri”, “Sat”, “Sun”"], ["day of the week, case-insensitive, accepts:", "“Mon”, “Tue”, “Wed”, “Thu”, “Fri”, “Sat”, “Sun”"]], "Returns": [["Column", "the column of computed results."]], "Category": ["Functions"], "index": 288}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.now.html#pyspark.sql.functions.now"], "Title": ["now"], "Feature": ["now"], "Description": "Returns the current timestamp at the start of query evaluation.\nSee alsopyspark.sql.functions.curdate()pyspark.sql.functions.current_date()pyspark.sql.functions.current_timestamp()pyspark.sql.functions.localtimestamp()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.now()).show(truncate=False) \n+--------------------------+\n|now()                     |\n+--------------------------+\n|2023-12-08 15:18:18.482269|\n+--------------------------+"], "Parameters": [], "Returns": [["Column", "current timestamp at the start of query evaluation."]], "Category": ["Functions"], "index": 289}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.foreach.html#pyspark.sql.DataFrame.foreach"], "Title": ["DataFrame.foreach"], "Feature": ["DataFrame.foreach"], "Description": "Applies theffunction to allRowof thisDataFrame.\nThis is a shorthand fordf.rdd.foreach().", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> def func(person):\n...     print(person.name)\n...\n>>> df.foreach(func)"], "Parameters": [["f function", "A function that accepts one parameter which will\nreceive each row to process."]], "Returns": [], "Category": ["DataFrame"], "index": 290}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.quarter.html#pyspark.sql.functions.quarter"], "Title": ["quarter"], "Feature": ["quarter"], "Description": "Extract the quarter of a given date/timestamp as integer.\nSee alsopyspark.sql.functions.year()pyspark.sql.functions.month()pyspark.sql.functions.day()pyspark.sql.functions.hour()pyspark.sql.functions.minute()pyspark.sql.functions.second()pyspark.sql.functions.extract()pyspark.sql.functions.datepart()pyspark.sql.functions.date_part()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.quarter('dt')).show()\n+----------+----------+-----------+\n|        dt|typeof(dt)|quarter(dt)|\n+----------+----------+-----------+\n|2015-04-08|    string|          2|\n|2024-10-31|    string|          4|\n+----------+----------+-----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.quarter('ts')).show()\n+-------------------+----------+-----------+\n|                 ts|typeof(ts)|quarter(ts)|\n+-------------------+----------+-----------+\n|2015-04-08 13:08:15|    string|          2|\n|2024-10-31 10:09:16|    string|          4|\n+-------------------+----------+-----------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.date(2015, 4, 8),),\n...     (datetime.date(2024, 10, 31),)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.quarter('dt')).show()\n+----------+----------+-----------+\n|        dt|typeof(dt)|quarter(dt)|\n+----------+----------+-----------+\n|2015-04-08|      date|          2|\n|2024-10-31|      date|          4|\n+----------+----------+-----------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.quarter('ts')).show()\n+-------------------+----------+-----------+\n|                 ts|typeof(ts)|quarter(ts)|\n+-------------------+----------+-----------+\n|2015-04-08 13:08:15| timestamp|          2|\n|2024-10-31 10:09:16| timestamp|          4|\n+-------------------+----------+-----------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "quarter of the date/timestamp as integer."]], "Category": ["Functions"], "index": 291}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.second.html#pyspark.sql.functions.second"], "Title": ["second"], "Feature": ["second"], "Description": "Extract the seconds of a given date as integer.\nSee alsopyspark.sql.functions.year()pyspark.sql.functions.quarter()pyspark.sql.functions.month()pyspark.sql.functions.day()pyspark.sql.functions.hour()pyspark.sql.functions.minute()pyspark.sql.functions.extract()pyspark.sql.functions.datepart()pyspark.sql.functions.date_part()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.second('ts')).show()\n+-------------------+----------+----------+\n|                 ts|typeof(ts)|second(ts)|\n+-------------------+----------+----------+\n|2015-04-08 13:08:15|    string|        15|\n|2024-10-31 10:09:16|    string|        16|\n+-------------------+----------+----------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.second('ts')).show()\n+-------------------+----------+----------+\n|                 ts|typeof(ts)|second(ts)|\n+-------------------+----------+----------+\n|2015-04-08 13:08:15| timestamp|        15|\n|2024-10-31 10:09:16| timestamp|        16|\n+-------------------+----------+----------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "seconds part of the timestamp as integer."]], "Category": ["Functions"], "index": 292}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.session_window.html#pyspark.sql.functions.session_window"], "Title": ["session_window"], "Feature": ["session_window"], "Description": "Generates session window given a timestamp specifying column.\nSession window is one of dynamic windows, which means the length of window is varying\naccording to the given inputs. The length of session window is defined as “the timestamp\nof latest input of the session + gap duration”, so when the new inputs are bound to the\ncurrent session window, the end time of session window can be expanded according to the new\ninputs.\nWindows can support microsecond precision. Windows in the order of months are not supported.\nFor a streaming query, you may use the functioncurrent_timestampto generate windows on\nprocessing time.\ngapDuration is provided as strings, e.g. ‘1 second’, ‘1 day 12 hours’, ‘2 minutes’. Valid\ninterval strings are ‘week’, ‘day’, ‘hour’, ‘minute’, ‘second’, ‘millisecond’, ‘microsecond’.\nIt could also be a Column which can be evaluated to gap duration dynamically based on the\ninput row.\nThe output column will be a struct called ‘session_window’ by default with the nested columns\n‘start’ and ‘end’, where ‘start’ and ‘end’ will be ofpyspark.sql.types.TimestampType.\nSee alsopyspark.sql.functions.window()pyspark.sql.functions.window_time()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2016-03-11 09:00:07', 1)], ['dt', 'v'])\n>>> df2 = df.groupBy(sf.session_window('dt', '5 seconds')).agg(sf.sum('v'))\n>>> df2.show(truncate=False)\n+------------------------------------------+------+\n|session_window                            |sum(v)|\n+------------------------------------------+------+\n|{2016-03-11 09:00:07, 2016-03-11 09:00:12}|1     |\n+------------------------------------------+------+", ">>> df2.printSchema()\nroot\n |-- session_window: struct (nullable = false)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n |-- sum(v): long (nullable = true)"], "Parameters": [["timeColumn Column or column name", "The column name or column to use as the timestamp for windowing by time.\nThe time column must be of TimestampType or TimestampNTZType."], ["gapDuration Column or literal string", "A Python string literal or column specifying the timeout of the session. It could be\nstatic value, e.g. 10 minutes , 1 second , or an expression/UDF that specifies gap\nduration dynamically based on the input row."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 293}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.timestamp_add.html#pyspark.sql.functions.timestamp_add"], "Title": ["timestamp_add"], "Feature": ["timestamp_add"], "Description": "Gets the difference between the timestamps in the specified units by truncating\nthe fraction part.\nSee alsopyspark.sql.functions.dateadd()pyspark.sql.functions.date_add()", "Examples": [">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...     [(datetime.datetime(2016, 3, 11, 9, 0, 7), 2),\n...      (datetime.datetime(2024, 4, 2, 9, 0, 7), 3)], ['ts', 'quantity'])\n>>> df.select('*', sf.timestamp_add('year', 'quantity', 'ts')).show()\n+-------------------+--------+--------------------------------+\n|                 ts|quantity|timestampadd(year, quantity, ts)|\n+-------------------+--------+--------------------------------+\n|2016-03-11 09:00:07|       2|             2018-03-11 09:00:07|\n|2024-04-02 09:00:07|       3|             2027-04-02 09:00:07|\n+-------------------+--------+--------------------------------+", ">>> df.select('*', sf.timestamp_add('WEEK', sf.lit(5), df.ts)).show()\n+-------------------+--------+-------------------------+\n|                 ts|quantity|timestampadd(WEEK, 5, ts)|\n+-------------------+--------+-------------------------+\n|2016-03-11 09:00:07|       2|      2016-04-15 09:00:07|\n|2024-04-02 09:00:07|       3|      2024-05-07 09:00:07|\n+-------------------+--------+-------------------------+", ">>> df.select('*', sf.timestamp_add('day', sf.lit(-5), 'ts')).show()\n+-------------------+--------+-------------------------+\n|                 ts|quantity|timestampadd(day, -5, ts)|\n+-------------------+--------+-------------------------+\n|2016-03-11 09:00:07|       2|      2016-03-06 09:00:07|\n|2024-04-02 09:00:07|       3|      2024-03-28 09:00:07|\n+-------------------+--------+-------------------------+"], "Parameters": [["unit literal string", "This indicates the units of the difference between the given timestamps.\nSupported options are (case insensitive): “YEAR”, “QUARTER”, “MONTH”, “WEEK”,\n“DAY”, “HOUR”, “MINUTE”, “SECOND”, “MILLISECOND” and “MICROSECOND”."], ["quantity Column or column name", "The number of units of time that you want to add."], ["ts Column or column name", "A timestamp to which you want to add."]], "Returns": [["Column", "the difference between the timestamps."]], "Category": ["Functions"], "index": 294}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.timestamp_diff.html#pyspark.sql.functions.timestamp_diff"], "Title": ["timestamp_diff"], "Feature": ["timestamp_diff"], "Description": "Gets the difference between the timestamps in the specified units by truncating\nthe fraction part.\nSee alsopyspark.sql.functions.datediff()pyspark.sql.functions.date_diff()", "Examples": [">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...     [(datetime.datetime(2016, 3, 11, 9, 0, 7), datetime.datetime(2024, 4, 2, 9, 0, 7))],\n...     ['ts1', 'ts2'])\n>>> df.select('*', sf.timestamp_diff('year', 'ts1', 'ts2')).show()\n+-------------------+-------------------+-----------------------------+\n|                ts1|                ts2|timestampdiff(year, ts1, ts2)|\n+-------------------+-------------------+-----------------------------+\n|2016-03-11 09:00:07|2024-04-02 09:00:07|                            8|\n+-------------------+-------------------+-----------------------------+", ">>> df.select('*', sf.timestamp_diff('WEEK', 'ts1', 'ts2')).show()\n+-------------------+-------------------+-----------------------------+\n|                ts1|                ts2|timestampdiff(WEEK, ts1, ts2)|\n+-------------------+-------------------+-----------------------------+\n|2016-03-11 09:00:07|2024-04-02 09:00:07|                          420|\n+-------------------+-------------------+-----------------------------+", ">>> df.select('*', sf.timestamp_diff('day', df.ts2, df.ts1)).show()\n+-------------------+-------------------+----------------------------+\n|                ts1|                ts2|timestampdiff(day, ts2, ts1)|\n+-------------------+-------------------+----------------------------+\n|2016-03-11 09:00:07|2024-04-02 09:00:07|                       -2944|\n+-------------------+-------------------+----------------------------+"], "Parameters": [["unit literal string", "This indicates the units of the difference between the given timestamps.\nSupported options are (case insensitive): “YEAR”, “QUARTER”, “MONTH”, “WEEK”,\n“DAY”, “HOUR”, “MINUTE”, “SECOND”, “MILLISECOND” and “MICROSECOND”."], ["start Column or column name", "A timestamp which the expression subtracts from endTimestamp ."], ["end Column or column name", "A timestamp from which the expression subtracts startTimestamp ."]], "Returns": [["Column", "the difference between the timestamps."]], "Category": ["Functions"], "index": 295}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.timestamp_micros.html#pyspark.sql.functions.timestamp_micros"], "Title": ["timestamp_micros"], "Feature": ["timestamp_micros"], "Description": "Creates timestamp from the number of microseconds since UTC epoch.\nSee alsopyspark.sql.functions.timestamp_seconds()pyspark.sql.functions.timestamp_millis()pyspark.sql.functions.unix_micros()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1230219000,), (1280219000,)], ['micros'])\n>>> df.select('*', sf.timestamp_micros('micros')).show(truncate=False)\n+----------+------------------------+\n|micros    |timestamp_micros(micros)|\n+----------+------------------------+\n|1230219000|1970-01-01 00:20:30.219 |\n|1280219000|1970-01-01 00:21:20.219 |\n+----------+------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["col Column or column name", "unix time values."]], "Returns": [["Column", "converted timestamp value."]], "Category": ["Functions"], "index": 296}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.timestamp_millis.html#pyspark.sql.functions.timestamp_millis"], "Title": ["timestamp_millis"], "Feature": ["timestamp_millis"], "Description": "Creates timestamp from the number of milliseconds since UTC epoch.\nSee alsopyspark.sql.functions.timestamp_seconds()pyspark.sql.functions.timestamp_micros()pyspark.sql.functions.unix_millis()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1230219000,), (1280219000,)], ['millis'])\n>>> df.select('*', sf.timestamp_millis('millis')).show()\n+----------+------------------------+\n|    millis|timestamp_millis(millis)|\n+----------+------------------------+\n|1230219000|     1970-01-15 05:43:39|\n|1280219000|     1970-01-15 19:36:59|\n+----------+------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["col Column or column name", "unix time values."]], "Returns": [["Column", "converted timestamp value."]], "Category": ["Functions"], "index": 297}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.timestamp_seconds.html#pyspark.sql.functions.timestamp_seconds"], "Title": ["timestamp_seconds"], "Feature": ["timestamp_seconds"], "Description": "Converts the number of seconds from the Unix epoch (1970-01-01T00:00:00Z)\nto a timestamp.\nSee alsopyspark.sql.functions.timestamp_millis()pyspark.sql.functions.timestamp_micros()pyspark.sql.functions.unix_seconds()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1230219000,), (1280219000,)], ['seconds'])\n>>> df.select('*', sf.timestamp_seconds('seconds')).show()\n+----------+--------------------------+\n|   seconds|timestamp_seconds(seconds)|\n+----------+--------------------------+\n|1230219000|       2008-12-25 15:30:00|\n|1280219000|       2010-07-27 08:23:20|\n+----------+--------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["col Column or column name", "unix time values."]], "Returns": [["Column", "converted timestamp value."]], "Category": ["Functions"], "index": 298}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_date.html#pyspark.sql.functions.to_date"], "Title": ["to_date"], "Feature": ["to_date"], "Description": "Converts aColumnintopyspark.sql.types.DateTypeusing the optionally specified format. Specify formats according todatetime pattern.\nBy default, it follows casting rules topyspark.sql.types.DateTypeif the format\nis omitted. Equivalent tocol.cast(\"date\")..\nSee alsopyspark.sql.functions.to_timestamp()pyspark.sql.functions.to_timestamp_ltz()pyspark.sql.functions.to_timestamp_ntz()pyspark.sql.functions.to_utc_timestamp()pyspark.sql.functions.try_to_timestamp()pyspark.sql.functions.date_format()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['ts'])\n>>> df.select('*', sf.to_date(df.ts)).show()\n+-------------------+-----------+\n|                 ts|to_date(ts)|\n+-------------------+-----------+\n|1997-02-28 10:30:00| 1997-02-28|\n+-------------------+-----------+", ">>> df.select('*', sf.to_date('ts', 'yyyy-MM-dd HH:mm:ss')).show()\n+-------------------+--------------------------------+\n|                 ts|to_date(ts, yyyy-MM-dd HH:mm:ss)|\n+-------------------+--------------------------------+\n|1997-02-28 10:30:00|                      1997-02-28|\n+-------------------+--------------------------------+"], "Parameters": [["col Column or column name", "input column of values to convert."], ["format: literal string, optional", "format to use to convert date values."]], "Returns": [["Column", "date value as pyspark.sql.types.DateType type."]], "Category": ["Functions"], "index": 299}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_timestamp.html#pyspark.sql.functions.to_timestamp"], "Title": ["to_timestamp"], "Feature": ["to_timestamp"], "Description": "Converts aColumnintopyspark.sql.types.TimestampTypeusing the optionally specified format. Specify formats according todatetime pattern.\nBy default, it follows casting rules topyspark.sql.types.TimestampTypeif the format\nis omitted. Equivalent tocol.cast(\"timestamp\")..\nSee alsopyspark.sql.functions.to_date()pyspark.sql.functions.to_timestamp_ltz()pyspark.sql.functions.to_timestamp_ntz()pyspark.sql.functions.to_utc_timestamp()pyspark.sql.functions.to_unix_timestamp()pyspark.sql.functions.try_to_timestamp()pyspark.sql.functions.date_format()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n>>> df.select(sf.to_timestamp(df.t)).show()\n+-------------------+\n|    to_timestamp(t)|\n+-------------------+\n|1997-02-28 10:30:00|\n+-------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n>>> df.select(sf.to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss')).show()\n+------------------------------------+\n|to_timestamp(t, yyyy-MM-dd HH:mm:ss)|\n+------------------------------------+\n|                 1997-02-28 10:30:00|\n+------------------------------------+"], "Parameters": [["col Column or column name", "column values to convert."], ["format: literal string, optional", "format to use to convert timestamp values."]], "Returns": [["Column", "timestamp value as pyspark.sql.types.TimestampType type."]], "Category": ["Functions"], "index": 300}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.foreachPartition.html#pyspark.sql.DataFrame.foreachPartition"], "Title": ["DataFrame.foreachPartition"], "Feature": ["DataFrame.foreachPartition"], "Description": "Applies theffunction to each partition of thisDataFrame.\nThis a shorthand fordf.rdd.foreachPartition().", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> def func(itr):\n...     for person in itr:\n...         print(person.name)\n...\n>>> df.foreachPartition(func)"], "Parameters": [["f function", "A function that accepts one parameter which will receive\neach partition to process."]], "Returns": [], "Category": ["DataFrame"], "index": 301}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_timestamp_ltz.html#pyspark.sql.functions.to_timestamp_ltz"], "Title": ["to_timestamp_ltz"], "Feature": ["to_timestamp_ltz"], "Description": "Parses thetimestampwith theformatto a timestamp with time zone.\nReturns null with invalid input.\nSee alsopyspark.sql.functions.to_date()pyspark.sql.functions.to_timestamp()pyspark.sql.functions.to_timestamp_ntz()pyspark.sql.functions.to_utc_timestamp()pyspark.sql.functions.to_unix_timestamp()pyspark.sql.functions.date_format()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 12:12:12',)], ['ts'])\n>>> df.select('*', sf.to_timestamp_ltz('ts')).show()\n+-------------------+--------------------+\n|                 ts|to_timestamp_ltz(ts)|\n+-------------------+--------------------+\n|2015-04-08 12:12:12| 2015-04-08 12:12:12|\n+-------------------+--------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2016-12-31',)], ['dt'])\n>>> df.select('*', sf.to_timestamp_ltz(df.dt, sf.lit('yyyy-MM-dd'))).show()\n+----------+--------------------------------+\n|        dt|to_timestamp_ltz(dt, yyyy-MM-dd)|\n+----------+--------------------------------+\n|2016-12-31|             2016-12-31 00:00:00|\n+----------+--------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame(\n...     [('2015-04-08', 'yyyy-MM-dd'), ('2025+01+09', 'yyyy+MM+dd')], ['dt', 'fmt'])\n>>> df.select('*', sf.to_timestamp_ltz('dt', 'fmt')).show()\n+----------+----------+-------------------------+\n|        dt|       fmt|to_timestamp_ltz(dt, fmt)|\n+----------+----------+-------------------------+\n|2015-04-08|yyyy-MM-dd|      2015-04-08 00:00:00|\n|2025+01+09|yyyy+MM+dd|      2025-01-09 00:00:00|\n+----------+----------+-------------------------+"], "Parameters": [["timestamp Column or column name", "Input column or strings."], ["format Column or column name, optional", "format to use to convert type TimestampType timestamp values."]], "Returns": [], "Category": ["Functions"], "index": 302}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_timestamp_ntz.html#pyspark.sql.functions.to_timestamp_ntz"], "Title": ["to_timestamp_ntz"], "Feature": ["to_timestamp_ntz"], "Description": "Parses thetimestampwith theformatto a timestamp without time zone.\nReturns null with invalid input.\nSee alsopyspark.sql.functions.to_date()pyspark.sql.functions.to_timestamp()pyspark.sql.functions.to_timestamp_ltz()pyspark.sql.functions.to_utc_timestamp()pyspark.sql.functions.to_unix_timestamp()pyspark.sql.functions.date_format()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 12:12:12',)], ['ts'])\n>>> df.select('*', sf.to_timestamp_ntz('ts')).show()\n+-------------------+--------------------+\n|                 ts|to_timestamp_ntz(ts)|\n+-------------------+--------------------+\n|2015-04-08 12:12:12| 2015-04-08 12:12:12|\n+-------------------+--------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2016-12-31',)], ['dt'])\n>>> df.select('*', sf.to_timestamp_ntz(df.dt, sf.lit('yyyy-MM-dd'))).show()\n+----------+--------------------------------+\n|        dt|to_timestamp_ntz(dt, yyyy-MM-dd)|\n+----------+--------------------------------+\n|2016-12-31|             2016-12-31 00:00:00|\n+----------+--------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame(\n...     [('2015-04-08', 'yyyy-MM-dd'), ('2025+01+09', 'yyyy+MM+dd')], ['dt', 'fmt'])\n>>> df.select('*', sf.to_timestamp_ntz('dt', 'fmt')).show()\n+----------+----------+-------------------------+\n|        dt|       fmt|to_timestamp_ntz(dt, fmt)|\n+----------+----------+-------------------------+\n|2015-04-08|yyyy-MM-dd|      2015-04-08 00:00:00|\n|2025+01+09|yyyy+MM+dd|      2025-01-09 00:00:00|\n+----------+----------+-------------------------+"], "Parameters": [["timestamp Column or column name", "Input column or strings."], ["format Column or column name, optional", "format to use to convert type TimestampNTZType timestamp values."]], "Returns": [], "Category": ["Functions"], "index": 303}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_unix_timestamp.html#pyspark.sql.functions.to_unix_timestamp"], "Title": ["to_unix_timestamp"], "Feature": ["to_unix_timestamp"], "Description": "Returns the UNIX timestamp of the given time.\nSee alsopyspark.sql.functions.to_date()pyspark.sql.functions.to_timestamp()pyspark.sql.functions.to_timestamp_ltz()pyspark.sql.functions.to_timestamp_ntz()pyspark.sql.functions.to_utc_timestamp()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 12:12:12',)], ['ts'])\n>>> df.select('*', sf.to_unix_timestamp('ts')).show()\n+-------------------+------------------------------------------+\n|                 ts|to_unix_timestamp(ts, yyyy-MM-dd HH:mm:ss)|\n+-------------------+------------------------------------------+\n|2015-04-08 12:12:12|                                1428520332|\n+-------------------+------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n>>> df.select('*', sf.to_unix_timestamp(df.dt, sf.lit('yyyy-MM-dd'))).show()\n+----------+---------------------------------+\n|        dt|to_unix_timestamp(dt, yyyy-MM-dd)|\n+----------+---------------------------------+\n|2015-04-08|                       1428476400|\n+----------+---------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame(\n...     [('2015-04-08', 'yyyy-MM-dd'), ('2025+01+09', 'yyyy+MM+dd')], ['dt', 'fmt'])\n>>> df.select('*', sf.to_unix_timestamp('dt', 'fmt')).show()\n+----------+----------+--------------------------+\n|        dt|       fmt|to_unix_timestamp(dt, fmt)|\n+----------+----------+--------------------------+\n|2015-04-08|yyyy-MM-dd|                1428476400|\n|2025+01+09|yyyy+MM+dd|                1736409600|\n+----------+----------+--------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["timestamp Column or column name", "Input column or strings."], ["format Column or column name, optional", "format to use to convert UNIX timestamp values."]], "Returns": [], "Category": ["Functions"], "index": 304}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_utc_timestamp.html#pyspark.sql.functions.to_utc_timestamp"], "Title": ["to_utc_timestamp"], "Feature": ["to_utc_timestamp"], "Description": "This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\ntakes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given\ntimezone, and renders that timestamp as a timestamp in UTC.\nHowever, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\ntimezone-agnostic. So in Spark this function just shift the timestamp value from the given\ntimezone to UTC timezone.\nThis function may return confusing result if the input is a string with timezone, e.g.\n‘2018-03-13T06:18:23+00:00’. The reason is that, Spark firstly cast the string to timestamp\naccording to the timezone in the string, and finally display the result by converting the\ntimestamp to string according to the session local timezone.\nSee alsopyspark.sql.functions.from_utc_timestamp()pyspark.sql.functions.to_timestamp()pyspark.sql.functions.to_timestamp_ltz()pyspark.sql.functions.to_timestamp_ntz()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n>>> df.select('*', sf.to_utc_timestamp('ts', \"PST\")).show()\n+-------------------+---+-------------------------+\n|                 ts| tz|to_utc_timestamp(ts, PST)|\n+-------------------+---+-------------------------+\n|1997-02-28 10:30:00|JST|      1997-02-28 18:30:00|\n+-------------------+---+-------------------------+", ">>> df.select('*', sf.to_utc_timestamp(df.ts, df.tz)).show()\n+-------------------+---+------------------------+\n|                 ts| tz|to_utc_timestamp(ts, tz)|\n+-------------------+---+------------------------+\n|1997-02-28 10:30:00|JST|     1997-02-28 01:30:00|\n+-------------------+---+------------------------+"], "Parameters": [["timestamp Column or column name", "the column that contains timestamps"], ["tz Column or literal string", "A string detailing the time zone ID that the input should be adjusted to. It should\nbe in the format of either region-based zone IDs or zone offsets. Region IDs must\nhave the form ‘area/city’, such as ‘America/Los_Angeles’. Zone offsets must be in\nthe format ‘(+|-)HH:mm’, for example ‘-08:00’ or ‘+01:00’. Also ‘UTC’ and ‘Z’ are\nsupported as aliases of ‘+00:00’. Other short names are not recommended to use\nbecause they can be ambiguous. Changed in version 2.4.0: tz can take a Column containing timezone ID strings."]], "Returns": [["Column", "timestamp value represented in UTC timezone."]], "Category": ["Functions"], "index": 305}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.trunc.html#pyspark.sql.functions.trunc"], "Title": ["trunc"], "Feature": ["trunc"], "Description": "Returns date truncated to the unit specified by the format.\nSee alsopyspark.sql.functions.date_trunc()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('1997-02-28',)], ['dt'])\n>>> df.select('*', sf.trunc(df.dt, 'year')).show()\n+----------+---------------+\n|        dt|trunc(dt, year)|\n+----------+---------------+\n|1997-02-28|     1997-01-01|\n+----------+---------------+", ">>> df.select('*', sf.trunc('dt', 'mon')).show()\n+----------+--------------+\n|        dt|trunc(dt, mon)|\n+----------+--------------+\n|1997-02-28|    1997-02-01|\n+----------+--------------+"], "Parameters": [["date Column or column name", "input column of values to truncate."], ["format literal string", "‘year’, ‘yyyy’, ‘yy’ to truncate by year,\nor ‘month’, ‘mon’, ‘mm’ to truncate by month\nOther options are: ‘week’, ‘quarter’"]], "Returns": [["Column", "truncated date."]], "Category": ["Functions"], "index": 306}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_make_interval.html#pyspark.sql.functions.try_make_interval"], "Title": ["try_make_interval"], "Feature": ["try_make_interval"], "Description": "This is a special version ofmake_intervalthat performs the same operation, but returns a\nNULL value instead of raising an error if interval cannot be created.\nSee alsopyspark.sql.functions.make_interval()pyspark.sql.functions.make_dt_interval()pyspark.sql.functions.make_ym_interval()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(\n...     sf.try_make_interval(df.year, df.month, 'week', df.day, 'hour', df.min, df.sec)\n... ).show(truncate=False)\n+---------------------------------------------------------------+\n|try_make_interval(year, month, week, day, hour, min, sec)      |\n+---------------------------------------------------------------+\n|100 years 11 months 8 days 12 hours 30 minutes 1.001001 seconds|\n+---------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(\n...     sf.try_make_interval(df.year, df.month, 'week', df.day, df.hour, df.min)\n... ).show(truncate=False)\n+-------------------------------------------------------+\n|try_make_interval(year, month, week, day, hour, min, 0)|\n+-------------------------------------------------------+\n|100 years 11 months 8 days 12 hours 30 minutes         |\n+-------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(\n...     sf.try_make_interval(df.year, df.month, 'week', df.day, df.hour)\n... ).show(truncate=False)\n+-----------------------------------------------------+\n|try_make_interval(year, month, week, day, hour, 0, 0)|\n+-----------------------------------------------------+\n|100 years 11 months 8 days 12 hours                  |\n+-----------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(sf.try_make_interval(df.year, 'month', df.week, df.day)).show(truncate=False)\n+--------------------------------------------------+\n|try_make_interval(year, month, week, day, 0, 0, 0)|\n+--------------------------------------------------+\n|100 years 11 months 8 days                        |\n+--------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(sf.try_make_interval(df.year, 'month', df.week)).show(truncate=False)\n+------------------------------------------------+\n|try_make_interval(year, month, week, 0, 0, 0, 0)|\n+------------------------------------------------+\n|100 years 11 months 7 days                      |\n+------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(sf.try_make_interval(df.year, 'month')).show(truncate=False)\n+---------------------------------------------+\n|try_make_interval(year, month, 0, 0, 0, 0, 0)|\n+---------------------------------------------+\n|100 years 11 months                          |\n+---------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n...     ['year', 'month', 'week', 'day', 'hour', 'min', 'sec'])\n>>> df.select(sf.try_make_interval(df.year)).show(truncate=False)\n+-----------------------------------------+\n|try_make_interval(year, 0, 0, 0, 0, 0, 0)|\n+-----------------------------------------+\n|100 years                                |\n+-----------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.try_make_interval()).show(truncate=False)\n+--------------------------------------+\n|try_make_interval(0, 0, 0, 0, 0, 0, 0)|\n+--------------------------------------+\n|0 seconds                             |\n+--------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.try_make_interval(sf.lit(2147483647))).show(truncate=False)\n+-----------------------------------------------+\n|try_make_interval(2147483647, 0, 0, 0, 0, 0, 0)|\n+-----------------------------------------------+\n|NULL                                           |\n+-----------------------------------------------+"], "Parameters": [["years Column or column name, optional", "The number of years, positive or negative."], ["months Column or column name, optional", "The number of months, positive or negative."], ["weeks Column or column name, optional", "The number of weeks, positive or negative."], ["days Column or column name, optional", "The number of days, positive or negative."], ["hours Column or column name, optional", "The number of hours, positive or negative."], ["mins Column or column name, optional", "The number of minutes, positive or negative."], ["secs Column or column name, optional", "The number of seconds with the fractional part in microsecond precision."]], "Returns": [["Column", "A new column that contains an interval."]], "Category": ["Functions"], "index": 307}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_make_timestamp.html#pyspark.sql.functions.try_make_timestamp"], "Title": ["try_make_timestamp"], "Feature": ["try_make_timestamp"], "Description": "Try to create timestamp from years, months, days, hours, mins, secs and timezone fields.\nThe result data type is consistent with the value of configurationspark.sql.timestampType.\nThe function returns NULL on invalid inputs.\nSee alsopyspark.sql.functions.make_timestamp()pyspark.sql.functions.make_timestamp_ltz()pyspark.sql.functions.make_timestamp_ntz()pyspark.sql.functions.try_make_timestamp_ltz()pyspark.sql.functions.try_make_timestamp_ntz()pyspark.sql.functions.make_interval()pyspark.sql.functions.try_make_interval()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887, 'CET']],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec', 'tz'])\n>>> df.select(\n...     sf.try_make_timestamp(df.year, df.month, df.day, 'hour', df.min, df.sec, 'tz')\n... ).show(truncate=False)\n+----------------------------------------------------+\n|try_make_timestamp(year, month, day, hour, min, sec)|\n+----------------------------------------------------+\n|2014-12-27 21:30:45.887                             |\n+----------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887, 'CET']],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec', 'tz'])\n>>> df.select(\n...     sf.try_make_timestamp(df.year, df.month, df.day, 'hour', df.min, df.sec)\n... ).show(truncate=False)\n+----------------------------------------------------+\n|try_make_timestamp(year, month, day, hour, min, sec)|\n+----------------------------------------------------+\n|2014-12-28 06:30:45.887                             |\n+----------------------------------------------------+\n>>> spark.conf.unset(\"spark.sql.session.timeZone\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 13, 28, 6, 30, 45.887, 'CET']],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec', 'tz'])\n>>> df.select(\n...     sf.try_make_timestamp(df.year, df.month, df.day, 'hour', df.min, df.sec)\n... ).show(truncate=False)\n+----------------------------------------------------+\n|try_make_timestamp(year, month, day, hour, min, sec)|\n+----------------------------------------------------+\n|NULL                                                |\n+----------------------------------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["years Column or column name", "The year to represent, from 1 to 9999"], ["months Column or column name", "The month-of-year to represent, from 1 (January) to 12 (December)"], ["days Column or column name", "The day-of-month to represent, from 1 to 31"], ["hours Column or column name", "The hour-of-day to represent, from 0 to 23"], ["mins Column or column name", "The minute-of-hour to represent, from 0 to 59"], ["secs Column or column name", "The second-of-minute and its micro-fraction to represent, from 0 to 60.\nThe value can be either an integer like 13 , or a fraction like 13.123.\nIf the sec argument equals to 60, the seconds field is set\nto 0 and 1 minute is added to the final timestamp."], ["timezone Column or column name, optional", "The time zone identifier. For example, CET, UTC and etc."]], "Returns": [["Column", "A new column that contains a timestamp or NULL in case of an error."]], "Category": ["Functions"], "index": 308}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_make_timestamp_ltz.html#pyspark.sql.functions.try_make_timestamp_ltz"], "Title": ["try_make_timestamp_ltz"], "Feature": ["try_make_timestamp_ltz"], "Description": "Try to create the current timestamp with local time zone from years, months, days, hours, mins,\nsecs and timezone fields.\nThe function returns NULL on invalid inputs.\nSee alsopyspark.sql.functions.make_timestamp()pyspark.sql.functions.make_timestamp_ltz()pyspark.sql.functions.make_timestamp_ntz()pyspark.sql.functions.try_make_timestamp()pyspark.sql.functions.try_make_timestamp_ntz()pyspark.sql.functions.make_interval()pyspark.sql.functions.try_make_interval()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887, 'CET']],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec', 'tz'])\n>>> df.select(\n...     sf.try_make_timestamp_ltz('year', 'month', df.day, df.hour, df.min, df.sec, 'tz')\n... ).show(truncate=False)\n+------------------------------------------------------------+\n|try_make_timestamp_ltz(year, month, day, hour, min, sec, tz)|\n+------------------------------------------------------------+\n|2014-12-27 21:30:45.887                                     |\n+------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887, 'CET']],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec', 'tz'])\n>>> df.select(\n...     sf.try_make_timestamp_ltz('year', 'month', df.day, df.hour, df.min, df.sec)\n... ).show(truncate=False)\n+--------------------------------------------------------+\n|try_make_timestamp_ltz(year, month, day, hour, min, sec)|\n+--------------------------------------------------------+\n|2014-12-28 06:30:45.887                                 |\n+--------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 13, 28, 6, 30, 45.887, 'CET']],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec', 'tz'])\n>>> df.select(\n...     sf.try_make_timestamp_ltz('year', 'month', df.day, df.hour, df.min, df.sec)\n... ).show(truncate=False)\n+--------------------------------------------------------+\n|try_make_timestamp_ltz(year, month, day, hour, min, sec)|\n+--------------------------------------------------------+\n|NULL                                                    |\n+--------------------------------------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["years Column or column name", "The year to represent, from 1 to 9999"], ["months Column or column name", "The month-of-year to represent, from 1 (January) to 12 (December)"], ["days Column or column name", "The day-of-month to represent, from 1 to 31"], ["hours Column or column name", "The hour-of-day to represent, from 0 to 23"], ["mins Column or column name", "The minute-of-hour to represent, from 0 to 59"], ["secs Column or column name", "The second-of-minute and its micro-fraction to represent, from 0 to 60.\nThe value can be either an integer like 13 , or a fraction like 13.123.\nIf the sec argument equals to 60, the seconds field is set\nto 0 and 1 minute is added to the final timestamp."], ["timezone Column or column name, optional", "The time zone identifier. For example, CET, UTC and etc."]], "Returns": [["Column", "A new column that contains a current timestamp, or NULL in case of an error."]], "Category": ["Functions"], "index": 309}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_make_timestamp_ntz.html#pyspark.sql.functions.try_make_timestamp_ntz"], "Title": ["try_make_timestamp_ntz"], "Feature": ["try_make_timestamp_ntz"], "Description": "Try to create local date-time from years, months, days, hours, mins, secs fields.\nThe function returns NULL on invalid inputs.\nSee alsopyspark.sql.functions.make_timestamp()pyspark.sql.functions.make_timestamp_ltz()pyspark.sql.functions.make_timestamp_ntz()pyspark.sql.functions.try_make_timestamp()pyspark.sql.functions.try_make_timestamp_ltz()pyspark.sql.functions.make_interval()pyspark.sql.functions.try_make_interval()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887]],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec'])\n>>> df.select(\n...     sf.try_make_timestamp_ntz('year', 'month', df.day, df.hour, df.min, df.sec)\n... ).show(truncate=False)\n+--------------------------------------------------------+\n|try_make_timestamp_ntz(year, month, day, hour, min, sec)|\n+--------------------------------------------------------+\n|2014-12-28 06:30:45.887                                 |\n+--------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[2014, 13, 28, 6, 30, 45.887]],\n...     ['year', 'month', 'day', 'hour', 'min', 'sec'])\n>>> df.select(\n...     sf.try_make_timestamp_ntz('year', 'month', df.day, df.hour, df.min, df.sec)\n... ).show(truncate=False)\n+--------------------------------------------------------+\n|try_make_timestamp_ntz(year, month, day, hour, min, sec)|\n+--------------------------------------------------------+\n|NULL                                                    |\n+--------------------------------------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["years Column or column name", "The year to represent, from 1 to 9999"], ["months Column or column name", "The month-of-year to represent, from 1 (January) to 12 (December)"], ["days Column or column name", "The day-of-month to represent, from 1 to 31"], ["hours Column or column name", "The hour-of-day to represent, from 0 to 23"], ["mins Column or column name", "The minute-of-hour to represent, from 0 to 59"], ["secs Column or column name", "The second-of-minute and its micro-fraction to represent, from 0 to 60.\nThe value can be either an integer like 13 , or a fraction like 13.123.\nIf the sec argument equals to 60, the seconds field is set\nto 0 and 1 minute is added to the final timestamp."]], "Returns": [["Column", "A new column that contains a local date-time, or NULL in case of an error."]], "Category": ["Functions"], "index": 310}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_to_timestamp.html#pyspark.sql.functions.try_to_timestamp"], "Title": ["try_to_timestamp"], "Feature": ["try_to_timestamp"], "Description": "Parses thecolwith theformatto a timestamp. The function always\nreturns null on an invalid input with/without ANSI SQL mode enabled. The result data type is\nconsistent with the value of configurationspark.sql.timestampType.\nSee alsopyspark.sql.functions.to_date()pyspark.sql.functions.to_timestamp()pyspark.sql.functions.to_utc_timestamp()pyspark.sql.functions.date_format()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n>>> df.select(sf.try_to_timestamp(df.t).alias('dt')).show()\n+-------------------+\n|                 dt|\n+-------------------+\n|1997-02-28 10:30:00|\n+-------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n>>> df.select(sf.try_to_timestamp(df.t, sf.lit('yyyy-MM-dd HH:mm:ss')).alias('dt')).show()\n+-------------------+\n|                 dt|\n+-------------------+\n|1997-02-28 10:30:00|\n+-------------------+", ">>> import pyspark.sql.functions as sf\n>>> origin = spark.conf.get(\"spark.sql.ansi.enabled\")\n>>> spark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\n>>> try:\n...     df = spark.createDataFrame([('malformed',)], ['t'])\n...     df.select(sf.try_to_timestamp(df.t)).show()\n... finally:\n...     spark.conf.set(\"spark.sql.ansi.enabled\", origin)\n+-------------------+\n|try_to_timestamp(t)|\n+-------------------+\n|               NULL|\n+-------------------+"], "Parameters": [["col Column or column name", "column values to convert."], ["format: literal string, optional", "format to use to convert timestamp values."]], "Returns": [], "Category": ["Functions"], "index": 311}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.freqItems.html#pyspark.sql.DataFrame.freqItems"], "Title": ["DataFrame.freqItems"], "Feature": ["DataFrame.freqItems"], "Description": "Finding frequent items for columns, possibly with false positives. Using the\nfrequent element count algorithm described in\n“https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou”.DataFrame.freqItems()andDataFrameStatFunctions.freqItems()are aliases.\nNotes\nThis function is meant for exploratory data analysis, as we make no\nguarantee about the backward compatibility of the schema of the resultingDataFrame.", "Examples": [">>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n>>> df.freqItems([\"c1\", \"c2\"]).show()  \n+------------+------------+\n|c1_freqItems|c2_freqItems|\n+------------+------------+\n|   [4, 1, 3]| [8, 11, 10]|\n+------------+------------+"], "Parameters": [["cols list or tuple", "Names of the columns to calculate frequent items for as a list or tuple of\nstrings."], ["support float, optional", "The frequency with which to consider an item ‘frequent’. Default is 1%.\nThe support must be greater than 1e-4."]], "Returns": [["DataFrame", "DataFrame with frequent items."]], "Category": ["DataFrame"], "index": 312}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.unix_date.html#pyspark.sql.functions.unix_date"], "Title": ["unix_date"], "Feature": ["unix_date"], "Description": "Returns the number of days since 1970-01-01.\nSee alsopyspark.sql.functions.date_from_unix_date()pyspark.sql.functions.unix_seconds()pyspark.sql.functions.unix_millis()pyspark.sql.functions.unix_micros()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('1970-01-02',), ('2022-01-02',)], ['dt'])\n>>> df.select('*', sf.unix_date(sf.to_date('dt'))).show()\n+----------+----------------------+\n|        dt|unix_date(to_date(dt))|\n+----------+----------------------+\n|1970-01-02|                     1|\n|2022-01-02|                 18994|\n+----------+----------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["col Column or column name", "input column of values to convert."]], "Returns": [["Column", "the number of days since 1970-01-01."]], "Category": ["Functions"], "index": 313}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.unix_micros.html#pyspark.sql.functions.unix_micros"], "Title": ["unix_micros"], "Feature": ["unix_micros"], "Description": "Returns the number of microseconds since 1970-01-01 00:00:00 UTC.\nSee alsopyspark.sql.functions.unix_date()pyspark.sql.functions.unix_seconds()pyspark.sql.functions.unix_millis()pyspark.sql.functions.timestamp_micros()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-07-22 10:00:00',), ('2022-10-09 11:12:13',)], ['ts'])\n>>> df.select('*', sf.unix_micros(sf.to_timestamp('ts'))).show()\n+-------------------+-----------------------------+\n|                 ts|unix_micros(to_timestamp(ts))|\n+-------------------+-----------------------------+\n|2015-07-22 10:00:00|             1437584400000000|\n|2022-10-09 11:12:13|             1665339133000000|\n+-------------------+-----------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["col Column or column name", "input column of values to convert."]], "Returns": [["Column", "the number of microseconds since 1970-01-01 00:00:00 UTC."]], "Category": ["Functions"], "index": 314}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.unix_millis.html#pyspark.sql.functions.unix_millis"], "Title": ["unix_millis"], "Feature": ["unix_millis"], "Description": "Returns the number of milliseconds since 1970-01-01 00:00:00 UTC.\nTruncates higher levels of precision.\nSee alsopyspark.sql.functions.unix_date()pyspark.sql.functions.unix_seconds()pyspark.sql.functions.unix_micros()pyspark.sql.functions.timestamp_millis()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-07-22 10:00:00',), ('2022-10-09 11:12:13',)], ['ts'])\n>>> df.select('*', sf.unix_millis(sf.to_timestamp('ts'))).show()\n+-------------------+-----------------------------+\n|                 ts|unix_millis(to_timestamp(ts))|\n+-------------------+-----------------------------+\n|2015-07-22 10:00:00|                1437584400000|\n|2022-10-09 11:12:13|                1665339133000|\n+-------------------+-----------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["col Column or column name", "input column of values to convert."]], "Returns": [["Column", "the number of milliseconds since 1970-01-01 00:00:00 UTC."]], "Category": ["Functions"], "index": 315}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.unix_seconds.html#pyspark.sql.functions.unix_seconds"], "Title": ["unix_seconds"], "Feature": ["unix_seconds"], "Description": "Returns the number of seconds since 1970-01-01 00:00:00 UTC.\nTruncates higher levels of precision.\nSee alsopyspark.sql.functions.unix_date()pyspark.sql.functions.unix_millis()pyspark.sql.functions.unix_micros()pyspark.sql.functions.from_unixtime()pyspark.sql.functions.timestamp_seconds()", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-07-22 10:00:00',), ('2022-10-09 11:12:13',)], ['ts'])\n>>> df.select('*', sf.unix_seconds(sf.to_timestamp('ts'))).show()\n+-------------------+------------------------------+\n|                 ts|unix_seconds(to_timestamp(ts))|\n+-------------------+------------------------------+\n|2015-07-22 10:00:00|                    1437584400|\n|2022-10-09 11:12:13|                    1665339133|\n+-------------------+------------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["col Column or column name", "input column of values to convert."]], "Returns": [["Column", "the number of seconds since 1970-01-01 00:00:00 UTC."]], "Category": ["Functions"], "index": 316}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.unix_timestamp.html#pyspark.sql.functions.unix_timestamp"], "Title": ["unix_timestamp"], "Feature": ["unix_timestamp"], "Description": "Convert time string with given pattern (‘yyyy-MM-dd HH:mm:ss’, by default)\nto Unix time stamp (in seconds), using the default timezone and the default\nlocale, returns null if failed.\niftimestampis None, then it returns current timestamp.", "Examples": [">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")", ">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.unix_timestamp()).show() \n+----------+\n| unix_time|\n+----------+\n|1702018137|\n+----------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 12:12:12',)], ['ts'])\n>>> df.select('*', sf.unix_timestamp('ts')).show()\n+-------------------+---------------------------------------+\n|                 ts|unix_timestamp(ts, yyyy-MM-dd HH:mm:ss)|\n+-------------------+---------------------------------------+\n|2015-04-08 12:12:12|                             1428520332|\n+-------------------+---------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n>>> df.select('*', sf.unix_timestamp('dt', 'yyyy-MM-dd')).show()\n+----------+------------------------------+\n|        dt|unix_timestamp(dt, yyyy-MM-dd)|\n+----------+------------------------------+\n|2015-04-08|                    1428476400|\n+----------+------------------------------+", ">>> spark.conf.unset(\"spark.sql.session.timeZone\")"], "Parameters": [["timestamp Column or column name, optional", "timestamps of string values."], ["format literal string, optional", "alternative format to use for converting (default: yyyy-MM-dd HH:mm:ss)."]], "Returns": [["Column", "unix time as long integer."]], "Category": ["Functions"], "index": 317}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.weekday.html#pyspark.sql.functions.weekday"], "Title": ["weekday"], "Feature": ["weekday"], "Description": "Returns the day of the week for date/timestamp (0 = Monday, 1 = Tuesday, …, 6 = Sunday).\nSee alsopyspark.sql.functions.day()pyspark.sql.functions.weekofyear()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.weekday('dt')).show()\n+----------+----------+-----------+\n|        dt|typeof(dt)|weekday(dt)|\n+----------+----------+-----------+\n|2015-04-08|    string|          2|\n|2024-10-31|    string|          3|\n+----------+----------+-----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.weekday('ts')).show()\n+-------------------+----------+-----------+\n|                 ts|typeof(ts)|weekday(ts)|\n+-------------------+----------+-----------+\n|2015-04-08 13:08:15|    string|          2|\n|2024-10-31 10:09:16|    string|          3|\n+-------------------+----------+-----------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.date(2015, 4, 8),),\n...     (datetime.date(2024, 10, 31),)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.weekday('dt')).show()\n+----------+----------+-----------+\n|        dt|typeof(dt)|weekday(dt)|\n+----------+----------+-----------+\n|2015-04-08|      date|          2|\n|2024-10-31|      date|          3|\n+----------+----------+-----------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.weekday('ts')).show()\n+-------------------+----------+-----------+\n|                 ts|typeof(ts)|weekday(ts)|\n+-------------------+----------+-----------+\n|2015-04-08 13:08:15| timestamp|          2|\n|2024-10-31 10:09:16| timestamp|          3|\n+-------------------+----------+-----------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "the day of the week for date/timestamp (0 = Monday, 1 = Tuesday, …, 6 = Sunday)."]], "Category": ["Functions"], "index": 318}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.weekofyear.html#pyspark.sql.functions.weekofyear"], "Title": ["weekofyear"], "Feature": ["weekofyear"], "Description": "Extract the week number of a given date as integer.\nA week is considered to start on a Monday and week 1 is the first week with more than 3 days,\nas defined by ISO 8601\nSee alsopyspark.sql.functions.weekday()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.weekofyear('dt')).show()\n+----------+----------+--------------+\n|        dt|typeof(dt)|weekofyear(dt)|\n+----------+----------+--------------+\n|2015-04-08|    string|            15|\n|2024-10-31|    string|            44|\n+----------+----------+--------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.weekofyear('ts')).show()\n+-------------------+----------+--------------+\n|                 ts|typeof(ts)|weekofyear(ts)|\n+-------------------+----------+--------------+\n|2015-04-08 13:08:15|    string|            15|\n|2024-10-31 10:09:16|    string|            44|\n+-------------------+----------+--------------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.date(2015, 4, 8),),\n...     (datetime.date(2024, 10, 31),)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.weekofyear('dt')).show()\n+----------+----------+--------------+\n|        dt|typeof(dt)|weekofyear(dt)|\n+----------+----------+--------------+\n|2015-04-08|      date|            15|\n|2024-10-31|      date|            44|\n+----------+----------+--------------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.weekofyear('ts')).show()\n+-------------------+----------+--------------+\n|                 ts|typeof(ts)|weekofyear(ts)|\n+-------------------+----------+--------------+\n|2015-04-08 13:08:15| timestamp|            15|\n|2024-10-31 10:09:16| timestamp|            44|\n+-------------------+----------+--------------+"], "Parameters": [["col Column or column name", "target timestamp column to work on."]], "Returns": [["Column", "week of the year for given date as integer."]], "Category": ["Functions"], "index": 319}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.window.html#pyspark.sql.functions.window"], "Title": ["window"], "Feature": ["window"], "Description": "Bucketize rows into one or more time windows given a timestamp specifying column. Window\nstarts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n[12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\nthe order of months are not supported.\nThe time column must be ofpyspark.sql.types.TimestampType.\nDurations are provided as strings, e.g. ‘1 second’, ‘1 day 12 hours’, ‘2 minutes’. Valid\ninterval strings are ‘week’, ‘day’, ‘hour’, ‘minute’, ‘second’, ‘millisecond’, ‘microsecond’.\nIf theslideDurationis not provided, the windows will be tumbling windows.\nThe startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\nwindow intervals. For example, in order to have hourly tumbling windows that start 15 minutes\npast the hour, e.g. 12:15-13:15, 13:15-14:15… providestartTimeas15 minutes.\nThe output column will be a struct called ‘window’ by default with the nested columns ‘start’\nand ‘end’, where ‘start’ and ‘end’ will be ofpyspark.sql.types.TimestampType.\nSee alsopyspark.sql.functions.window_time()pyspark.sql.functions.session_window()", "Examples": [">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(datetime.datetime(2016, 3, 11, 9, 0, 7), 1)], ['dt', 'v'])\n>>> df2 = df.groupBy(sf.window('dt', '5 seconds')).agg(sf.sum('v'))\n>>> df2.show(truncate=False)\n+------------------------------------------+------+\n|window                                    |sum(v)|\n+------------------------------------------+------+\n|{2016-03-11 09:00:05, 2016-03-11 09:00:10}|1     |\n+------------------------------------------+------+", ">>> df2.printSchema()\nroot\n |-- window: struct (nullable = false)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n |-- sum(v): long (nullable = true)"], "Parameters": [["timeColumn Column or column name", "The column or the expression to use as the timestamp for windowing by time.\nThe time column must be of TimestampType or TimestampNTZType."], ["windowDuration literal string", "A string specifying the width of the window, e.g. 10 minutes , 1 second . Check org.apache.spark.unsafe.types.CalendarInterval for\nvalid duration identifiers. Note that the duration is a fixed length of\ntime, and does not vary over time according to a calendar. For example, 1 day always means 86,400,000 milliseconds, not a calendar day."], ["slideDuration literal string, optional", "A new window will be generated every slideDuration . Must be less than\nor equal to the windowDuration . Check org.apache.spark.unsafe.types.CalendarInterval for valid duration\nidentifiers. This duration is likewise absolute, and does not vary\naccording to a calendar."], ["startTime literal string, optional", "The offset with respect to 1970-01-01 00:00:00 UTC with which to start\nwindow intervals. For example, in order to have hourly tumbling windows that\nstart 15 minutes past the hour, e.g. 12:15-13:15, 13:15-14:15… provide startTime as 15 minutes ."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 320}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.window_time.html#pyspark.sql.functions.window_time"], "Title": ["window_time"], "Feature": ["window_time"], "Description": "Computes the event time from a window column. The column window values are produced\nby window aggregating operators and are of typeSTRUCT<start: TIMESTAMP, end: TIMESTAMP>where start is inclusive and end is exclusive. The event time of records produced by window\naggregating operators can be computed aswindow_time(window)and arewindow.end-lit(1).alias(\"microsecond\")(as microsecond is the minimal supported event\ntime precision). The window column must be one produced by a window aggregating operator.\nSee alsopyspark.sql.functions.window()pyspark.sql.functions.session_window()", "Examples": [">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(datetime.datetime(2016, 3, 11, 9, 0, 7), 1)], ['dt', 'v'])", ">>> df2 = df.groupBy(sf.window('dt', '5 seconds')).agg(sf.sum('v'))", ">>> df2.select('*', sf.window_time('window')).show(truncate=False)\n+------------------------------------------+------+--------------------------+\n|window                                    |sum(v)|window_time(window)       |\n+------------------------------------------+------+--------------------------+\n|{2016-03-11 09:00:05, 2016-03-11 09:00:10}|1     |2016-03-11 09:00:09.999999|\n+------------------------------------------+------+--------------------------+"], "Parameters": [["windowColumn Column or column name", "The window column of a window aggregate records."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 321}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.year.html#pyspark.sql.functions.year"], "Title": ["year"], "Feature": ["year"], "Description": "Extract the year of a given date/timestamp as integer.\nSee alsopyspark.sql.functions.quarter()pyspark.sql.functions.month()pyspark.sql.functions.day()pyspark.sql.functions.hour()pyspark.sql.functions.minute()pyspark.sql.functions.second()pyspark.sql.functions.extract()pyspark.sql.functions.datepart()pyspark.sql.functions.date_part()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.year('dt')).show()\n+----------+----------+--------+\n|        dt|typeof(dt)|year(dt)|\n+----------+----------+--------+\n|2015-04-08|    string|    2015|\n|2024-10-31|    string|    2024|\n+----------+----------+--------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('2015-04-08 13:08:15',), ('2024-10-31 10:09:16',)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.year('ts')).show()\n+-------------------+----------+--------+\n|                 ts|typeof(ts)|year(ts)|\n+-------------------+----------+--------+\n|2015-04-08 13:08:15|    string|    2015|\n|2024-10-31 10:09:16|    string|    2024|\n+-------------------+----------+--------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.date(2015, 4, 8),),\n...     (datetime.date(2024, 10, 31),)], ['dt'])\n>>> df.select(\"*\", sf.typeof('dt'), sf.year('dt')).show()\n+----------+----------+--------+\n|        dt|typeof(dt)|year(dt)|\n+----------+----------+--------+\n|2015-04-08|      date|    2015|\n|2024-10-31|      date|    2024|\n+----------+----------+--------+", ">>> import datetime\n>>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (datetime.datetime(2015, 4, 8, 13, 8, 15),),\n...     (datetime.datetime(2024, 10, 31, 10, 9, 16),)], ['ts'])\n>>> df.select(\"*\", sf.typeof('ts'), sf.year('ts')).show()\n+-------------------+----------+--------+\n|                 ts|typeof(ts)|year(ts)|\n+-------------------+----------+--------+\n|2015-04-08 13:08:15| timestamp|    2015|\n|2024-10-31 10:09:16| timestamp|    2024|\n+-------------------+----------+--------+"], "Parameters": [["col Column or column name", "target date/timestamp column to work on."]], "Returns": [["Column", "year part of the date/timestamp as integer."]], "Category": ["Functions"], "index": 322}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupingSets.html#pyspark.sql.DataFrame.groupingSets"], "Title": ["DataFrame.groupingSets"], "Feature": ["DataFrame.groupingSets"], "Description": "Create multi-dimensional aggregation for the currentDataFrameusing the specified\ngrouping sets, so we can run aggregation on them.\nSee alsoDataFrame.rollupCompute hierarchical summaries at multiple levels.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (100, 'Fremont', 'Honda Civic', 10),\n...     (100, 'Fremont', 'Honda Accord', 15),\n...     (100, 'Fremont', 'Honda CRV', 7),\n...     (200, 'Dublin', 'Honda Civic', 20),\n...     (200, 'Dublin', 'Honda Accord', 10),\n...     (200, 'Dublin', 'Honda CRV', 3),\n...     (300, 'San Jose', 'Honda Civic', 5),\n...     (300, 'San Jose', 'Honda Accord', 8)\n... ], schema=\"id INT, city STRING, car_model STRING, quantity INT\")", ">>> df.groupingSets(\n...     [(\"city\", \"car_model\"), (\"city\",), ()],\n...     \"city\", \"car_model\"\n... ).agg(sf.sum(sf.col(\"quantity\")).alias(\"sum\")).sort(\"city\", \"car_model\").show()\n+--------+------------+---+\n|    city|   car_model|sum|\n+--------+------------+---+\n|    NULL|        NULL| 78|\n|  Dublin|        NULL| 33|\n|  Dublin|Honda Accord| 10|\n|  Dublin|   Honda CRV|  3|\n|  Dublin| Honda Civic| 20|\n| Fremont|        NULL| 32|\n| Fremont|Honda Accord| 15|\n| Fremont|   Honda CRV|  7|\n| Fremont| Honda Civic| 10|\n|San Jose|        NULL| 13|\n|San Jose|Honda Accord|  8|\n|San Jose| Honda Civic|  5|\n+--------+------------+---+", ">>> df.groupingSets(\n...     [(\"city\", \"car_model\"), (\"city\",), ()],\n...     \"city\", \"car_model\"\n... ).agg(\n...     sf.avg(sf.col(\"quantity\")).alias(\"avg_quantity\"),\n...     sf.sum(sf.col(\"quantity\")).alias(\"sum_quantity\")\n... ).sort(\"city\", \"car_model\").show()\n+--------+------------+------------------+------------+\n|    city|   car_model|      avg_quantity|sum_quantity|\n+--------+------------+------------------+------------+\n|    NULL|        NULL|              9.75|          78|\n|  Dublin|        NULL|              11.0|          33|\n|  Dublin|Honda Accord|              10.0|          10|\n|  Dublin|   Honda CRV|               3.0|           3|\n|  Dublin| Honda Civic|              20.0|          20|\n| Fremont|        NULL|10.666666666666666|          32|\n| Fremont|Honda Accord|              15.0|          15|\n| Fremont|   Honda CRV|               7.0|           7|\n| Fremont| Honda Civic|              10.0|          10|\n|San Jose|        NULL|               6.5|          13|\n|San Jose|Honda Accord|               8.0|           8|\n|San Jose| Honda Civic|               5.0|           5|\n+--------+------------+------------------+------------+"], "Parameters": [["groupingSets sequence of sequence of columns or str", "Individual set of columns to group on."], ["cols Column or str", "Additional grouping columns specified by users.\nThose columns are shown as the output columns after aggregation."]], "Returns": [["GroupedData", "Grouping sets of the data based on the specified columns."]], "Category": ["DataFrame"], "index": 323}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.crc32.html#pyspark.sql.functions.crc32"], "Title": ["crc32"], "Feature": ["crc32"], "Description": "Calculates the cyclic redundancy check value (CRC32) of a binary column and\nreturns the value as a bigint.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('ABC',)], ['a'])\n>>> df.select('*', sf.crc32('a')).show(truncate=False)\n+---+----------+\n|a  |crc32(a)  |\n+---+----------+\n|ABC|2743272264|\n+---+----------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 324}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.hash.html#pyspark.sql.functions.hash"], "Title": ["hash"], "Feature": ["hash"], "Description": "Calculates the hash code of given columns, and returns the result as an int column.\nSee alsopyspark.sql.functions.xxhash64()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('ABC', 'DEF')], ['c1', 'c2'])\n>>> df.select('*', sf.hash('c1')).show()\n+---+---+----------+\n| c1| c2|  hash(c1)|\n+---+---+----------+\n|ABC|DEF|-757602832|\n+---+---+----------+", ">>> df.select('*', sf.hash('c1', df.c2)).show()\n+---+---+------------+\n| c1| c2|hash(c1, c2)|\n+---+---+------------+\n|ABC|DEF|   599895104|\n+---+---+------------+", ">>> df.select('*', sf.hash('*')).show()\n+---+---+------------+\n| c1| c2|hash(c1, c2)|\n+---+---+------------+\n|ABC|DEF|   599895104|\n+---+---+------------+"], "Parameters": [["cols Column or column name", "one or more columns to compute on."]], "Returns": [["Column", "hash value as int column."]], "Category": ["Functions"], "index": 325}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.md5.html#pyspark.sql.functions.md5"], "Title": ["md5"], "Feature": ["md5"], "Description": "Calculates the MD5 digest and returns the value as a 32 character hex string.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('ABC',)], ['a'])\n>>> df.select('*', sf.md5('a')).show(truncate=False)\n+---+--------------------------------+\n|a  |md5(a)                          |\n+---+--------------------------------+\n|ABC|902fbdd2b1df0c4f70b4a5d23525e932|\n+---+--------------------------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 326}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sha.html#pyspark.sql.functions.sha"], "Title": ["sha"], "Feature": ["sha"], "Description": "Returns a sha1 hash value as a hex string of thecol.\nSee alsopyspark.sql.functions.sha1()pyspark.sql.functions.sha2()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.sha(sf.lit(\"Spark\"))).show()\n+--------------------+\n|          sha(Spark)|\n+--------------------+\n|85f5955f4b27a9a4c...|\n+--------------------+"], "Parameters": [["col Column or column name", ""]], "Returns": [], "Category": ["Functions"], "index": 327}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sha1.html#pyspark.sql.functions.sha1"], "Title": ["sha1"], "Feature": ["sha1"], "Description": "Returns the hex string result of SHA-1.\nSee alsopyspark.sql.functions.sha()pyspark.sql.functions.sha2()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('ABC',)], ['a'])\n>>> df.select('*', sf.sha1('a')).show(truncate=False)\n+---+----------------------------------------+\n|a  |sha1(a)                                 |\n+---+----------------------------------------+\n|ABC|3c01bdbb26f358bab27f267924aa2c9a03fcfdb8|\n+---+----------------------------------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 328}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sha2.html#pyspark.sql.functions.sha2"], "Title": ["sha2"], "Feature": ["sha2"], "Description": "Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,\nand SHA-512). The numBits indicates the desired bit length of the result, which must have a\nvalue of 224, 256, 384, 512, or 0 (which is equivalent to 256).\nSee alsopyspark.sql.functions.sha()pyspark.sql.functions.sha1()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([['Alice'], ['Bob']], ['name'])\n>>> df.select('*', sf.sha2('name', 256)).show(truncate=False)\n+-----+----------------------------------------------------------------+\n|name |sha2(name, 256)                                                 |\n+-----+----------------------------------------------------------------+\n|Alice|3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043|\n|Bob  |cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961|\n+-----+----------------------------------------------------------------+"], "Parameters": [["col Column or column name", "target column to compute on."], ["numBits int", "the desired bit length of the result, which must have a\nvalue of 224, 256, 384, 512, or 0 (which is equivalent to 256)."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 329}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.xxhash64.html#pyspark.sql.functions.xxhash64"], "Title": ["xxhash64"], "Feature": ["xxhash64"], "Description": "Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm,\nand returns the result as a long column. The hash computation uses an initial seed of 42.\nSee alsopyspark.sql.functions.hash()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('ABC', 'DEF')], ['c1', 'c2'])\n>>> df.select('*', sf.xxhash64('c1')).show()\n+---+---+-------------------+\n| c1| c2|       xxhash64(c1)|\n+---+---+-------------------+\n|ABC|DEF|4105715581806190027|\n+---+---+-------------------+", ">>> df.select('*', sf.xxhash64('c1', df.c2)).show()\n+---+---+-------------------+\n| c1| c2|   xxhash64(c1, c2)|\n+---+---+-------------------+\n|ABC|DEF|3233247871021311208|\n+---+---+-------------------+", ">>> df.select('*', sf.xxhash64('*')).show()\n+---+---+-------------------+\n| c1| c2|   xxhash64(c1, c2)|\n+---+---+-------------------+\n|ABC|DEF|3233247871021311208|\n+---+---+-------------------+"], "Parameters": [["cols Column or column name", "one or more columns to compute on."]], "Returns": [["Column", "hash value as long column."]], "Category": ["Functions"], "index": 330}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.aggregate.html#pyspark.sql.functions.aggregate"], "Title": ["aggregate"], "Feature": ["aggregate"], "Description": "Applies a binary operator to an initial state and all elements in the array,\nand reduces this to a single state. The final state is converted into the final result\nby applying a finish function.\nBoth functions can use methods ofColumn, functions defined inpyspark.sql.functionsand ScalaUserDefinedFunctions.\nPythonUserDefinedFunctionsare not supported\n(SPARK-27052).", "Examples": [">>> df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\n>>> df.select(aggregate(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()\n+----+\n| sum|\n+----+\n|42.0|\n+----+", ">>> def merge(acc, x):\n...     count = acc.count + 1\n...     sum = acc.sum + x\n...     return struct(count.alias(\"count\"), sum.alias(\"sum\"))\n...\n>>> df.select(\n...     aggregate(\n...         \"values\",\n...         struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\n...         merge,\n...         lambda acc: acc.sum / acc.count,\n...     ).alias(\"mean\")\n... ).show()\n+----+\n|mean|\n+----+\n| 8.4|\n+----+"], "Parameters": [["col Column or str", "name of column or expression"], ["initialValue Column or str", "initial value. Name of column or expression"], ["merge function", "a binary function (acc: Column, x: Column) -> Column... returning expression\nof the same type as initialValue"], ["finish function, optional", "an optional unary function (x: Column) -> Column: ... used to convert accumulated value."]], "Returns": [["Column", "final value after aggregate function is applied."]], "Category": ["Functions"], "index": 331}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_sort.html#pyspark.sql.functions.array_sort"], "Title": ["array_sort"], "Feature": ["array_sort"], "Description": "Collection function: sorts the input array in ascending order. The elements of the input array\nmust be orderable. Null elements will be placed at the end of the returned array.", "Examples": [">>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n>>> df.select(array_sort(df.data).alias('r')).collect()\n[Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]\n>>> df = spark.createDataFrame([([\"foo\", \"foobar\", None, \"bar\"],),([\"foo\"],),([],)], ['data'])\n>>> df.select(array_sort(\n...     \"data\",\n...     lambda x, y: when(x.isNull() | y.isNull(), lit(0)).otherwise(length(y) - length(x))\n... ).alias(\"r\")).collect()\n[Row(r=['foobar', 'foo', None, 'bar']), Row(r=['foo']), Row(r=[])]"], "Parameters": [["col Column or str", "name of column or expression"], ["comparator callable, optional", "A binary (Column, Column) -> Column: ... .\nThe comparator will take two\narguments representing two elements of the array. It returns a negative integer, 0, or a\npositive integer as the first element is less than, equal to, or greater than the second\nelement. If the comparator function returns null, the function will fail and raise an error."]], "Returns": [["Column", "sorted array."]], "Category": ["Functions"], "index": 332}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.cardinality.html#pyspark.sql.functions.cardinality"], "Title": ["cardinality"], "Feature": ["cardinality"], "Description": "Collection function: returns the length of the array or map stored in the column.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [([1, 2, 3],),([1],),([],)], ['data']\n... ).select(sf.cardinality(\"data\")).show()\n+-----------------+\n|cardinality(data)|\n+-----------------+\n|                3|\n|                1|\n|                0|\n+-----------------+"], "Parameters": [["col Column or str", "target column to compute on."]], "Returns": [["Column", "length of the array/map."]], "Category": ["Functions"], "index": 333}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.approxQuantile.html#pyspark.sql.DataFrame.approxQuantile"], "Title": ["DataFrame.approxQuantile"], "Feature": ["DataFrame.approxQuantile"], "Description": "Calculates the approximate quantiles of numerical columns of aDataFrame.\nThe result of this algorithm has the following deterministic bound:\nIf theDataFramehas N elements and if we request the quantile at\nprobabilitypup to errorerr, then the algorithm will return\na samplexfrom theDataFrameso that theexactrank ofxis\nclose to (p * N). More precisely,\nThis method implements a variation of the Greenwald-Khanna\nalgorithm (with some speed optimizations). The algorithm was first\npresent in [[https://doi.org/10.1145/375663.375670Space-efficient Online Computation of Quantile Summaries]]\nby Greenwald and Khanna.\nNotes\nNull values will be ignored in numerical columns before calculation.\nFor columns only containing null values, an empty list is returned.", "Examples": [">>> data = [(1,), (2,), (3,), (4,), (5,)]\n>>> df = spark.createDataFrame(data, [\"values\"])\n>>> quantiles = df.approxQuantile(\"values\", [0.0, 0.5, 1.0], 0.05)\n>>> quantiles\n[1.0, 3.0, 5.0]", ">>> data = [(1, 10), (2, 20), (3, 30), (4, 40), (5, 50)]\n>>> df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n>>> quantiles = df.approxQuantile([\"col1\", \"col2\"], [0.0, 0.5, 1.0], 0.05)\n>>> quantiles\n[[1.0, 3.0, 5.0], [10.0, 30.0, 50.0]]", ">>> data = [(1,), (None,), (3,), (4,), (None,)]\n>>> df = spark.createDataFrame(data, [\"values\"])\n>>> quantiles = df.approxQuantile(\"values\", [0.0, 0.5, 1.0], 0.05)\n>>> quantiles\n[1.0, 3.0, 4.0]", ">>> data = [(1,), (2,), (3,), (4,), (5,)]\n>>> df = spark.createDataFrame(data, [\"values\"])\n>>> quantiles = df.approxQuantile(\"values\", [0.0, 0.2, 1.0], 0.1)\n>>> quantiles\n[1.0, 1.0, 5.0]"], "Parameters": [["col: str, tuple or list", "Can be a single column name, or a list of names for multiple columns. Changed in version 2.2.0: Added support for multiple columns."], ["probabilities list or tuple of floats", "a list of quantile probabilities\nEach number must be a float in the range [0, 1].\nFor example 0.0 is the minimum, 0.5 is the median, 1.0 is the maximum."], ["relativeError float", "The relative target precision to achieve\n(>= 0). If set to zero, the exact quantiles are computed, which\ncould be very expensive. Note that values greater than 1 are\naccepted but gives the same result as 1."]], "Returns": [["list", "the approximate quantiles at the given probabilities. If the input col is a string, the output is a list of floats. If the input col is a list or tuple of strings, the output is also a list, but each element in it is a list of floats, i.e., the output\nis a list of list of floats."], ["If the input col is a list or tuple of strings, the output is also a", "list, but each element in it is a list of floats, i.e., the output\nis a list of list of floats."]], "Category": ["DataFrame"], "index": 334}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.head.html#pyspark.sql.DataFrame.head"], "Title": ["DataFrame.head"], "Feature": ["DataFrame.head"], "Description": "Returns the firstnrows.\nNotes\nThis method should only be used if the resulting array is expected\nto be small, as all the data is loaded into the driver’s memory.", "Examples": [">>> df = spark.createDataFrame([\n...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.head()\nRow(age=2, name='Alice')\n>>> df.head(1)\n[Row(age=2, name='Alice')]\n>>> df.head(0)\n[]"], "Parameters": [["n int, optional", "default 1. Number of rows to return."]], "Returns": [["If n is supplied, return a list of Row of length n", ""], ["or less if the DataFrame has fewer elements.", ""], ["If n is missing, return a single Row.", ""]], "Category": ["DataFrame"], "index": 335}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.concat.html#pyspark.sql.functions.concat"], "Title": ["concat"], "Feature": ["concat"], "Description": "Collection function: Concatenates multiple input columns together into a single column.\nThe function works with strings, numeric, binary and compatible array columns.\nSee alsopyspark.sql.functions.concat_ws()pyspark.sql.functions.array_join()to concatenate string columns with delimiter", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n>>> df.select(sf.concat(df.s, df.d)).show()\n+------------+\n|concat(s, d)|\n+------------+\n|     abcd123|\n+------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n>>> df.select(sf.concat(df.a, df.b, df.c)).show()\n+---------------+\n|concat(a, b, c)|\n+---------------+\n|[1, 2, 3, 4, 5]|\n|           NULL|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, 2, 3)], ['a', 'b', 'c'])\n>>> df.select(sf.concat(df.a, df.b, df.c)).show()\n+---------------+\n|concat(a, b, c)|\n+---------------+\n|            123|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(bytearray(b'abc'), bytearray(b'def'))], ['a', 'b'])\n>>> df.select(sf.concat(df.a, df.b)).show()\n+-------------------+\n|       concat(a, b)|\n+-------------------+\n|[61 62 63 64 65 66]|\n+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1,\"abc\",3,\"def\")], ['a','b','c','d'])\n>>> df.select(sf.concat(df.a, df.b, df.c, df.d)).show()\n+------------------+\n|concat(a, b, c, d)|\n+------------------+\n|          1abc3def|\n+------------------+"], "Parameters": [["cols Column or str", "target column or columns to work on."]], "Returns": [["Column", "concatenated values. Type of the Column depends on input columns’ type."]], "Category": ["Functions"], "index": 336}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.element_at.html#pyspark.sql.functions.element_at"], "Title": ["element_at"], "Feature": ["element_at"], "Description": "Collection function:\n(array, index) - Returns element of array at given (1-based) index. If Index is 0, Spark will\nthrow an error. If index < 0, accesses elements from the last to the first.\nIf ‘spark.sql.ansi.enabled’ is set to true, an exception will be thrown if the index is out\nof array boundaries instead of returning NULL.\n(map, key) - Returns value for given key inextractionif col is map. The function always\nreturns NULL if the key is not contained in the map.\nSee alsopyspark.sql.functions.get()pyspark.sql.functions.try_element_at()\nNotes\nThe position is not zero based, but 1 based index.\nIf extraction is a string,element_at()treats it as a literal string,\nwhiletry_element_at()treats it as a column name.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n>>> df.select(sf.element_at(df.data, 1)).show()\n+-------------------+\n|element_at(data, 1)|\n+-------------------+\n|                  a|\n+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n>>> df.select(sf.element_at(df.data, -1)).show()\n+--------------------+\n|element_at(data, -1)|\n+--------------------+\n|                   c|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},)], ['data'])\n>>> df.select(sf.element_at(df.data, sf.lit(\"a\"))).show()\n+-------------------+\n|element_at(data, a)|\n+-------------------+\n|                1.0|\n+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},)], ['data'])\n>>> df.select(sf.element_at(df.data, sf.lit(\"c\"))).show()\n+-------------------+\n|element_at(data, c)|\n+-------------------+\n|               NULL|\n+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0}, \"a\")], ['data', 'b'])\n>>> df.select(sf.element_at(df.data, 'b')).show()\n+-------------------+\n|element_at(data, b)|\n+-------------------+\n|                2.0|\n+-------------------+"], "Parameters": [["col Column or str", "name of column containing array or map"], ["extraction", "index to check for in array or key to check for in map"]], "Returns": [["Column", "value at given position."]], "Category": ["Functions"], "index": 337}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.exists.html#pyspark.sql.functions.exists"], "Title": ["exists"], "Feature": ["exists"], "Description": "Returns whether a predicate holds for one or more elements in the array.", "Examples": [">>> df = spark.createDataFrame([(1, [1, 2, 3, 4]), (2, [3, -1, 0])],(\"key\", \"values\"))\n>>> df.select(exists(\"values\", lambda x: x < 0).alias(\"any_negative\")).show()\n+------------+\n|any_negative|\n+------------+\n|       false|\n|        true|\n+------------+"], "Parameters": [["col Column or str", "name of column or expression"], ["f function", "(x: Column) -> Column: ... returning the Boolean expression.\nCan use methods of Column , functions defined in pyspark.sql.functions and Scala UserDefinedFunctions .\nPython UserDefinedFunctions are not supported\n( SPARK-27052 )."]], "Returns": [["Column", "True if “any” element of an array evaluates to True when passed as an argument to\ngiven function and False otherwise."]], "Category": ["Functions"], "index": 338}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.filter.html#pyspark.sql.functions.filter"], "Title": ["filter"], "Feature": ["filter"], "Description": "Returns an array of elements for which a predicate holds in a given array.", "Examples": [">>> df = spark.createDataFrame(\n...     [(1, [\"2018-09-20\",  \"2019-02-03\", \"2019-07-01\", \"2020-06-01\"])],\n...     (\"key\", \"values\")\n... )\n>>> def after_second_quarter(x):\n...     return month(to_date(x)) > 6\n...\n>>> df.select(\n...     filter(\"values\", after_second_quarter).alias(\"after_second_quarter\")\n... ).show(truncate=False)\n+------------------------+\n|after_second_quarter    |\n+------------------------+\n|[2018-09-20, 2019-07-01]|\n+------------------------+"], "Parameters": [["col Column or str", "name of column or expression"], ["f function", "A function that returns the Boolean expression.\nCan take one of the following forms: Unary (x: Column) -> Column: ... Binary (x: Column, i: Column) -> Column... , where the second argument is a 0-based index of the element. and can use methods of Column , functions defined in pyspark.sql.functions and Scala UserDefinedFunctions .\nPython UserDefinedFunctions are not supported\n( SPARK-27052 )."], ["Binary (x: Column, i: Column) -> Column... , where the second argument is", "a 0-based index of the element."]], "Returns": [["Column", "filtered array of elements where given function evaluated to True\nwhen passed as an argument."]], "Category": ["Functions"], "index": 339}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.forall.html#pyspark.sql.functions.forall"], "Title": ["forall"], "Feature": ["forall"], "Description": "Returns whether a predicate holds for every element in the array.", "Examples": [">>> df = spark.createDataFrame(\n...     [(1, [\"bar\"]), (2, [\"foo\", \"bar\"]), (3, [\"foobar\", \"foo\"])],\n...     (\"key\", \"values\")\n... )\n>>> df.select(forall(\"values\", lambda x: x.rlike(\"foo\")).alias(\"all_foo\")).show()\n+-------+\n|all_foo|\n+-------+\n|  false|\n|  false|\n|   true|\n+-------+"], "Parameters": [["col Column or str", "name of column or expression"], ["f function", "(x: Column) -> Column: ... returning the Boolean expression.\nCan use methods of Column , functions defined in pyspark.sql.functions and Scala UserDefinedFunctions .\nPython UserDefinedFunctions are not supported\n( SPARK-27052 )."]], "Returns": [["Column", "True if “all” elements of an array evaluates to True when passed as an argument to\ngiven function and False otherwise."]], "Category": ["Functions"], "index": 340}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.map_filter.html#pyspark.sql.functions.map_filter"], "Title": ["map_filter"], "Feature": ["map_filter"], "Description": "Collection function: Returns a new map column whose key-value pairs satisfy a given\npredicate function.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\"))\n>>> row = df.select(\n...   sf.map_filter(\"data\", lambda _, v: v > 30.0).alias(\"data_filtered\")\n... ).head()\n>>> sorted(row[\"data_filtered\"].items())\n[('baz', 32.0), ('foo', 42.0)]", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\"))\n>>> row = df.select(\n...   sf.map_filter(\"data\", lambda k, _: k.startswith(\"b\")).alias(\"data_filtered\")\n... ).head()\n>>> sorted(row[\"data_filtered\"].items())\n[('bar', 1.0), ('baz', 32.0)]", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\"))\n>>> row = df.select(\n...   sf.map_filter(\"data\", lambda k, v: k.startswith(\"b\") & (v > 1.0)).alias(\"data_filtered\")\n... ).head()\n>>> sorted(row[\"data_filtered\"].items())\n[('baz', 32.0)]"], "Parameters": [["col Column or str", "The name of the column or a column expression representing the map to be filtered."], ["f function", "A binary function (k: Column, v: Column) -> Column... that defines the predicate.\nThis function should return a boolean column that will be used to filter the input map.\nCan use methods of Column , functions defined in pyspark.sql.functions and Scala UserDefinedFunctions .\nPython UserDefinedFunctions are not supported\n( SPARK-27052 )."]], "Returns": [["Column", "A new map column containing only the key-value pairs that satisfy the predicate."]], "Category": ["Functions"], "index": 341}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.map_zip_with.html#pyspark.sql.functions.map_zip_with"], "Title": ["map_zip_with"], "Feature": ["map_zip_with"], "Description": "Collection: Merges two given maps into a single map by applying a function to\nthe key-value pairs.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...   (1, {\"A\": 1, \"B\": 2}, {\"A\": 3, \"B\": 4})],\n...   (\"id\", \"map1\", \"map2\"))\n>>> row = df.select(\n...   sf.map_zip_with(\"map1\", \"map2\", lambda _, v1, v2: v1 + v2).alias(\"updated_data\")\n... ).head()\n>>> sorted(row[\"updated_data\"].items())\n[('A', 4), ('B', 6)]", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...   (1, {\"A\": 1, \"B\": 2}, {\"A\": 3, \"B\": 4})],\n...   (\"id\", \"map1\", \"map2\"))\n>>> row = df.select(\n...   sf.map_zip_with(\"map1\", \"map2\",\n...     lambda k, v1, v2: sf.when(k == \"A\", v1 + v2).otherwise(v1 - v2)\n...   ).alias(\"updated_data\")\n... ).head()\n>>> sorted(row[\"updated_data\"].items())\n[('A', 4), ('B', -2)]", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...   (1, {\"A\": 1, \"B\": 2}, {\"B\": 3, \"C\": 4})],\n...   (\"id\", \"map1\", \"map2\"))\n>>> row = df.select(\n...   sf.map_zip_with(\"map1\", \"map2\",\n...     lambda _, v1, v2: sf.when(v2.isNull(), v1).otherwise(v1 + v2)\n...   ).alias(\"updated_data\")\n... ).head()\n>>> sorted(row[\"updated_data\"].items())\n[('A', 1), ('B', 5), ('C', None)]"], "Parameters": [["col1 Column or str", "The name of the first column or a column expression representing the first map."], ["col2 Column or str", "The name of the second column or a column expression representing the second map."], ["f function", "A ternary function (k: Column, v1: Column, v2: Column) -> Column... that defines\nhow to merge the values from the two maps. This function should return a column that\nwill be used as the value in the resulting map.\nCan use methods of Column , functions defined in pyspark.sql.functions and Scala UserDefinedFunctions .\nPython UserDefinedFunctions are not supported\n( SPARK-27052 )."]], "Returns": [["Column", "A new map column where each key-value pair is the result of applying the function to\nthe corresponding key-value pairs in the input maps."]], "Category": ["Functions"], "index": 342}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.reduce.html#pyspark.sql.functions.reduce"], "Title": ["reduce"], "Feature": ["reduce"], "Description": "Applies a binary operator to an initial state and all elements in the array,\nand reduces this to a single state. The final state is converted into the final result\nby applying a finish function.\nBoth functions can use methods ofColumn, functions defined inpyspark.sql.functionsand ScalaUserDefinedFunctions.\nPythonUserDefinedFunctionsare not supported\n(SPARK-27052).", "Examples": [">>> df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\n>>> df.select(reduce(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()\n+----+\n| sum|\n+----+\n|42.0|\n+----+", ">>> def merge(acc, x):\n...     count = acc.count + 1\n...     sum = acc.sum + x\n...     return struct(count.alias(\"count\"), sum.alias(\"sum\"))\n...\n>>> df.select(\n...     reduce(\n...         \"values\",\n...         struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\n...         merge,\n...         lambda acc: acc.sum / acc.count,\n...     ).alias(\"mean\")\n... ).show()\n+----+\n|mean|\n+----+\n| 8.4|\n+----+"], "Parameters": [["col Column or str", "name of column or expression"], ["initialValue Column or str", "initial value. Name of column or expression"], ["merge function", "a binary function (acc: Column, x: Column) -> Column... returning expression\nof the same type as zero"], ["finish function, optional", "an optional unary function (x: Column) -> Column: ... used to convert accumulated value."]], "Returns": [["Column", "final value after aggregate function is applied."]], "Category": ["Functions"], "index": 343}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.reverse.html#pyspark.sql.functions.reverse"], "Title": ["reverse"], "Feature": ["reverse"], "Description": "Collection function: returns a reversed string or an array with elements in reverse order.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('Spark SQL',)], ['data'])\n>>> df.select(sf.reverse(df.data)).show()\n+-------------+\n|reverse(data)|\n+-------------+\n|    LQS krapS|\n+-------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([2, 1, 3],) ,([1],) ,([],)], ['data'])\n>>> df.select(sf.reverse(df.data)).show()\n+-------------+\n|reverse(data)|\n+-------------+\n|    [3, 1, 2]|\n|          [1]|\n|           []|\n+-------------+"], "Parameters": [["col Column or str", "The name of the column or an expression that represents the element to be reversed."]], "Returns": [["Column", "A new column that contains a reversed string or an array with elements in reverse order."]], "Category": ["Functions"], "index": 344}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.size.html#pyspark.sql.functions.size"], "Title": ["size"], "Feature": ["size"], "Description": "Collection function: returns the length of the array or map stored in the column.", "Examples": [">>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n>>> df.select(size(df.data)).collect()\n[Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]"], "Parameters": [["col Column or str", "name of column or expression"]], "Returns": [["Column", "length of the array/map."]], "Category": ["Functions"], "index": 345}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.hint.html#pyspark.sql.DataFrame.hint"], "Title": ["DataFrame.hint"], "Feature": ["DataFrame.hint"], "Description": "Specifies some hint on the currentDataFrame..", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n>>> df.join(df2, \"name\").explain()  \n== Physical Plan ==\n...\n... +- SortMergeJoin ...\n...", ">>> df.join(df2.hint(\"broadcast\"), \"name\").explain()\n== Physical Plan ==\n...\n... +- BroadcastHashJoin ...\n..."], "Parameters": [["name str", "A name of the hint."], ["parameters str, list, float or int", "Optional parameters."]], "Returns": [["DataFrame", "Hinted DataFrame"]], "Category": ["DataFrame"], "index": 346}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.transform.html#pyspark.sql.functions.transform"], "Title": ["transform"], "Feature": ["transform"], "Description": "Returns an array of elements after applying a transformation to each element in the input array.", "Examples": [">>> df = spark.createDataFrame([(1, [1, 2, 3, 4])], (\"key\", \"values\"))\n>>> df.select(transform(\"values\", lambda x: x * 2).alias(\"doubled\")).show()\n+------------+\n|     doubled|\n+------------+\n|[2, 4, 6, 8]|\n+------------+", ">>> def alternate(x, i):\n...     return when(i % 2 == 0, x).otherwise(-x)\n...\n>>> df.select(transform(\"values\", alternate).alias(\"alternated\")).show()\n+--------------+\n|    alternated|\n+--------------+\n|[1, -2, 3, -4]|\n+--------------+"], "Parameters": [["col Column or str", "name of column or expression"], ["f function", "a function that is applied to each element of the input array.\nCan take one of the following forms: Unary (x: Column) -> Column: ... Binary (x: Column, i: Column) -> Column... , where the second argument is a 0-based index of the element. and can use methods of Column , functions defined in pyspark.sql.functions and Scala UserDefinedFunctions .\nPython UserDefinedFunctions are not supported\n( SPARK-27052 )."], ["Binary (x: Column, i: Column) -> Column... , where the second argument is", "a 0-based index of the element."]], "Returns": [["Column", "a new array of transformed elements."]], "Category": ["Functions"], "index": 347}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.transform_keys.html#pyspark.sql.functions.transform_keys"], "Title": ["transform_keys"], "Feature": ["transform_keys"], "Description": "Applies a function to every key-value pair in a map and returns\na map with the results of those applications as the new keys for the pairs.", "Examples": [">>> df = spark.createDataFrame([(1, {\"foo\": -2.0, \"bar\": 2.0})], (\"id\", \"data\"))\n>>> row = df.select(transform_keys(\n...     \"data\", lambda k, _: upper(k)).alias(\"data_upper\")\n... ).head()\n>>> sorted(row[\"data_upper\"].items())\n[('BAR', 2.0), ('FOO', -2.0)]"], "Parameters": [["col Column or str", "name of column or expression"], ["f function", "a binary function (k: Column, v: Column) -> Column... Can use methods of Column , functions defined in pyspark.sql.functions and Scala UserDefinedFunctions .\nPython UserDefinedFunctions are not supported\n( SPARK-27052 )."]], "Returns": [["Column", "a new map of entries where new keys were calculated by applying given function to\neach key value argument."]], "Category": ["Functions"], "index": 348}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.transform_values.html#pyspark.sql.functions.transform_values"], "Title": ["transform_values"], "Feature": ["transform_values"], "Description": "Applies a function to every key-value pair in a map and returns\na map with the results of those applications as the new values for the pairs.", "Examples": [">>> df = spark.createDataFrame([(1, {\"IT\": 10.0, \"SALES\": 2.0, \"OPS\": 24.0})], (\"id\", \"data\"))\n>>> row = df.select(transform_values(\n...     \"data\", lambda k, v: when(k.isin(\"IT\", \"OPS\"), v + 10.0).otherwise(v)\n... ).alias(\"new_data\")).head()\n>>> sorted(row[\"new_data\"].items())\n[('IT', 20.0), ('OPS', 34.0), ('SALES', 2.0)]"], "Parameters": [["col Column or str", "name of column or expression"], ["f function", "a binary function (k: Column, v: Column) -> Column... Can use methods of Column , functions defined in pyspark.sql.functions and Scala UserDefinedFunctions .\nPython UserDefinedFunctions are not supported\n( SPARK-27052 )."]], "Returns": [["Column", "a new map of entries where new values were calculated by applying given function to\neach key value argument."]], "Category": ["Functions"], "index": 349}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_element_at.html#pyspark.sql.functions.try_element_at"], "Title": ["try_element_at"], "Feature": ["try_element_at"], "Description": "Collection function:\n(array, index) - Returns element of array at given (1-based) index. If Index is 0, Spark will\nthrow an error. If index < 0, accesses elements from the last to the first. The function\nalways returns NULL if the index exceeds the length of the array.\n(map, key) - Returns value for given key. The function always returns NULL if the key is not\ncontained in the map.\nSee alsopyspark.sql.functions.get()pyspark.sql.functions.element_at()\nNotes\nThe position is not zero based, but 1 based index.\nIf extraction is a string,try_element_at()treats it as a column name,\nwhileelement_at()treats it as a literal string.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n>>> df.select(sf.try_element_at(df.data, sf.lit(1))).show()\n+-----------------------+\n|try_element_at(data, 1)|\n+-----------------------+\n|                      a|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n>>> df.select(sf.try_element_at(df.data, sf.lit(-1))).show()\n+------------------------+\n|try_element_at(data, -1)|\n+------------------------+\n|                       c|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},)], ['data'])\n>>> df.select(sf.try_element_at(df.data, sf.lit(\"a\"))).show()\n+-----------------------+\n|try_element_at(data, a)|\n+-----------------------+\n|                    1.0|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n>>> df.select(sf.try_element_at(df.data, sf.lit(4))).show()\n+-----------------------+\n|try_element_at(data, 4)|\n+-----------------------+\n|                   NULL|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},)], ['data'])\n>>> df.select(sf.try_element_at(df.data, sf.lit(\"c\"))).show()\n+-----------------------+\n|try_element_at(data, c)|\n+-----------------------+\n|                   NULL|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0}, \"a\")], ['data', 'b'])\n>>> df.select(sf.try_element_at(df.data, 'b')).show()\n+-----------------------+\n|try_element_at(data, b)|\n+-----------------------+\n|                    1.0|\n+-----------------------+"], "Parameters": [["col Column or str", "name of column containing array or map"], ["extraction", "index to check for in array or key to check for in map"]], "Returns": [], "Category": ["Functions"], "index": 350}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.zip_with.html#pyspark.sql.functions.zip_with"], "Title": ["zip_with"], "Feature": ["zip_with"], "Description": "Merge two given arrays, element-wise, into a single array using a function.\nIf one array is shorter, nulls are appended at the end to match the length of the longer\narray, before applying the function.", "Examples": [">>> df = spark.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (\"id\", \"xs\", \"ys\"))\n>>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: x ** y).alias(\"powers\")).show(truncate=False)\n+---------------------------+\n|powers                     |\n+---------------------------+\n|[1.0, 9.0, 625.0, 262144.0]|\n+---------------------------+", ">>> df = spark.createDataFrame([(1, [\"foo\", \"bar\"], [1, 2, 3])], (\"id\", \"xs\", \"ys\"))\n>>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: concat_ws(\"_\", x, y)).alias(\"xs_ys\")).show()\n+-----------------+\n|            xs_ys|\n+-----------------+\n|[foo_1, bar_2, 3]|\n+-----------------+"], "Parameters": [["left Column or str", "name of the first column or expression"], ["right Column or str", "name of the second column or expression"], ["f function", "a binary function (x1: Column, x2: Column) -> Column... Can use methods of Column , functions defined in pyspark.sql.functions and Scala UserDefinedFunctions .\nPython UserDefinedFunctions are not supported\n( SPARK-27052 )."]], "Returns": [["Column", "array of calculated values derived by applying given function to each pair of arguments."]], "Category": ["Functions"], "index": 351}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array.html#pyspark.sql.functions.array"], "Title": ["array"], "Feature": ["array"], "Description": "Collection function: Creates a new array column from the input columns or column names.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", \"doctor\"), (\"Bob\", \"engineer\")],\n...     (\"name\", \"occupation\"))\n>>> df.select(sf.array('name', 'occupation')).show()\n+-----------------------+\n|array(name, occupation)|\n+-----------------------+\n|        [Alice, doctor]|\n|        [Bob, engineer]|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", \"doctor\"), (\"Bob\", \"engineer\")],\n...     (\"name\", \"occupation\"))\n>>> df.select(sf.array(df.name, df.occupation)).show()\n+-----------------------+\n|array(name, occupation)|\n+-----------------------+\n|        [Alice, doctor]|\n|        [Bob, engineer]|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", \"doctor\"), (\"Bob\", \"engineer\")],\n...     (\"name\", \"occupation\"))\n>>> df.select(sf.array(['name', 'occupation'])).show()\n+-----------------------+\n|array(name, occupation)|\n+-----------------------+\n|        [Alice, doctor]|\n|        [Bob, engineer]|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...     [(\"Alice\", 2, 22.2), (\"Bob\", 5, 36.1)],\n...     (\"name\", \"age\", \"weight\"))\n>>> df.select(sf.array(['age', 'weight'])).show()\n+------------------+\n|array(age, weight)|\n+------------------+\n|       [2.0, 22.2]|\n|       [5.0, 36.1]|\n+------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", None), (\"Bob\", \"engineer\")],\n...     (\"name\", \"occupation\"))\n>>> df.select(sf.array('name', 'occupation')).show()\n+-----------------------+\n|array(name, occupation)|\n+-----------------------+\n|          [Alice, NULL]|\n|        [Bob, engineer]|\n+-----------------------+"], "Parameters": [["cols Column or str", "Column names or Column objects that have the same data type."]], "Returns": [["Column", "A new Column of array type, where each value is an array containing the corresponding values\nfrom the input columns."]], "Category": ["Functions"], "index": 352}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_append.html#pyspark.sql.functions.array_append"], "Title": ["array_append"], "Feature": ["array_append"], "Description": "Array function: returns a new array column by appendingvalueto the existing arraycol.", "Examples": [">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=\"c\")])\n>>> df.select(sf.array_append(df.c1, df.c2)).show()\n+--------------------+\n|array_append(c1, c2)|\n+--------------------+\n|        [b, a, c, c]|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3],)], ['data'])\n>>> df.select(sf.array_append(df.data, 4)).show()\n+---------------------+\n|array_append(data, 4)|\n+---------------------+\n|         [1, 2, 3, 4]|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3],)], ['data'])\n>>> df.select(sf.array_append(df.data, None)).show()\n+------------------------+\n|array_append(data, NULL)|\n+------------------------+\n|         [1, 2, 3, NULL]|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n>>> schema = StructType([\n...   StructField(\"data\", ArrayType(IntegerType()), True)\n... ])\n>>> df = spark.createDataFrame([(None,)], schema=schema)\n>>> df.select(sf.array_append(df.data, 4)).show()\n+---------------------+\n|array_append(data, 4)|\n+---------------------+\n|                 NULL|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n>>> schema = StructType([\n...   StructField(\"data\", ArrayType(IntegerType()), True)\n... ])\n>>> df = spark.createDataFrame([([],)], schema=schema)\n>>> df.select(sf.array_append(df.data, 1)).show()\n+---------------------+\n|array_append(data, 1)|\n+---------------------+\n|                  [1]|\n+---------------------+"], "Parameters": [["col Column or str", "The name of the column containing the array."], ["value", "A literal value, or a Column expression to be appended to the array."]], "Returns": [["Column", "A new array column with value appended to the original array."]], "Category": ["Functions"], "index": 353}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_compact.html#pyspark.sql.functions.array_compact"], "Title": ["array_compact"], "Feature": ["array_compact"], "Description": "Array function: removes null values from the array.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, None, 2, 3],)], ['data'])\n>>> df.select(sf.array_compact(df.data)).show()\n+-------------------+\n|array_compact(data)|\n+-------------------+\n|          [1, 2, 3]|\n+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, None, 2, 3],), ([4, 5, None, 4],)], ['data'])\n>>> df.select(sf.array_compact(df.data)).show()\n+-------------------+\n|array_compact(data)|\n+-------------------+\n|          [1, 2, 3]|\n|          [4, 5, 4]|\n+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, StringType, StructField, StructType\n>>> schema = StructType([\n...   StructField(\"data\", ArrayType(StringType()), True)\n... ])\n>>> df = spark.createDataFrame([([None, None, None],)], schema)\n>>> df.select(sf.array_compact(df.data)).show()\n+-------------------+\n|array_compact(data)|\n+-------------------+\n|                 []|\n+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3],)], ['data'])\n>>> df.select(sf.array_compact(df.data)).show()\n+-------------------+\n|array_compact(data)|\n+-------------------+\n|          [1, 2, 3]|\n+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, StringType, StructField, StructType\n>>> schema = StructType([\n...   StructField(\"data\", ArrayType(StringType()), True)\n... ])\n>>> df = spark.createDataFrame([([],)], schema)\n>>> df.select(sf.array_compact(df.data)).show()\n+-------------------+\n|array_compact(data)|\n+-------------------+\n|                 []|\n+-------------------+"], "Parameters": [["col Column or str", "name of column or expression"]], "Returns": [["Column", "A new column that is an array excluding the null values from the input column."]], "Category": ["Functions"], "index": 354}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_contains.html#pyspark.sql.functions.array_contains"], "Title": ["array_contains"], "Feature": ["array_contains"], "Description": "Collection function: This function returns a boolean indicating whether the array\ncontains the given value, returning null if the array is null, true if the array\ncontains the given value, and false otherwise.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n>>> df.select(sf.array_contains(df.data, \"a\")).show()\n+-----------------------+\n|array_contains(data, a)|\n+-----------------------+\n|                   true|\n|                  false|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"], \"c\"),\n...                            ([\"c\", \"d\", \"e\"], \"d\"),\n...                            ([\"e\", \"a\", \"c\"], \"b\")], [\"data\", \"item\"])\n>>> df.select(sf.array_contains(df.data, sf.col(\"item\"))).show()\n+--------------------------+\n|array_contains(data, item)|\n+--------------------------+\n|                      true|\n|                      true|\n|                     false|\n+--------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(None,), ([\"a\", \"b\", \"c\"],)], ['data'])\n>>> df.select(sf.array_contains(df.data, \"a\")).show()\n+-----------------------+\n|array_contains(data, a)|\n+-----------------------+\n|                   NULL|\n|                   true|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", None, \"c\"],)], ['data'])\n>>> df.select(sf.array_contains(df.data, \"a\")).show()\n+-----------------------+\n|array_contains(data, a)|\n+-----------------------+\n|                   true|\n+-----------------------+"], "Parameters": [["col Column or str", "The target column containing the arrays."], ["value", "The value or column to check for in the array."]], "Returns": [["Column", "A new Column of Boolean type, where each value indicates whether the corresponding array\nfrom the input column contains the specified value."]], "Category": ["Functions"], "index": 355}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_distinct.html#pyspark.sql.functions.array_distinct"], "Title": ["array_distinct"], "Feature": ["array_distinct"], "Description": "Array function: removes duplicate values from the array.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3, 2],)], ['data'])\n>>> df.select(sf.array_distinct(df.data)).show()\n+--------------------+\n|array_distinct(data)|\n+--------------------+\n|           [1, 2, 3]|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], ['data'])\n>>> df.select(sf.array_distinct(df.data)).show()\n+--------------------+\n|array_distinct(data)|\n+--------------------+\n|           [1, 2, 3]|\n|              [4, 5]|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 1, 1],)], ['data'])\n>>> df.select(sf.array_distinct(df.data)).show()\n+--------------------+\n|array_distinct(data)|\n+--------------------+\n|                 [1]|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3],)], ['data'])\n>>> df.select(sf.array_distinct(df.data)).show()\n+--------------------+\n|array_distinct(data)|\n+--------------------+\n|           [1, 2, 3]|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n>>> schema = StructType([\n...   StructField(\"data\", ArrayType(IntegerType()), True)\n... ])\n>>> df = spark.createDataFrame([([],)], schema)\n>>> df.select(sf.array_distinct(df.data)).show()\n+--------------------+\n|array_distinct(data)|\n+--------------------+\n|                  []|\n+--------------------+"], "Parameters": [["col Column or str", "name of column or expression"]], "Returns": [["Column", "A new column that is an array of unique values from the input column."]], "Category": ["Functions"], "index": 356}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.inputFiles.html#pyspark.sql.DataFrame.inputFiles"], "Title": ["DataFrame.inputFiles"], "Feature": ["DataFrame.inputFiles"], "Description": "Returns a best-effort snapshot of the files that compose thisDataFrame.\nThis method simply asks each constituent BaseRelation for its respective files and\ntakes the union of all results. Depending on the source relations, this may not find\nall input files. Duplicates are removed.", "Examples": [">>> import tempfile\n>>> with tempfile.TemporaryDirectory(prefix=\"inputFiles\") as d:\n...     # Write a single-row DataFrame into a JSON file\n...     spark.createDataFrame(\n...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n...     ).repartition(1).write.json(d, mode=\"overwrite\")\n...\n...     # Read the JSON file as a DataFrame.\n...     df = spark.read.format(\"json\").load(d)\n...\n...     # Returns the number of input files.\n...     len(df.inputFiles())\n1"], "Parameters": [], "Returns": [["list", "List of file paths."]], "Category": ["DataFrame"], "index": 357}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_except.html#pyspark.sql.functions.array_except"], "Title": ["array_except"], "Feature": ["array_except"], "Description": "Array function: returns a new array containing the elements present in col1 but not in col2,\nwithout duplicates.\nNotes\nThis function does not preserve the order of the elements in the input arrays.", "Examples": [">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n>>> df.select(sf.array_except(df.c1, df.c2)).show()\n+--------------------+\n|array_except(c1, c2)|\n+--------------------+\n|                 [b]|\n+--------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"d\", \"e\", \"f\"])])\n>>> df.select(sf.sort_array(sf.array_except(df.c1, df.c2))).show()\n+--------------------------------------+\n|sort_array(array_except(c1, c2), true)|\n+--------------------------------------+\n|                             [a, b, c]|\n+--------------------------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"a\", \"b\", \"c\"], c2=[\"a\", \"b\", \"c\"])])\n>>> df.select(sf.array_except(df.c1, df.c2)).show()\n+--------------------+\n|array_except(c1, c2)|\n+--------------------+\n|                  []|\n+--------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"a\", \"b\", None], c2=[\"a\", None, \"c\"])])\n>>> df.select(sf.array_except(df.c1, df.c2)).show()\n+--------------------+\n|array_except(c1, c2)|\n+--------------------+\n|                 [b]|\n+--------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> from pyspark.sql.types import ArrayType, StringType, StructField, StructType\n>>> data = [Row(c1=[], c2=[\"a\", \"b\", \"c\"])]\n>>> schema = StructType([\n...   StructField(\"c1\", ArrayType(StringType()), True),\n...   StructField(\"c2\", ArrayType(StringType()), True)\n... ])\n>>> df = spark.createDataFrame(data, schema)\n>>> df.select(sf.array_except(df.c1, df.c2)).show()\n+--------------------+\n|array_except(c1, c2)|\n+--------------------+\n|                  []|\n+--------------------+"], "Parameters": [["col1 Column or str", "Name of column containing the first array."], ["col2 Column or str", "Name of column containing the second array."]], "Returns": [["Column", "A new array containing the elements present in col1 but not in col2."]], "Category": ["Functions"], "index": 358}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_insert.html#pyspark.sql.functions.array_insert"], "Title": ["array_insert"], "Feature": ["array_insert"], "Description": "Array function: Inserts an item into a given array at a specified array index.\nArray indices start at 1, or start from the end if index is negative.\nIndex above array size appends the array, or prepends the array if index is negative,\nwith ‘null’ elements.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(['a', 'b', 'c'],)], ['data'])\n>>> df.select(sf.array_insert(df.data, 2, 'd')).show()\n+------------------------+\n|array_insert(data, 2, d)|\n+------------------------+\n|            [a, d, b, c]|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(['a', 'b', 'c'],)], ['data'])\n>>> df.select(sf.array_insert(df.data, -2, 'd')).show()\n+-------------------------+\n|array_insert(data, -2, d)|\n+-------------------------+\n|             [a, b, d, c]|\n+-------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(['a', 'b', 'c'],)], ['data'])\n>>> df.select(sf.array_insert(df.data, 5, 'e')).show()\n+------------------------+\n|array_insert(data, 5, e)|\n+------------------------+\n|      [a, b, c, NULL, e]|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(['a', 'b', 'c'],)], ['data'])\n>>> df.select(sf.array_insert(df.data, 2, sf.lit(None))).show()\n+---------------------------+\n|array_insert(data, 2, NULL)|\n+---------------------------+\n|            [a, NULL, b, c]|\n+---------------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n>>> schema = StructType([StructField(\"data\", ArrayType(IntegerType()), True)])\n>>> df = spark.createDataFrame([(None,)], schema=schema)\n>>> df.select(sf.array_insert(df.data, 1, 5)).show()\n+------------------------+\n|array_insert(data, 1, 5)|\n+------------------------+\n|                    NULL|\n+------------------------+"], "Parameters": [["arr Column or str", "name of column containing an array"], ["pos Column or str or int", "name of Numeric type column indicating position of insertion\n(starting at index 1, negative position is a start from the back of the array)"], ["value", "a literal value, or a Column expression."]], "Returns": [["Column", "an array of values, including the new specified value"]], "Category": ["Functions"], "index": 359}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_intersect.html#pyspark.sql.functions.array_intersect"], "Title": ["array_intersect"], "Feature": ["array_intersect"], "Description": "Array function: returns a new array containing the intersection of elements in col1 and col2,\nwithout duplicates.\nNotes\nThis function does not preserve the order of the elements in the input arrays.", "Examples": [">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n>>> df.select(sf.sort_array(sf.array_intersect(df.c1, df.c2))).show()\n+-----------------------------------------+\n|sort_array(array_intersect(c1, c2), true)|\n+-----------------------------------------+\n|                                   [a, c]|\n+-----------------------------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"d\", \"e\", \"f\"])])\n>>> df.select(sf.array_intersect(df.c1, df.c2)).show()\n+-----------------------+\n|array_intersect(c1, c2)|\n+-----------------------+\n|                     []|\n+-----------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"a\", \"b\", \"c\"], c2=[\"a\", \"b\", \"c\"])])\n>>> df.select(sf.sort_array(sf.array_intersect(df.c1, df.c2))).show()\n+-----------------------------------------+\n|sort_array(array_intersect(c1, c2), true)|\n+-----------------------------------------+\n|                                [a, b, c]|\n+-----------------------------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"a\", \"b\", None], c2=[\"a\", None, \"c\"])])\n>>> df.select(sf.sort_array(sf.array_intersect(df.c1, df.c2))).show()\n+-----------------------------------------+\n|sort_array(array_intersect(c1, c2), true)|\n+-----------------------------------------+\n|                                [NULL, a]|\n+-----------------------------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> from pyspark.sql.types import ArrayType, StringType, StructField, StructType\n>>> data = [Row(c1=[], c2=[\"a\", \"b\", \"c\"])]\n>>> schema = StructType([\n...   StructField(\"c1\", ArrayType(StringType()), True),\n...   StructField(\"c2\", ArrayType(StringType()), True)\n... ])\n>>> df = spark.createDataFrame(data, schema)\n>>> df.select(sf.array_intersect(df.c1, df.c2)).show()\n+-----------------------+\n|array_intersect(c1, c2)|\n+-----------------------+\n|                     []|\n+-----------------------+"], "Parameters": [["col1 Column or str", "Name of column containing the first array."], ["col2 Column or str", "Name of column containing the second array."]], "Returns": [["Column", "A new array containing the intersection of elements in col1 and col2."]], "Category": ["Functions"], "index": 360}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_join.html#pyspark.sql.functions.array_join"], "Title": ["array_join"], "Feature": ["array_join"], "Description": "Array function: Returns a string column by concatenating the elements of the input\narray column using the delimiter. Null values within the array can be replaced with\na specified string through the null_replacement argument. If null_replacement is\nnot set, null values are ignored.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", \"b\"],)], ['data'])\n>>> df.select(sf.array_join(df.data, \",\")).show()\n+-------------------+\n|array_join(data, ,)|\n+-------------------+\n|              a,b,c|\n|                a,b|\n+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", None, \"c\"],)], ['data'])\n>>> df.select(sf.array_join(df.data, \",\", \"NULL\")).show()\n+-------------------------+\n|array_join(data, ,, NULL)|\n+-------------------------+\n|                 a,NULL,c|\n+-------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", None, \"c\"],)], ['data'])\n>>> df.select(sf.array_join(df.data, \",\")).show()\n+-------------------+\n|array_join(data, ,)|\n+-------------------+\n|                a,c|\n+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import StructType, StructField, ArrayType, StringType\n>>> schema = StructType([StructField(\"data\", ArrayType(StringType()), True)])\n>>> df = spark.createDataFrame([(None,)], schema)\n>>> df.select(sf.array_join(df.data, \",\")).show()\n+-------------------+\n|array_join(data, ,)|\n+-------------------+\n|               NULL|\n+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import StructType, StructField, ArrayType, StringType\n>>> schema = StructType([StructField(\"data\", ArrayType(StringType()), True)])\n>>> df = spark.createDataFrame([([None, None],)], schema)\n>>> df.select(sf.array_join(df.data, \",\", \"NULL\")).show()\n+-------------------------+\n|array_join(data, ,, NULL)|\n+-------------------------+\n|                NULL,NULL|\n+-------------------------+"], "Parameters": [["col Column or str", "The input column containing the arrays to be joined."], ["delimiter str", "The string to be used as the delimiter when joining the array elements."], ["null_replacement str, optional", "The string to replace null values within the array. If not set, null values are ignored."]], "Returns": [["Column", "A new column of string type, where each value is the result of joining the corresponding\narray from the input column."]], "Category": ["Functions"], "index": 361}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_max.html#pyspark.sql.functions.array_max"], "Title": ["array_max"], "Feature": ["array_max"], "Description": "Array function: returns the maximum value of the array.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n>>> df.select(sf.array_max(df.data)).show()\n+---------------+\n|array_max(data)|\n+---------------+\n|              3|\n|             10|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(['apple', 'banana', 'cherry'],)], ['data'])\n>>> df.select(sf.array_max(df.data)).show()\n+---------------+\n|array_max(data)|\n+---------------+\n|         cherry|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(['apple', 1, 'cherry'],)], ['data'])\n>>> df.select(sf.array_max(df.data)).show()\n+---------------+\n|array_max(data)|\n+---------------+\n|         cherry|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([[2, 1], [3, 4]],)], ['data'])\n>>> df.select(sf.array_max(df.data)).show()\n+---------------+\n|array_max(data)|\n+---------------+\n|         [3, 4]|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n>>> schema = StructType([\n...   StructField(\"data\", ArrayType(IntegerType()), True)\n... ])\n>>> df = spark.createDataFrame([([],)], schema=schema)\n>>> df.select(sf.array_max(df.data)).show()\n+---------------+\n|array_max(data)|\n+---------------+\n|           NULL|\n+---------------+"], "Parameters": [["col Column or str", "The name of the column or an expression that represents the array."]], "Returns": [["Column", "A new column that contains the maximum value of each array."]], "Category": ["Functions"], "index": 362}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_min.html#pyspark.sql.functions.array_min"], "Title": ["array_min"], "Feature": ["array_min"], "Description": "Array function: returns the minimum value of the array.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n>>> df.select(sf.array_min(df.data)).show()\n+---------------+\n|array_min(data)|\n+---------------+\n|              1|\n|             -1|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(['apple', 'banana', 'cherry'],)], ['data'])\n>>> df.select(sf.array_min(df.data)).show()\n+---------------+\n|array_min(data)|\n+---------------+\n|          apple|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(['apple', 1, 'cherry'],)], ['data'])\n>>> df.select(sf.array_min(df.data)).show()\n+---------------+\n|array_min(data)|\n+---------------+\n|              1|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([[2, 1], [3, 4]],)], ['data'])\n>>> df.select(sf.array_min(df.data)).show()\n+---------------+\n|array_min(data)|\n+---------------+\n|         [2, 1]|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n>>> schema = StructType([\n...   StructField(\"data\", ArrayType(IntegerType()), True)\n... ])\n>>> df = spark.createDataFrame([([],)], schema=schema)\n>>> df.select(sf.array_min(df.data)).show()\n+---------------+\n|array_min(data)|\n+---------------+\n|           NULL|\n+---------------+"], "Parameters": [["col Column or str", "The name of the column or an expression that represents the array."]], "Returns": [["Column", "A new column that contains the minimum value of each array."]], "Category": ["Functions"], "index": 363}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_position.html#pyspark.sql.functions.array_position"], "Title": ["array_position"], "Feature": ["array_position"], "Description": "Array function: Locates the position of the first occurrence of the given value\nin the given array. Returns null if either of the arguments are null.\nNotes\nThe position is not zero based, but 1 based index. Returns 0 if the given\nvalue could not be found in the array.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],)], ['data'])\n>>> df.select(sf.array_position(df.data, \"a\")).show()\n+-----------------------+\n|array_position(data, a)|\n+-----------------------+\n|                      3|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, StringType, StructField, StructType\n>>> schema = StructType([StructField(\"data\", ArrayType(StringType()), True)])\n>>> df = spark.createDataFrame([([],)], schema=schema)\n>>> df.select(sf.array_position(df.data, \"a\")).show()\n+-----------------------+\n|array_position(data, a)|\n+-----------------------+\n|                      0|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3],)], ['data'])\n>>> df.select(sf.array_position(df.data, 2)).show()\n+-----------------------+\n|array_position(data, 2)|\n+-----------------------+\n|                      2|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],)], ['data'])\n>>> df.select(sf.array_position(df.data, \"d\")).show()\n+-----------------------+\n|array_position(data, d)|\n+-----------------------+\n|                      0|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([None, \"b\", \"a\"],)], ['data'])\n>>> df.select(sf.array_position(df.data, \"a\")).show()\n+-----------------------+\n|array_position(data, a)|\n+-----------------------+\n|                      3|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([10, 20, 30], 20)], ['data', 'col'])\n>>> df.select(sf.array_position(df.data, df.col)).show()\n+-------------------------+\n|array_position(data, col)|\n+-------------------------+\n|                        2|\n+-------------------------+"], "Parameters": [["col Column or str", "target column to work on."], ["value Any", "value or a Column expression to look for. Changed in version 4.0.0: value now also accepts a Column type."]], "Returns": [["Column", "position of the value in the given array if found and 0 otherwise."]], "Category": ["Functions"], "index": 364}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_prepend.html#pyspark.sql.functions.array_prepend"], "Title": ["array_prepend"], "Feature": ["array_prepend"], "Description": "Array function: Returns an array containing the given element as\nthe first element and the rest of the elements from the original array.", "Examples": [">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=\"c\")])\n>>> df.select(sf.array_prepend(df.c1, df.c2)).show()\n+---------------------+\n|array_prepend(c1, c2)|\n+---------------------+\n|         [c, b, a, c]|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3],)], ['data'])\n>>> df.select(sf.array_prepend(df.data, 4)).show()\n+----------------------+\n|array_prepend(data, 4)|\n+----------------------+\n|          [4, 1, 2, 3]|\n+----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3],)], ['data'])\n>>> df.select(sf.array_prepend(df.data, None)).show()\n+-------------------------+\n|array_prepend(data, NULL)|\n+-------------------------+\n|          [NULL, 1, 2, 3]|\n+-------------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n>>> schema = StructType([\n...   StructField(\"data\", ArrayType(IntegerType()), True)\n... ])\n>>> df = spark.createDataFrame([(None,)], schema=schema)\n>>> df.select(sf.array_prepend(df.data, 4)).show()\n+----------------------+\n|array_prepend(data, 4)|\n+----------------------+\n|                  NULL|\n+----------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n>>> schema = StructType([\n...   StructField(\"data\", ArrayType(IntegerType()), True)\n... ])\n>>> df = spark.createDataFrame([([],)], schema=schema)\n>>> df.select(sf.array_prepend(df.data, 1)).show()\n+----------------------+\n|array_prepend(data, 1)|\n+----------------------+\n|                   [1]|\n+----------------------+"], "Parameters": [["col Column or str", "name of column containing array"], ["value", "a literal value, or a Column expression."]], "Returns": [["Column", "an array with the given value prepended."]], "Category": ["Functions"], "index": 365}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_remove.html#pyspark.sql.functions.array_remove"], "Title": ["array_remove"], "Feature": ["array_remove"], "Description": "Array function: Remove all elements that equal to element from the given array.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3, 1, 1],)], ['data'])\n>>> df.select(sf.array_remove(df.data, 1)).show()\n+---------------------+\n|array_remove(data, 1)|\n+---------------------+\n|               [2, 3]|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([4, 5, 5, 4],)], ['data'])\n>>> df.select(sf.array_remove(df.data, 5)).show()\n+---------------------+\n|array_remove(data, 5)|\n+---------------------+\n|      [1, 2, 3, 1, 1]|\n|               [4, 4]|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3],)], ['data'])\n>>> df.select(sf.array_remove(df.data, 4)).show()\n+---------------------+\n|array_remove(data, 4)|\n+---------------------+\n|            [1, 2, 3]|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 1, 1],)], ['data'])\n>>> df.select(sf.array_remove(df.data, 1)).show()\n+---------------------+\n|array_remove(data, 1)|\n+---------------------+\n|                   []|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n>>> schema = StructType([\n...   StructField(\"data\", ArrayType(IntegerType()), True)\n... ])\n>>> df = spark.createDataFrame([([],)], schema)\n>>> df.select(sf.array_remove(df.data, 1)).show()\n+---------------------+\n|array_remove(data, 1)|\n+---------------------+\n|                   []|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3, 1, 1], 1)], ['data', 'col'])\n>>> df.select(sf.array_remove(df.data, df.col)).show()\n+-----------------------+\n|array_remove(data, col)|\n+-----------------------+\n|                 [2, 3]|\n+-----------------------+"], "Parameters": [["col Column or str", "name of column containing array"], ["element", "element or a Column expression to be removed from the array Changed in version 4.0.0: element now also accepts a Column type."]], "Returns": [["Column", "A new column that is an array excluding the given value from the input column."]], "Category": ["Functions"], "index": 366}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_repeat.html#pyspark.sql.functions.array_repeat"], "Title": ["array_repeat"], "Feature": ["array_repeat"], "Description": "Array function: creates an array containing a column repeated count times.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('ab',)], ['data'])\n>>> df.select(sf.array_repeat(df.data, 3)).show()\n+---------------------+\n|array_repeat(data, 3)|\n+---------------------+\n|         [ab, ab, ab]|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(3,)], ['data'])\n>>> df.select(sf.array_repeat(df.data, 2)).show()\n+---------------------+\n|array_repeat(data, 2)|\n+---------------------+\n|               [3, 3]|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(['apple', 'banana'],)], ['data'])\n>>> df.select(sf.array_repeat(df.data, 2)).show(truncate=False)\n+----------------------------------+\n|array_repeat(data, 2)             |\n+----------------------------------+\n|[[apple, banana], [apple, banana]]|\n+----------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import IntegerType, StructType, StructField\n>>> schema = StructType([\n...   StructField(\"data\", IntegerType(), True)\n... ])\n>>> df = spark.createDataFrame([(None, )], schema=schema)\n>>> df.select(sf.array_repeat(df.data, 3)).show()\n+---------------------+\n|array_repeat(data, 3)|\n+---------------------+\n|   [NULL, NULL, NULL]|\n+---------------------+"], "Parameters": [["col Column or str", "The name of the column or an expression that represents the element to be repeated."], ["count Column or str or int", "The name of the column, an expression,\nor an integer that represents the number of times to repeat the element."]], "Returns": [["Column", "A new column that contains an array of repeated elements."]], "Category": ["Functions"], "index": 367}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.intersectAll.html#pyspark.sql.DataFrame.intersectAll"], "Title": ["DataFrame.intersectAll"], "Feature": ["DataFrame.intersectAll"], "Description": "Return a newDataFramecontaining rows in both thisDataFrameand anotherDataFramewhile preserving duplicates.\nThis is equivalent toINTERSECT ALLin SQL. As standard in SQL, this function\nresolves columns by position (not by name).", "Examples": [">>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n>>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n>>> result_df = df1.intersectAll(df2).sort(\"C1\", \"C2\")\n>>> result_df.show()\n+---+---+\n| C1| C2|\n+---+---+\n|  a|  1|\n|  a|  1|\n|  b|  3|\n+---+---+", ">>> df1 = spark.createDataFrame([(1, \"A\"), (2, \"B\")], [\"id\", \"value\"])\n>>> df2 = spark.createDataFrame([(2, \"B\"), (3, \"C\")], [\"id\", \"value\"])\n>>> result_df = df1.intersectAll(df2).sort(\"id\", \"value\")\n>>> result_df.show()\n+---+-----+\n| id|value|\n+---+-----+\n|  2|    B|\n+---+-----+", ">>> df1 = spark.createDataFrame([(1, 2), (1, 2), (3, 4)], [\"A\", \"B\"])\n>>> df2 = spark.createDataFrame([(1, 2), (1, 2)], [\"C\", \"D\"])\n>>> result_df = df1.intersectAll(df2).sort(\"A\", \"B\")\n>>> result_df.show()\n+---+---+\n|  A|  B|\n+---+---+\n|  1|  2|\n|  1|  2|\n+---+---+"], "Parameters": [["other DataFrame", "Another DataFrame that needs to be combined."]], "Returns": [["DataFrame", "Combined DataFrame."]], "Category": ["DataFrame"], "index": 368}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_size.html#pyspark.sql.functions.array_size"], "Title": ["array_size"], "Feature": ["array_size"], "Description": "Array function: returns the total number of elements in the array.\nThe function returns null for null input.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([2, 1, 3],), (None,)], ['data'])\n>>> df.select(sf.array_size(df.data)).show()\n+----------------+\n|array_size(data)|\n+----------------+\n|               3|\n|            NULL|\n+----------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(['apple', 'banana', 'cherry'],)], ['data'])\n>>> df.select(sf.array_size(df.data)).show()\n+----------------+\n|array_size(data)|\n+----------------+\n|               3|\n+----------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(['apple', 1, 'cherry'],)], ['data'])\n>>> df.select(sf.array_size(df.data)).show()\n+----------------+\n|array_size(data)|\n+----------------+\n|               3|\n+----------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([[2, 1], [3, 4]],)], ['data'])\n>>> df.select(sf.array_size(df.data)).show()\n+----------------+\n|array_size(data)|\n+----------------+\n|               2|\n+----------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n>>> schema = StructType([\n...   StructField(\"data\", ArrayType(IntegerType()), True)\n... ])\n>>> df = spark.createDataFrame([([],)], schema=schema)\n>>> df.select(sf.array_size(df.data)).show()\n+----------------+\n|array_size(data)|\n+----------------+\n|               0|\n+----------------+"], "Parameters": [["col Column or str", "The name of the column or an expression that represents the array."]], "Returns": [["Column", "A new column that contains the size of each array."]], "Category": ["Functions"], "index": 369}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_union.html#pyspark.sql.functions.array_union"], "Title": ["array_union"], "Feature": ["array_union"], "Description": "Array function: returns a new array containing the union of elements in col1 and col2,\nwithout duplicates.\nNotes\nThis function does not preserve the order of the elements in the input arrays.", "Examples": [">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n>>> df.select(sf.sort_array(sf.array_union(df.c1, df.c2))).show()\n+-------------------------------------+\n|sort_array(array_union(c1, c2), true)|\n+-------------------------------------+\n|                      [a, b, c, d, f]|\n+-------------------------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"d\", \"e\", \"f\"])])\n>>> df.select(sf.sort_array(sf.array_union(df.c1, df.c2))).show()\n+-------------------------------------+\n|sort_array(array_union(c1, c2), true)|\n+-------------------------------------+\n|                   [a, b, c, d, e, f]|\n+-------------------------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"a\", \"b\", \"c\"], c2=[\"a\", \"b\", \"c\"])])\n>>> df.select(sf.sort_array(sf.array_union(df.c1, df.c2))).show()\n+-------------------------------------+\n|sort_array(array_union(c1, c2), true)|\n+-------------------------------------+\n|                            [a, b, c]|\n+-------------------------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([Row(c1=[\"a\", \"b\", None], c2=[\"a\", None, \"c\"])])\n>>> df.select(sf.sort_array(sf.array_union(df.c1, df.c2))).show()\n+-------------------------------------+\n|sort_array(array_union(c1, c2), true)|\n+-------------------------------------+\n|                      [NULL, a, b, c]|\n+-------------------------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> from pyspark.sql.types import ArrayType, StringType, StructField, StructType\n>>> data = [Row(c1=[], c2=[\"a\", \"b\", \"c\"])]\n>>> schema = StructType([\n...   StructField(\"c1\", ArrayType(StringType()), True),\n...   StructField(\"c2\", ArrayType(StringType()), True)\n... ])\n>>> df = spark.createDataFrame(data, schema)\n>>> df.select(sf.sort_array(sf.array_union(df.c1, df.c2))).show()\n+-------------------------------------+\n|sort_array(array_union(c1, c2), true)|\n+-------------------------------------+\n|                            [a, b, c]|\n+-------------------------------------+"], "Parameters": [["col1 Column or str", "Name of column containing the first array."], ["col2 Column or str", "Name of column containing the second array."]], "Returns": [["Column", "A new array containing the union of elements in col1 and col2."]], "Category": ["Functions"], "index": 370}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.arrays_overlap.html#pyspark.sql.functions.arrays_overlap"], "Title": ["arrays_overlap"], "Feature": ["arrays_overlap"], "Description": "Collection function: This function returns a boolean column indicating if the input arrays\nhave common non-null elements, returning true if they do, null if the arrays do not contain\nany common elements but are not empty and at least one of them contains a null element,\nand false otherwise.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n>>> df.select(sf.arrays_overlap(df.x, df.y)).show()\n+--------------------+\n|arrays_overlap(x, y)|\n+--------------------+\n|                true|\n|               false|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", None], [\"b\", None]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n>>> df.select(sf.arrays_overlap(df.x, df.y)).show()\n+--------------------+\n|arrays_overlap(x, y)|\n+--------------------+\n|                NULL|\n|               false|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(None, [\"b\", \"c\"]), ([\"a\"], None)], ['x', 'y'])\n>>> df.select(sf.arrays_overlap(df.x, df.y)).show()\n+--------------------+\n|arrays_overlap(x, y)|\n+--------------------+\n|                NULL|\n|                NULL|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\"], [\"a\", \"b\"]), ([\"a\"], [\"a\"])], ['x', 'y'])\n>>> df.select(sf.arrays_overlap(df.x, df.y)).show()\n+--------------------+\n|arrays_overlap(x, y)|\n+--------------------+\n|                true|\n|                true|\n+--------------------+"], "Parameters": [["a1, a2 Column or str", "The names of the columns that contain the input arrays."]], "Returns": [["Column", "A new Column of Boolean type, where each value indicates whether the corresponding arrays\nfrom the input columns contain any common elements."]], "Category": ["Functions"], "index": 371}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.arrays_zip.html#pyspark.sql.functions.arrays_zip"], "Title": ["arrays_zip"], "Feature": ["arrays_zip"], "Description": "Array function: Returns a merged array of structs in which the N-th struct contains all\nN-th values of input arrays. If one of the arrays is shorter than others then\nthe resulting struct type value will be anullfor missing elements.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3], ['a', 'b', 'c'])], ['nums', 'letters'])\n>>> df.select(sf.arrays_zip(df.nums, df.letters)).show(truncate=False)\n+-------------------------+\n|arrays_zip(nums, letters)|\n+-------------------------+\n|[{1, a}, {2, b}, {3, c}] |\n+-------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2], ['a', 'b', 'c'])], ['nums', 'letters'])\n>>> df.select(sf.arrays_zip(df.nums, df.letters)).show(truncate=False)\n+---------------------------+\n|arrays_zip(nums, letters)  |\n+---------------------------+\n|[{1, a}, {2, b}, {NULL, c}]|\n+---------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [([1, 2], ['a', 'b'], [True, False])], ['nums', 'letters', 'bools'])\n>>> df.select(sf.arrays_zip(df.nums, df.letters, df.bools)).show(truncate=False)\n+--------------------------------+\n|arrays_zip(nums, letters, bools)|\n+--------------------------------+\n|[{1, a, true}, {2, b, false}]   |\n+--------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, None], ['a', None, 'c'])], ['nums', 'letters'])\n>>> df.select(sf.arrays_zip(df.nums, df.letters)).show(truncate=False)\n+------------------------------+\n|arrays_zip(nums, letters)     |\n+------------------------------+\n|[{1, a}, {2, NULL}, {NULL, c}]|\n+------------------------------+"], "Parameters": [["cols Column or str", "Columns of arrays to be merged."]], "Returns": [["Column", "Merged array of entries."]], "Category": ["Functions"], "index": 372}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.flatten.html#pyspark.sql.functions.flatten"], "Title": ["flatten"], "Feature": ["flatten"], "Description": "Array function: creates a single array from an array of arrays.\nIf a structure of nested arrays is deeper than two levels,\nonly one level of nesting is removed.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],)], ['data'])\n>>> df.select(sf.flatten(df.data)).show()\n+------------------+\n|     flatten(data)|\n+------------------+\n|[1, 2, 3, 4, 5, 6]|\n+------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([None, [4, 5]],)], ['data'])\n>>> df.select(sf.flatten(df.data)).show()\n+-------------+\n|flatten(data)|\n+-------------+\n|         NULL|\n+-------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([[[1, 2], [3, 4]], [[5, 6], [7, 8]]],)], ['data'])\n>>> df.select(sf.flatten(df.data)).show(truncate=False)\n+--------------------------------+\n|flatten(data)                   |\n+--------------------------------+\n|[[1, 2], [3, 4], [5, 6], [7, 8]]|\n+--------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([['a', 'b', 'c'], [1, 2, 3]],)], ['data'])\n>>> df.select(sf.flatten(df.data)).show()\n+------------------+\n|     flatten(data)|\n+------------------+\n|[a, b, c, 1, 2, 3]|\n+------------------+"], "Parameters": [["col Column or str", "The name of the column or expression to be flattened."]], "Returns": [["Column", "A new column that contains the flattened array."]], "Category": ["Functions"], "index": 373}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.get.html#pyspark.sql.functions.get"], "Title": ["get"], "Feature": ["get"], "Description": "Array function: Returns the element of an array at the given (0-based) index.\nIf the index points outside of the array boundaries, then this function\nreturns NULL.\nSee alsopyspark.sql.functions.element_at()\nNotes\nThe position is not 1-based, but 0-based index.\nSupports Spark Connect.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n>>> df.select(sf.get(df.data, 1)).show()\n+------------+\n|get(data, 1)|\n+------------+\n|           b|\n+------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n>>> df.select(sf.get(df.data, 3)).show()\n+------------+\n|get(data, 3)|\n+------------+\n|        NULL|\n+------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"], 2)], ['data', 'index'])\n>>> df.select(sf.get(df.data, df.index)).show()\n+----------------+\n|get(data, index)|\n+----------------+\n|               c|\n+----------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"], 2)], ['data', 'index'])\n>>> df.select(sf.get(df.data, df.index - 1)).show()\n+----------------------+\n|get(data, (index - 1))|\n+----------------------+\n|                     b|\n+----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"], )], ['data'])\n>>> df.select(sf.get(df.data, -1)).show()\n+-------------+\n|get(data, -1)|\n+-------------+\n|         NULL|\n+-------------+"], "Parameters": [["col Column or str", "Name of the column containing the array."], ["index Column or str or int", "Index to check for in the array."]], "Returns": [["Column", "Value at the given position."]], "Category": ["Functions"], "index": 374}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sequence.html#pyspark.sql.functions.sequence"], "Title": ["sequence"], "Feature": ["sequence"], "Description": "Array function: Generate a sequence of integers fromstarttostop, incrementing bystep.\nIfstepis not set, the function increments by 1 ifstartis less than or equal tostop,\notherwise it decrements by 1.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(-2, 2)], ['start', 'stop'])\n>>> df.select(sf.sequence(df.start, df.stop)).show()\n+---------------------+\n|sequence(start, stop)|\n+---------------------+\n|    [-2, -1, 0, 1, 2]|\n+---------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(4, -4, -2)], ['start', 'stop', 'step'])\n>>> df.select(sf.sequence(df.start, df.stop, df.step)).show()\n+---------------------------+\n|sequence(start, stop, step)|\n+---------------------------+\n|          [4, 2, 0, -2, -4]|\n+---------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(5, 1, -1)], ['start', 'stop', 'step'])\n>>> df.select(sf.sequence(df.start, df.stop, df.step)).show()\n+---------------------------+\n|sequence(start, stop, step)|\n+---------------------------+\n|            [5, 4, 3, 2, 1]|\n+---------------------------+"], "Parameters": [["start Column or str", "The starting value (inclusive) of the sequence."], ["stop Column or str", "The last value (inclusive) of the sequence."], ["step Column or str, optional", "The value to add to the current element to get the next element in the sequence.\nThe default is 1 if start is less than or equal to stop , otherwise -1."]], "Returns": [["Column", "A new column that contains an array of sequence values."]], "Category": ["Functions"], "index": 375}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.shuffle.html#pyspark.sql.functions.shuffle"], "Title": ["shuffle"], "Feature": ["shuffle"], "Description": "Array function: Generates a random permutation of the given array.\nNotes\nTheshufflefunction is non-deterministic, meaning the order of the output array\ncan be different for each execution.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT ARRAY(1, 20, 3, 5) AS data\")\n>>> df.select(\"*\", sf.shuffle(df.data, sf.lit(123))).show()\n+-------------+-------------+\n|         data|shuffle(data)|\n+-------------+-------------+\n|[1, 20, 3, 5]|[5, 1, 20, 3]|\n+-------------+-------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT ARRAY(1, 20, NULL, 5) AS data\")\n>>> df.select(\"*\", sf.shuffle(sf.col(\"data\"), 234)).show()\n+----------------+----------------+\n|            data|   shuffle(data)|\n+----------------+----------------+\n|[1, 20, NULL, 5]|[NULL, 5, 20, 1]|\n+----------------+----------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT ARRAY(1, 2, 2, 3, 3, 3) AS data\")\n>>> df.select(\"*\", sf.shuffle(\"data\", 345)).show()\n+------------------+------------------+\n|              data|     shuffle(data)|\n+------------------+------------------+\n|[1, 2, 2, 3, 3, 3]|[2, 3, 3, 1, 2, 3]|\n+------------------+------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT ARRAY(1, 2, 2, 3, 3, 3) AS data\")\n>>> df.select(\"*\", sf.shuffle(\"data\")).show() \n+------------------+------------------+\n|              data|     shuffle(data)|\n+------------------+------------------+\n|[1, 2, 2, 3, 3, 3]|[3, 3, 2, 3, 2, 1]|\n+------------------+------------------+"], "Parameters": [["col Column or str", "The name of the column or expression to be shuffled."], ["seed Column or int, optional", "Seed value for the random generator. New in version 4.0.0."]], "Returns": [["Column", "A new column that contains an array of elements in random order."]], "Category": ["Functions"], "index": 376}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.slice.html#pyspark.sql.functions.slice"], "Title": ["slice"], "Feature": ["slice"], "Description": "Array function: Returns a new array column by slicing the input array column from\na start index to a specific length. The indices start at 1, and can be negative to index\nfrom the end of the array. The length specifies the number of elements in the resulting array.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n>>> df.select(sf.slice(df.x, 2, 2)).show()\n+--------------+\n|slice(x, 2, 2)|\n+--------------+\n|        [2, 3]|\n|           [5]|\n+--------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n>>> df.select(sf.slice(df.x, -1, 1)).show()\n+---------------+\n|slice(x, -1, 1)|\n+---------------+\n|            [3]|\n|            [5]|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2, 3], 2, 2), ([4, 5], 1, 3)], ['x', 'start', 'length'])\n>>> df.select(sf.slice(df.x, df.start, df.length)).show()\n+-----------------------+\n|slice(x, start, length)|\n+-----------------------+\n|                 [2, 3]|\n|                 [4, 5]|\n+-----------------------+"], "Parameters": [["x Column or str", "Input array column or column name to be sliced."], ["start Column , str, or int", "The start index for the slice operation. If negative, starts the index from the\nend of the array."], ["length Column , str, or int", "The length of the slice, representing number of elements in the resulting array."]], "Returns": [["Column", "A new Column object of Array type, where each value is a slice of the corresponding\nlist from the input column."]], "Category": ["Functions"], "index": 377}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sort_array.html#pyspark.sql.functions.sort_array"], "Title": ["sort_array"], "Feature": ["sort_array"], "Description": "Array function: Sorts the input array in ascending or descending order according\nto the natural ordering of the array elements. Null elements will be placed at the beginning\nof the returned array in ascending order or at the end of the returned array in descending\norder.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([([2, 1, None, 3],)], ['data'])\n>>> df.select(sf.sort_array(df.data)).show()\n+----------------------+\n|sort_array(data, true)|\n+----------------------+\n|       [NULL, 1, 2, 3]|\n+----------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([([2, 1, None, 3],)], ['data'])\n>>> df.select(sf.sort_array(df.data, asc=False)).show()\n+-----------------------+\n|sort_array(data, false)|\n+-----------------------+\n|        [3, 2, 1, NULL]|\n+-----------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([([1],)], ['data'])\n>>> df.select(sf.sort_array(df.data)).show()\n+----------------------+\n|sort_array(data, true)|\n+----------------------+\n|                   [1]|\n+----------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, StringType, StructField, StructType\n>>> schema = StructType([StructField(\"data\", ArrayType(StringType()), True)])\n>>> df = spark.createDataFrame([([],)], schema=schema)\n>>> df.select(sf.sort_array(df.data)).show()\n+----------------------+\n|sort_array(data, true)|\n+----------------------+\n|                    []|\n+----------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n>>> schema = StructType([StructField(\"data\", ArrayType(IntegerType()), True)])\n>>> df = spark.createDataFrame([([None, None, None],)], schema=schema)\n>>> df.select(sf.sort_array(df.data)).show()\n+----------------------+\n|sort_array(data, true)|\n+----------------------+\n|    [NULL, NULL, NULL]|\n+----------------------+"], "Parameters": [["col Column or str", "Name of the column or expression."], ["asc bool, optional", "Whether to sort in ascending or descending order. If asc is True (default),\nthen the sorting is in ascending order. If False, then in descending order."]], "Returns": [["Column", "Sorted array."]], "Category": ["Functions"], "index": 378}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.isEmpty.html#pyspark.sql.DataFrame.isEmpty"], "Title": ["DataFrame.isEmpty"], "Feature": ["DataFrame.isEmpty"], "Description": "Checks if theDataFrameis empty and returns a boolean value.\nSee alsoDataFrame.countCounts the number of rows in DataFrame.\nNotes\nUnlikecount(), this method does not trigger any computation.An empty DataFrame has no rows. It may have columns, but no data.", "Examples": [">>> df_empty = spark.createDataFrame([], 'a STRING')\n>>> df_empty.isEmpty()\nTrue", ">>> df_non_empty = spark.createDataFrame([\"a\"], 'STRING')\n>>> df_non_empty.isEmpty()\nFalse", ">>> df_nulls = spark.createDataFrame([(None, None)], 'a STRING, b INT')\n>>> df_nulls.isEmpty()\nFalse", ">>> df_no_rows = spark.createDataFrame([], 'id INT, value STRING')\n>>> df_no_rows.isEmpty()\nTrue"], "Parameters": [], "Returns": [["bool", "Returns True if the DataFrame is empty, False otherwise."]], "Category": ["DataFrame"], "index": 379}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.named_struct.html#pyspark.sql.functions.named_struct"], "Title": ["named_struct"], "Feature": ["named_struct"], "Description": "Creates a struct with the given field names and values.\nSee alsopyspark.sql.functions.struct()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1, 2)], ['a', 'b'])\n>>> df.select(\"*\", sf.named_struct(sf.lit('x'), df.a, sf.lit('y'), \"b\")).show()\n+---+---+------------------------+\n|  a|  b|named_struct(x, a, y, b)|\n+---+---+------------------------+\n|  1|  2|                  {1, 2}|\n+---+---+------------------------+"], "Parameters": [["cols Column or column name", "list of columns to work on."]], "Returns": [["Column", ""]], "Category": ["Functions"], "index": 380}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.struct.html#pyspark.sql.functions.struct"], "Title": ["struct"], "Feature": ["struct"], "Description": "Creates a new struct column.\nSee alsopyspark.sql.functions.named_struct()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n>>> df.select(\"*\", sf.struct('age', df.name)).show()\n+-----+---+-----------------+\n| name|age|struct(age, name)|\n+-----+---+-----------------+\n|Alice|  2|       {2, Alice}|\n|  Bob|  5|         {5, Bob}|\n+-----+---+-----------------+"], "Parameters": [["cols list, set, Column or column name", "column names or Column s to contain in the output struct."]], "Returns": [["Column", "a struct type column of given columns."]], "Category": ["Functions"], "index": 381}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.create_map.html#pyspark.sql.functions.create_map"], "Title": ["create_map"], "Feature": ["create_map"], "Description": "Map function: Creates a new map column from an even number of input columns or\ncolumn references. The input columns are grouped into key-value pairs to form a map.\nFor instance, the input (key1, value1, key2, value2, …) would produce a map that\nassociates key1 with value1, key2 with value2, and so on. The function supports\ngrouping columns as a list as well.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n>>> df.select(sf.create_map('name', 'age')).show()\n+--------------+\n|map(name, age)|\n+--------------+\n|  {Alice -> 2}|\n|    {Bob -> 5}|\n+--------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n>>> df.select(sf.create_map([df.name, df.age])).show()\n+--------------+\n|map(name, age)|\n+--------------+\n|  {Alice -> 2}|\n|    {Bob -> 5}|\n+--------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", 2, \"female\"),\n...     (\"Bob\", 5, \"male\")], (\"name\", \"age\", \"gender\"))\n>>> df.select(sf.create_map(sf.lit('name'), df['name'],\n...     sf.lit('gender'), df['gender'])).show(truncate=False)\n+---------------------------------+\n|map(name, name, gender, gender)  |\n+---------------------------------+\n|{name -> Alice, gender -> female}|\n|{name -> Bob, gender -> male}    |\n+---------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", 2, 22.2),\n...     (\"Bob\", 5, 36.1)], (\"name\", \"age\", \"weight\"))\n>>> df.select(sf.create_map(sf.lit('age'), df['age'],\n...     sf.lit('weight'), df['weight'])).show(truncate=False)\n+-----------------------------+\n|map(age, age, weight, weight)|\n+-----------------------------+\n|{age -> 2.0, weight -> 22.2} |\n|{age -> 5.0, weight -> 36.1} |\n+-----------------------------+"], "Parameters": [["cols Column or str", "The input column names or Column objects grouped into\nkey-value pairs. These can also be expressed as a list of columns."]], "Returns": [["Column", "A new Column of Map type, where each value is a map formed from the corresponding\nkey-value pairs provided in the input arguments."]], "Category": ["Functions"], "index": 382}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.map_concat.html#pyspark.sql.functions.map_concat"], "Title": ["map_concat"], "Feature": ["map_concat"], "Description": "Map function: Returns the union of all given maps.\nNotes\nFor duplicate keys in input maps, the handling is governed byspark.sql.mapKeyDedupPolicy.\nBy default, it throws an exception. If set toLAST_WIN, it uses the last map’s value.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c') as map2\")\n>>> df.select(sf.map_concat(\"map1\", \"map2\")).show(truncate=False)\n+------------------------+\n|map_concat(map1, map2)  |\n+------------------------+\n|{1 -> a, 2 -> b, 3 -> c}|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> originalmapKeyDedupPolicy = spark.conf.get(\"spark.sql.mapKeyDedupPolicy\")\n>>> spark.conf.set(\"spark.sql.mapKeyDedupPolicy\", \"LAST_WIN\")\n>>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(2, 'c', 3, 'd') as map2\")\n>>> df.select(sf.map_concat(\"map1\", \"map2\")).show(truncate=False)\n+------------------------+\n|map_concat(map1, map2)  |\n+------------------------+\n|{1 -> a, 2 -> c, 3 -> d}|\n+------------------------+\n>>> spark.conf.set(\"spark.sql.mapKeyDedupPolicy\", originalmapKeyDedupPolicy)", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, 'a') as map1, map(2, 'b') as map2, map(3, 'c') as map3\")\n>>> df.select(sf.map_concat(\"map1\", \"map2\", \"map3\")).show(truncate=False)\n+----------------------------+\n|map_concat(map1, map2, map3)|\n+----------------------------+\n|{1 -> a, 2 -> b, 3 -> c}    |\n+----------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map() as map2\")\n>>> df.select(sf.map_concat(\"map1\", \"map2\")).show(truncate=False)\n+----------------------+\n|map_concat(map1, map2)|\n+----------------------+\n|{1 -> a, 2 -> b}      |\n+----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, null) as map2\")\n>>> df.select(sf.map_concat(\"map1\", \"map2\")).show(truncate=False)\n+---------------------------+\n|map_concat(map1, map2)     |\n+---------------------------+\n|{1 -> a, 2 -> b, 3 -> NULL}|\n+---------------------------+"], "Parameters": [["cols Column or str", "Column names or Column"]], "Returns": [["Column", "A map of merged entries from other maps."]], "Category": ["Functions"], "index": 383}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.map_contains_key.html#pyspark.sql.functions.map_contains_key"], "Title": ["map_contains_key"], "Feature": ["map_contains_key"], "Description": "Map function: Returns true if the map contains the key.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n>>> df.select(sf.map_contains_key(\"data\", 1)).show()\n+-------------------------+\n|map_contains_key(data, 1)|\n+-------------------------+\n|                     true|\n+-------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n>>> df.select(sf.map_contains_key(\"data\", -1)).show()\n+--------------------------+\n|map_contains_key(data, -1)|\n+--------------------------+\n|                     false|\n+--------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data, 1 as key\")\n>>> df.select(sf.map_contains_key(\"data\", sf.col(\"key\"))).show()\n+---------------------------+\n|map_contains_key(data, key)|\n+---------------------------+\n|                       true|\n+---------------------------+"], "Parameters": [["col Column or str", "The name of the column or an expression that represents the map."], ["value", "A literal value, or a Column expression. Changed in version 4.0.0: value now also accepts a Column type."]], "Returns": [["Column", "True if key is in the map and False otherwise."]], "Category": ["Functions"], "index": 384}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.map_entries.html#pyspark.sql.functions.map_entries"], "Title": ["map_entries"], "Feature": ["map_entries"], "Description": "Map function: Returns an unordered array of all entries in the given map.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n>>> df.select(sf.sort_array(sf.map_entries(\"data\"))).show()\n+-----------------------------------+\n|sort_array(map_entries(data), true)|\n+-----------------------------------+\n|                   [{1, a}, {2, b}]|\n+-----------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(array(1, 2), array('a', 'b'), \"\n...   \"array(3, 4), array('c', 'd')) as data\")\n>>> df.select(sf.sort_array(sf.map_entries(\"data\"))).show(truncate=False)\n+------------------------------------+\n|sort_array(map_entries(data), true) |\n+------------------------------------+\n|[{[1, 2], [a, b]}, {[3, 4], [c, d]}]|\n+------------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> originalmapKeyDedupPolicy = spark.conf.get(\"spark.sql.mapKeyDedupPolicy\")\n>>> spark.conf.set(\"spark.sql.mapKeyDedupPolicy\", \"LAST_WIN\")\n>>> df = spark.sql(\"SELECT map(1, 'a', 1, 'b') as data\")\n>>> df.select(sf.map_entries(\"data\")).show()\n+-----------------+\n|map_entries(data)|\n+-----------------+\n|         [{1, b}]|\n+-----------------+\n>>> spark.conf.set(\"spark.sql.mapKeyDedupPolicy\", originalmapKeyDedupPolicy)", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map() as data\")\n>>> df.select(sf.map_entries(\"data\")).show()\n+-----------------+\n|map_entries(data)|\n+-----------------+\n|               []|\n+-----------------+"], "Parameters": [["col Column or str", "Name of column or expression"]], "Returns": [["Column", "An array of key value pairs as a struct type"]], "Category": ["Functions"], "index": 385}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.map_from_arrays.html#pyspark.sql.functions.map_from_arrays"], "Title": ["map_from_arrays"], "Feature": ["map_from_arrays"], "Description": "Map function: Creates a new map from two arrays. This function takes two arrays of\nkeys and values respectively, and returns a new map column.\n.. versionadded:: 2.4.0\nNotes\nThe input arrays for keys and values must have the same length and all elements\nin keys should not be null. If these conditions are not met, an exception will be thrown.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([2, 5], ['a', 'b'])], ['k', 'v'])\n>>> df.select(sf.map_from_arrays(df.k, df.v)).show()\n+---------------------+\n|map_from_arrays(k, v)|\n+---------------------+\n|     {2 -> a, 5 -> b}|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([([1, 2], ['a', None])], ['k', 'v'])\n>>> df.select(sf.map_from_arrays(df.k, df.v)).show()\n+---------------------+\n|map_from_arrays(k, v)|\n+---------------------+\n|  {1 -> a, 2 -> NULL}|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, StringType, IntegerType, StructType, StructField\n>>> schema = StructType([\n...   StructField('k', ArrayType(IntegerType())),\n...   StructField('v', ArrayType(StringType()))\n... ])\n>>> df = spark.createDataFrame([([], [])], schema=schema)\n>>> df.select(sf.map_from_arrays(df.k, df.v)).show()\n+---------------------+\n|map_from_arrays(k, v)|\n+---------------------+\n|                   {}|\n+---------------------+"], "Parameters": [["col1 Column or str", "Name of column containing a set of keys. All elements should not be null."], ["col2 Column or str", "Name of column containing a set of values."]], "Returns": [["Column", "A column of map type."]], "Category": ["Functions"], "index": 386}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.map_from_entries.html#pyspark.sql.functions.map_from_entries"], "Title": ["map_from_entries"], "Feature": ["map_from_entries"], "Description": "Map function: Transforms an array of key-value pair entries (structs with two fields)\ninto a map. The first field of each entry is used as the key and the second field\nas the value in the resulting map column", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT array(struct(1, 'a'), struct(2, 'b')) as data\")\n>>> df.select(sf.map_from_entries(df.data)).show()\n+----------------------+\n|map_from_entries(data)|\n+----------------------+\n|      {1 -> a, 2 -> b}|\n+----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT array(struct(1, null), struct(2, 'b')) as data\")\n>>> df.select(sf.map_from_entries(df.data)).show()\n+----------------------+\n|map_from_entries(data)|\n+----------------------+\n|   {1 -> NULL, 2 -> b}|\n+----------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> df = spark.createDataFrame([([Row(1, \"a\"), Row(2, \"b\")],), ([Row(3, \"c\")],)], ['data'])\n>>> df.select(sf.map_from_entries(df.data)).show()\n+----------------------+\n|map_from_entries(data)|\n+----------------------+\n|      {1 -> a, 2 -> b}|\n|              {3 -> c}|\n+----------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, StringType, IntegerType, StructType, StructField\n>>> schema = StructType([\n...   StructField(\"data\", ArrayType(\n...     StructType([\n...       StructField(\"key\", IntegerType()),\n...       StructField(\"value\", StringType())\n...     ])\n...   ), True)\n... ])\n>>> df = spark.createDataFrame([([],)], schema=schema)\n>>> df.select(sf.map_from_entries(df.data)).show()\n+----------------------+\n|map_from_entries(data)|\n+----------------------+\n|                    {}|\n+----------------------+"], "Parameters": [["col Column or str", "Name of column or expression"]], "Returns": [["Column", "A map created from the given array of entries."]], "Category": ["Functions"], "index": 387}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.map_keys.html#pyspark.sql.functions.map_keys"], "Title": ["map_keys"], "Feature": ["map_keys"], "Description": "Map function: Returns an unordered array containing the keys of the map.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n>>> df.select(sf.sort_array(sf.map_keys(\"data\"))).show()\n+--------------------------------+\n|sort_array(map_keys(data), true)|\n+--------------------------------+\n|                          [1, 2]|\n+--------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(array(1, 2), 'a', array(3, 4), 'b') as data\")\n>>> df.select(sf.sort_array(sf.map_keys(\"data\"))).show()\n+--------------------------------+\n|sort_array(map_keys(data), true)|\n+--------------------------------+\n|                [[1, 2], [3, 4]]|\n+--------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> originalmapKeyDedupPolicy = spark.conf.get(\"spark.sql.mapKeyDedupPolicy\")\n>>> spark.conf.set(\"spark.sql.mapKeyDedupPolicy\", \"LAST_WIN\")\n>>> df = spark.sql(\"SELECT map(1, 'a', 1, 'b') as data\")\n>>> df.select(sf.map_keys(\"data\")).show()\n+--------------+\n|map_keys(data)|\n+--------------+\n|           [1]|\n+--------------+\n>>> spark.conf.set(\"spark.sql.mapKeyDedupPolicy\", originalmapKeyDedupPolicy)", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map() as data\")\n>>> df.select(sf.map_keys(\"data\")).show()\n+--------------+\n|map_keys(data)|\n+--------------+\n|            []|\n+--------------+"], "Parameters": [["col Column or str", "Name of column or expression"]], "Returns": [["Column", "Keys of the map as an array."]], "Category": ["Functions"], "index": 388}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.map_values.html#pyspark.sql.functions.map_values"], "Title": ["map_values"], "Feature": ["map_values"], "Description": "Map function: Returns an unordered array containing the values of the map.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n>>> df.select(sf.sort_array(sf.map_values(\"data\"))).show()\n+----------------------------------+\n|sort_array(map_values(data), true)|\n+----------------------------------+\n|                            [a, b]|\n+----------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, array('a', 'b'), 2, array('c', 'd')) as data\")\n>>> df.select(sf.sort_array(sf.map_values(\"data\"))).show()\n+----------------------------------+\n|sort_array(map_values(data), true)|\n+----------------------------------+\n|                  [[a, b], [c, d]]|\n+----------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, null, 2, 'b') as data\")\n>>> df.select(sf.sort_array(sf.map_values(\"data\"))).show()\n+----------------------------------+\n|sort_array(map_values(data), true)|\n+----------------------------------+\n|                         [NULL, b]|\n+----------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map(1, 'a', 2, 'a') as data\")\n>>> df.select(sf.map_values(\"data\")).show()\n+----------------+\n|map_values(data)|\n+----------------+\n|          [a, a]|\n+----------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT map() as data\")\n>>> df.select(sf.map_values(\"data\")).show()\n+----------------+\n|map_values(data)|\n+----------------+\n|              []|\n+----------------+"], "Parameters": [["col Column or str", "Name of column or expression"]], "Returns": [["Column", "Values of the map as an array."]], "Category": ["Functions"], "index": 389}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.isLocal.html#pyspark.sql.DataFrame.isLocal"], "Title": ["DataFrame.isLocal"], "Feature": ["DataFrame.isLocal"], "Description": "ReturnsTrueif thecollect()andtake()methods can be run locally\n(without any Spark executors).", "Examples": [">>> df = spark.sql(\"SHOW TABLES\")\n>>> df.isLocal()\nTrue"], "Parameters": [], "Returns": [["bool", ""]], "Category": ["DataFrame"], "index": 390}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.str_to_map.html#pyspark.sql.functions.str_to_map"], "Title": ["str_to_map"], "Feature": ["str_to_map"], "Description": "Map function: Converts a string into a map after splitting the text into key/value pairs\nusing delimiters. BothpairDelimandkeyValueDelimare treated as regular expressions.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"a:1,b:2,c:3\",)], [\"e\"])\n>>> df.select(sf.str_to_map(df.e)).show(truncate=False)\n+------------------------+\n|str_to_map(e, ,, :)     |\n+------------------------+\n|{a -> 1, b -> 2, c -> 3}|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"a=1;b=2;c=3\",)], [\"e\"])\n>>> df.select(sf.str_to_map(df.e, sf.lit(\";\"), sf.lit(\"=\"))).show(truncate=False)\n+------------------------+\n|str_to_map(e, ;, =)     |\n+------------------------+\n|{a -> 1, b -> 2, c -> 3}|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"a:1,b:2,c:3\",), (\"d=4;e=5;f=6\",)], [\"e\"])\n>>> df.select(sf.str_to_map(df.e,\n...   sf.when(df.e.contains(\";\"), sf.lit(\";\")).otherwise(sf.lit(\",\")),\n...   sf.when(df.e.contains(\"=\"), sf.lit(\"=\")).otherwise(sf.lit(\":\"))).alias(\"str_to_map\")\n... ).show(truncate=False)\n+------------------------+\n|str_to_map              |\n+------------------------+\n|{a -> 1, b -> 2, c -> 3}|\n|{d -> 4, e -> 5, f -> 6}|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"a:1,b:2,c:3\", \",\"), (\"d=4;e=5;f=6\", \";\")], [\"e\", \"delim\"])\n>>> df.select(sf.str_to_map(df.e, df.delim, sf.lit(\":\"))).show(truncate=False)\n+---------------------------------------+\n|str_to_map(e, delim, :)                |\n+---------------------------------------+\n|{a -> 1, b -> 2, c -> 3}               |\n|{d=4 -> NULL, e=5 -> NULL, f=6 -> NULL}|\n+---------------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"a:1,b:2,c:3\", \":\"), (\"d=4;e=5;f=6\", \"=\")], [\"e\", \"delim\"])\n>>> df.select(sf.str_to_map(df.e, sf.lit(\",\"), df.delim)).show(truncate=False)\n+------------------------+\n|str_to_map(e, ,, delim) |\n+------------------------+\n|{a -> 1, b -> 2, c -> 3}|\n|{d -> 4;e=5;f=6}        |\n+------------------------+"], "Parameters": [["text Column or str", "Input column or strings."], ["pairDelim Column or str, optional", "Delimiter to use to split pairs. Default is comma (,)."], ["keyValueDelim Column or str, optional", "Delimiter to use to split key/value. Default is colon (:)."]], "Returns": [["Column", "A new column of map type where each string in the original column is converted into a map."]], "Category": ["Functions"], "index": 391}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.any_value.html#pyspark.sql.functions.any_value"], "Title": ["any_value"], "Feature": ["any_value"], "Description": "Returns some value ofcolfor a group of rows.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...     [(None, 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"c1\", \"c2\"])\n>>> df.select(sf.any_value('c1'), sf.any_value('c2')).show()\n+-------------+-------------+\n|any_value(c1)|any_value(c2)|\n+-------------+-------------+\n|         NULL|            1|\n+-------------+-------------+", ">>> df.select(sf.any_value('c1', True), sf.any_value('c2', True)).show()\n+-------------+-------------+\n|any_value(c1)|any_value(c2)|\n+-------------+-------------+\n|            a|            1|\n+-------------+-------------+"], "Parameters": [["col Column or column name", "target column to work on."], ["ignoreNulls Column or bool, optional", "if first value is null then look for first non-null value."]], "Returns": [["Column", "some value of col for a group of rows."]], "Category": ["Functions"], "index": 392}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.approx_count_distinct.html#pyspark.sql.functions.approx_count_distinct"], "Title": ["approx_count_distinct"], "Feature": ["approx_count_distinct"], "Description": "This aggregate function returns a newColumn, which estimates\nthe approximate distinct count of elements in a specified column or a group of columns.\nSee alsopyspark.sql.functions.count_distinct()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([1,2,2,3], \"int\")\n>>> df.agg(sf.approx_count_distinct(\"value\")).show()\n+----------------------------+\n|approx_count_distinct(value)|\n+----------------------------+\n|                           3|\n+----------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"apple\",), (\"orange\",), (\"apple\",), (\"banana\",)], ['fruit'])\n>>> df.agg(sf.approx_count_distinct(\"fruit\")).show()\n+----------------------------+\n|approx_count_distinct(fruit)|\n+----------------------------+\n|                           3|\n+----------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...     [(\"Alice\", 1), (\"Alice\", 2), (\"Bob\", 3), (\"Bob\", 3)], [\"name\", \"value\"])\n>>> df = df.withColumn(\"combined\", sf.struct(\"name\", \"value\"))\n>>> df.agg(sf.approx_count_distinct(df.combined)).show()\n+-------------------------------+\n|approx_count_distinct(combined)|\n+-------------------------------+\n|                              3|\n+-------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.range(100000).agg(\n...     sf.approx_count_distinct(\"id\").alias('with_default_rsd'),\n...     sf.approx_count_distinct(\"id\", 0.1).alias('with_rsd_0.1')\n... ).show()\n+----------------+------------+\n|with_default_rsd|with_rsd_0.1|\n+----------------+------------+\n|           95546|      102065|\n+----------------+------------+"], "Parameters": [["col Column or column name", "The label of the column to count distinct values in."], ["rsd float, optional", "The maximum allowed relative standard deviation (default = 0.05).\nIf rsd < 0.01, it would be more efficient to use count_distinct() ."]], "Returns": [["Column", "A new Column object representing the approximate unique count."]], "Category": ["Functions"], "index": 393}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.approx_percentile.html#pyspark.sql.functions.approx_percentile"], "Title": ["approx_percentile"], "Feature": ["approx_percentile"], "Description": "Returns the approximatepercentileof the numeric columncolwhich is the smallest value\nin the orderedcolvalues (sorted from least to greatest) such that no more thanpercentageofcolvalues is less than the value or equal to that value.\nSee alsopyspark.sql.functions.median()pyspark.sql.functions.percentile()pyspark.sql.functions.percentile_approx()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> key = (sf.col(\"id\") % 3).alias(\"key\")\n>>> value = (sf.randn(42) + key * 10).alias(\"value\")\n>>> df = spark.range(0, 1000, 1, 1).select(key, value)\n>>> df.select(\n...     sf.approx_percentile(\"value\", [0.25, 0.5, 0.75], 1000000)\n... ).show(truncate=False)\n+----------------------------------------------------------+\n|approx_percentile(value, array(0.25, 0.5, 0.75), 1000000) |\n+----------------------------------------------------------+\n|[0.7264430125286..., 9.98975299938..., 19.335304783039...]|\n+----------------------------------------------------------+", ">>> df.groupBy(\"key\").agg(\n...     sf.approx_percentile(\"value\", sf.lit(0.5), sf.lit(1000000))\n... ).sort(\"key\").show()\n+---+--------------------------------------+\n|key|approx_percentile(value, 0.5, 1000000)|\n+---+--------------------------------------+\n|  0|                  -0.03519435193070...|\n|  1|                     9.990389751837...|\n|  2|                    19.967859769284...|\n+---+--------------------------------------+"], "Parameters": [["col Column or column name", "input column."], ["percentage Column , float, list of floats or tuple of floats", "percentage in decimal (must be between 0.0 and 1.0).\nWhen percentage is an array, each value of the percentage array must be between 0.0 and 1.0.\nIn this case, returns the approximate percentile array of column col\nat the given percentage array."], ["accuracy Column or int", "is a positive numeric literal which controls approximation accuracy\nat the cost of memory. Higher value of accuracy yields better accuracy,\n1.0/accuracy is the relative error of the approximation. (default: 10000)."]], "Returns": [["Column", "approximate percentile of the numeric column."]], "Category": ["Functions"], "index": 394}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_agg.html#pyspark.sql.functions.array_agg"], "Title": ["array_agg"], "Feature": ["array_agg"], "Description": "Aggregate function: returns a list of objects with duplicates.\nSee alsopyspark.sql.functions.collect_list()pyspark.sql.functions.collect_set()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n>>> df.agg(sf.sort_array(sf.array_agg('c')).alias('sorted_list')).show()\n+-----------+\n|sorted_list|\n+-----------+\n|  [1, 1, 2]|\n+-----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[\"apple\"],[\"apple\"],[\"banana\"]], [\"c\"])\n>>> df.agg(sf.sort_array(sf.array_agg('c')).alias('sorted_list')).show(truncate=False)\n+----------------------+\n|sorted_list           |\n+----------------------+\n|[apple, apple, banana]|\n+----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1],[None],[2]], [\"c\"])\n>>> df.agg(sf.sort_array(sf.array_agg('c')).alias('sorted_list')).show()\n+-----------+\n|sorted_list|\n+-----------+\n|     [1, 2]|\n+-----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1],[\"apple\"],[2]], [\"c\"])\n>>> df.agg(sf.sort_array(sf.array_agg('c')).alias('sorted_list')).show()\n+-------------+\n|  sorted_list|\n+-------------+\n|[1, 2, apple]|\n+-------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "list of objects with duplicates."]], "Category": ["Functions"], "index": 395}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.avg.html#pyspark.sql.functions.avg"], "Title": ["avg"], "Feature": ["avg"], "Description": "Aggregate function: returns the average of the values in a group.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1982, 15), (1990, 2)], [\"birth\", \"age\"])\n>>> df.select(sf.avg(\"age\")).show()\n+--------+\n|avg(age)|\n+--------+\n|     8.5|\n+--------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1982, None), (1990, 2), (2000, 4)], [\"birth\", \"age\"])\n>>> df.select(sf.avg(\"age\")).show()\n+--------+\n|avg(age)|\n+--------+\n|     3.0|\n+--------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 396}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bit_and.html#pyspark.sql.functions.bit_and"], "Title": ["bit_and"], "Feature": ["bit_and"], "Description": "Aggregate function: returns the bitwise AND of all non-null input values, or null if none.\nSee alsopyspark.sql.functions.bit_or()pyspark.sql.functions.bit_xor()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n>>> df.select(sf.bit_and(\"c\")).show()\n+----------+\n|bit_and(c)|\n+----------+\n|         0|\n+----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1],[None],[2]], [\"c\"])\n>>> df.select(sf.bit_and(\"c\")).show()\n+----------+\n|bit_and(c)|\n+----------+\n|         0|\n+----------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import IntegerType, StructType, StructField\n>>> schema = StructType([StructField(\"c\", IntegerType(), True)])\n>>> df = spark.createDataFrame([[None],[None],[None]], schema=schema)\n>>> df.select(sf.bit_and(\"c\")).show()\n+----------+\n|bit_and(c)|\n+----------+\n|      NULL|\n+----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[5]], [\"c\"])\n>>> df.select(sf.bit_and(\"c\")).show()\n+----------+\n|bit_and(c)|\n+----------+\n|         5|\n+----------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the bitwise AND of all non-null input values, or null if none."]], "Category": ["Functions"], "index": 397}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bit_or.html#pyspark.sql.functions.bit_or"], "Title": ["bit_or"], "Feature": ["bit_or"], "Description": "Aggregate function: returns the bitwise OR of all non-null input values, or null if none.\nSee alsopyspark.sql.functions.bit_and()pyspark.sql.functions.bit_xor()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n>>> df.select(sf.bit_or(\"c\")).show()\n+---------+\n|bit_or(c)|\n+---------+\n|        3|\n+---------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1],[None],[2]], [\"c\"])\n>>> df.select(sf.bit_or(\"c\")).show()\n+---------+\n|bit_or(c)|\n+---------+\n|        3|\n+---------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import IntegerType, StructType, StructField\n>>> schema = StructType([StructField(\"c\", IntegerType(), True)])\n>>> df = spark.createDataFrame([[None],[None],[None]], schema=schema)\n>>> df.select(sf.bit_or(\"c\")).show()\n+---------+\n|bit_or(c)|\n+---------+\n|     NULL|\n+---------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[5]], [\"c\"])\n>>> df.select(sf.bit_or(\"c\")).show()\n+---------+\n|bit_or(c)|\n+---------+\n|        5|\n+---------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the bitwise OR of all non-null input values, or null if none."]], "Category": ["Functions"], "index": 398}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bit_xor.html#pyspark.sql.functions.bit_xor"], "Title": ["bit_xor"], "Feature": ["bit_xor"], "Description": "Aggregate function: returns the bitwise XOR of all non-null input values, or null if none.\nSee alsopyspark.sql.functions.bit_and()pyspark.sql.functions.bit_or()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n>>> df.select(sf.bit_xor(\"c\")).show()\n+----------+\n|bit_xor(c)|\n+----------+\n|         2|\n+----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1],[None],[2]], [\"c\"])\n>>> df.select(sf.bit_xor(\"c\")).show()\n+----------+\n|bit_xor(c)|\n+----------+\n|         3|\n+----------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import IntegerType, StructType, StructField\n>>> schema = StructType([StructField(\"c\", IntegerType(), True)])\n>>> df = spark.createDataFrame([[None],[None],[None]], schema=schema)\n>>> df.select(sf.bit_xor(\"c\")).show()\n+----------+\n|bit_xor(c)|\n+----------+\n|      NULL|\n+----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[5]], [\"c\"])\n>>> df.select(sf.bit_xor(\"c\")).show()\n+----------+\n|bit_xor(c)|\n+----------+\n|         5|\n+----------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the bitwise XOR of all non-null input values, or null if none."]], "Category": ["Functions"], "index": 399}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bitmap_construct_agg.html#pyspark.sql.functions.bitmap_construct_agg"], "Title": ["bitmap_construct_agg"], "Feature": ["bitmap_construct_agg"], "Description": "Returns a bitmap with the positions of the bits set from all the values from the input column.\nThe input column will most likely be bitmap_bit_position().\nSee alsopyspark.sql.functions.bitmap_bit_position()pyspark.sql.functions.bitmap_bucket_number()pyspark.sql.functions.bitmap_count()pyspark.sql.functions.bitmap_or_agg()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1,),(2,),(3,)], [\"a\"])\n>>> df.select(\n...     sf.bitmap_construct_agg(sf.bitmap_bit_position('a'))\n... ).show()\n+--------------------------------------------+\n|bitmap_construct_agg(bitmap_bit_position(a))|\n+--------------------------------------------+\n|                        [07 00 00 00 00 0...|\n+--------------------------------------------+"], "Parameters": [["col Column or column name", "The input column will most likely be bitmap_bit_position()."]], "Returns": [], "Category": ["Functions"], "index": 400}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.isStreaming.html#pyspark.sql.DataFrame.isStreaming"], "Title": ["DataFrame.isStreaming"], "Feature": ["DataFrame.isStreaming"], "Description": "ReturnsTrueif thisDataFramecontains one or more sources that\ncontinuously return data as it arrives. ADataFramethat reads data from a\nstreaming source must be executed as aStreamingQueryusing thestart()method inDataStreamWriter.  Methods that return a single answer, (e.g.,count()orcollect()) will throw anAnalysisExceptionwhen there\nis a streaming source present.\nNotes\nThis API is evolving.", "Examples": [">>> df = spark.readStream.format(\"rate\").load()\n>>> df.isStreaming\nTrue"], "Parameters": [], "Returns": [["bool", "Whether it’s streaming DataFrame or not."]], "Category": ["DataFrame"], "index": 401}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bitmap_or_agg.html#pyspark.sql.functions.bitmap_or_agg"], "Title": ["bitmap_or_agg"], "Feature": ["bitmap_or_agg"], "Description": "Returns a bitmap that is the bitwise OR of all of the bitmaps from the input column.\nThe input column should be bitmaps created from bitmap_construct_agg().\nSee alsopyspark.sql.functions.bitmap_bit_position()pyspark.sql.functions.bitmap_bucket_number()pyspark.sql.functions.bitmap_construct_agg()pyspark.sql.functions.bitmap_count()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"10\",),(\"20\",),(\"40\",)], [\"a\"])\n>>> df.select(sf.bitmap_or_agg(sf.to_binary(df.a, sf.lit(\"hex\")))).show()\n+--------------------------------+\n|bitmap_or_agg(to_binary(a, hex))|\n+--------------------------------+\n|            [70 00 00 00 00 0...|\n+--------------------------------+"], "Parameters": [["col Column or column name", "The input column should be bitmaps created from bitmap_construct_agg()."]], "Returns": [], "Category": ["Functions"], "index": 402}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bool_and.html#pyspark.sql.functions.bool_and"], "Title": ["bool_and"], "Feature": ["bool_and"], "Description": "Aggregate function: returns true if all values ofcolare true.\nSee alsopyspark.sql.functions.bool_or()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[True], [True], [True]], [\"flag\"])\n>>> df.select(sf.bool_and(\"flag\")).show()\n+--------------+\n|bool_and(flag)|\n+--------------+\n|          true|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[True], [False], [True]], [\"flag\"])\n>>> df.select(sf.bool_and(\"flag\")).show()\n+--------------+\n|bool_and(flag)|\n+--------------+\n|         false|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([[False], [False], [False]], [\"flag\"])\n>>> df.select(sf.bool_and(\"flag\")).show()\n+--------------+\n|bool_and(flag)|\n+--------------+\n|         false|\n+--------------+"], "Parameters": [["col Column or column name", "column to check if all values are true."]], "Returns": [["Column", "true if all values of col are true, false otherwise."]], "Category": ["Functions"], "index": 403}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bool_or.html#pyspark.sql.functions.bool_or"], "Title": ["bool_or"], "Feature": ["bool_or"], "Description": "Aggregate function: returns true if at least one value ofcolis true.\nSee alsopyspark.sql.functions.bool_and()", "Examples": [">>> df = spark.createDataFrame([[True], [True], [True]], [\"flag\"])\n>>> df.select(bool_or(\"flag\")).show()\n+-------------+\n|bool_or(flag)|\n+-------------+\n|         true|\n+-------------+\n>>> df = spark.createDataFrame([[True], [False], [True]], [\"flag\"])\n>>> df.select(bool_or(\"flag\")).show()\n+-------------+\n|bool_or(flag)|\n+-------------+\n|         true|\n+-------------+\n>>> df = spark.createDataFrame([[False], [False], [False]], [\"flag\"])\n>>> df.select(bool_or(\"flag\")).show()\n+-------------+\n|bool_or(flag)|\n+-------------+\n|        false|\n+-------------+"], "Parameters": [["col Column or column name", "column to check if at least one value is true."]], "Returns": [["Column", "true if at least one value of col is true, false otherwise."]], "Category": ["Functions"], "index": 404}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_list.html#pyspark.sql.functions.collect_list"], "Title": ["collect_list"], "Feature": ["collect_list"], "Description": "Aggregate function: Collects the values from a column into a list,\nmaintaining duplicates, and returns this list of objects.\nSee alsopyspark.sql.functions.array_agg()pyspark.sql.functions.collect_set()\nNotes\nThe function is non-deterministic as the order of collected results depends\non the order of the rows, which possibly becomes non-deterministic after shuffle operations.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1,), (2,), (2,)], ('value',))\n>>> df.select(sf.sort_array(sf.collect_list('value')).alias('sorted_list')).show()\n+-----------+\n|sorted_list|\n+-----------+\n|  [1, 2, 2]|\n+-----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n>>> df.select(sf.sort_array(sf.collect_list('age'), asc=False).alias('sorted_list')).show()\n+-----------+\n|sorted_list|\n+-----------+\n|  [5, 5, 2]|\n+-----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, \"John\"), (2, \"John\"), (3, \"Ana\")], (\"id\", \"name\"))\n>>> df = df.groupBy(\"name\").agg(sf.sort_array(sf.collect_list('id')).alias('sorted_list'))\n>>> df.orderBy(sf.desc(\"name\")).show()\n+----+-----------+\n|name|sorted_list|\n+----+-----------+\n|John|     [1, 2]|\n| Ana|        [3]|\n+----+-----------+"], "Parameters": [["col Column or column name", "The target column on which the function is computed."]], "Returns": [["Column", "A new Column object representing a list of collected values, with duplicate values included."]], "Category": ["Functions"], "index": 405}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_set.html#pyspark.sql.functions.collect_set"], "Title": ["collect_set"], "Feature": ["collect_set"], "Description": "Aggregate function: Collects the values from a column into a set,\neliminating duplicates, and returns this set of objects.\nSee alsopyspark.sql.functions.array_agg()pyspark.sql.functions.collect_list()\nNotes\nThis function is non-deterministic as the order of collected results depends\non the order of the rows, which may be non-deterministic after any shuffle operations.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1,), (2,), (2,)], ('value',))\n>>> df.select(sf.sort_array(sf.collect_set('value')).alias('sorted_set')).show()\n+----------+\n|sorted_set|\n+----------+\n|    [1, 2]|\n+----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n>>> df.select(sf.sort_array(sf.collect_set('age'), asc=False).alias('sorted_set')).show()\n+----------+\n|sorted_set|\n+----------+\n|    [5, 2]|\n+----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, \"John\"), (2, \"John\"), (3, \"Ana\")], (\"id\", \"name\"))\n>>> df = df.groupBy(\"name\").agg(sf.sort_array(sf.collect_set('id')).alias('sorted_set'))\n>>> df.orderBy(sf.desc(\"name\")).show()\n+----+----------+\n|name|sorted_set|\n+----+----------+\n|John|    [1, 2]|\n| Ana|       [3]|\n+----+----------+"], "Parameters": [["col Column or column name", "The target column on which the function is computed."]], "Returns": [["Column", "A new Column object representing a set of collected values, duplicates excluded."]], "Category": ["Functions"], "index": 406}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.corr.html#pyspark.sql.functions.corr"], "Title": ["corr"], "Feature": ["corr"], "Description": "Returns a newColumnfor the Pearson Correlation Coefficient forcol1andcol2.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> a = range(20)\n>>> b = [2 * x for x in range(20)]\n>>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n>>> df.agg(sf.corr(\"a\", df.b)).show()\n+----------+\n|corr(a, b)|\n+----------+\n|       1.0|\n+----------+"], "Parameters": [["col1 Column or column name", "first column to calculate correlation."], ["col2 Column or column name", "second column to calculate correlation."]], "Returns": [["Column", "Pearson Correlation Coefficient of these two column values."]], "Category": ["Functions"], "index": 407}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.count.html#pyspark.sql.functions.count"], "Title": ["count"], "Feature": ["count"], "Description": "Aggregate function: returns the number of items in a group.\nSee alsopyspark.sql.functions.count_if()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(None,), (\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n>>> df.select(sf.count(sf.expr(\"*\"))).show()\n+--------+\n|count(1)|\n+--------+\n|       4|\n+--------+", ">>> from pyspark.sql import functions as sf\n>>> df.select(sf.count(df.alphabets)).show()\n+----------------+\n|count(alphabets)|\n+----------------+\n|               3|\n+----------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...     [(1, \"apple\"), (2, \"banana\"), (3, None)], schema=[\"id\", \"fruit\"])\n>>> df.select(sf.count(sf.expr(\"*\"))).show()\n+--------+\n|count(1)|\n+--------+\n|       3|\n+--------+", ">>> from pyspark.sql import functions as sf\n>>> df.select(sf.count(df.id), sf.count(df.fruit)).show()\n+---------+------------+\n|count(id)|count(fruit)|\n+---------+------------+\n|        3|           2|\n+---------+------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "column for computed results."]], "Category": ["Functions"], "index": 408}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.count_distinct.html#pyspark.sql.functions.count_distinct"], "Title": ["count_distinct"], "Feature": ["count_distinct"], "Description": "Returns a newColumnfor distinct count ofcolorcols.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1,), (1,), (3,)], [\"value\"])\n>>> df.select(sf.count_distinct(df.value)).show()\n+---------------------+\n|count(DISTINCT value)|\n+---------------------+\n|                    2|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, 1), (1, 2)], [\"value1\", \"value2\"])\n>>> df.select(sf.count_distinct(df.value1, df.value2)).show()\n+------------------------------+\n|count(DISTINCT value1, value2)|\n+------------------------------+\n|                             2|\n+------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, 1), (1, 2)], [\"value1\", \"value2\"])\n>>> df.select(sf.count_distinct(\"value1\", \"value2\")).show()\n+------------------------------+\n|count(DISTINCT value1, value2)|\n+------------------------------+\n|                             2|\n+------------------------------+"], "Parameters": [["col Column or column name", "first column to compute on."], ["cols Column or column name", "other columns to compute on."]], "Returns": [["Column", "distinct values of these two column values."]], "Category": ["Functions"], "index": 409}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.count_if.html#pyspark.sql.functions.count_if"], "Title": ["count_if"], "Feature": ["count_if"], "Description": "Aggregate function: Returns the number ofTRUEvalues for thecol.\nSee alsopyspark.sql.functions.count()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"c1\", \"c2\"])\n>>> df.select(sf.count_if(sf.col('c2') % 2 == 0)).show()\n+------------------------+\n|count_if(((c2 % 2) = 0))|\n+------------------------+\n|                       3|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(\"apple\",), (\"banana\",), (\"cherry\",), (\"apple\",), (\"banana\",)], [\"fruit\"])\n>>> df.select(sf.count_if(sf.col('fruit').startswith('a'))).show()\n+------------------------------+\n|count_if(startswith(fruit, a))|\n+------------------------------+\n|                             2|\n+------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1,), (2,), (3,), (4,), (5,)], [\"num\"])\n>>> df.select(sf.count_if(sf.col('num') > 3)).show()\n+-------------------+\n|count_if((num > 3))|\n+-------------------+\n|                  2|\n+-------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(True,), (False,), (True,), (False,), (True,)], [\"b\"])\n>>> df.select(sf.count('b'), sf.count_if('b')).show()\n+--------+-----------+\n|count(b)|count_if(b)|\n+--------+-----------+\n|       5|          3|\n+--------+-----------+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "the number of TRUE values for the col ."]], "Category": ["Functions"], "index": 410}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.count_min_sketch.html#pyspark.sql.functions.count_min_sketch"], "Title": ["count_min_sketch"], "Feature": ["count_min_sketch"], "Description": "Returns a count-min sketch of a column with the given esp, confidence and seed.\nThe result is an array of bytes, which can be deserialized to aCountMinSketchbefore usage.\nCount-min sketch is a probabilistic data structure used for cardinality estimation\nusing sub-linear space.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(100).select(\n...     sf.hex(sf.count_min_sketch(sf.col(\"id\"), sf.lit(3.0), sf.lit(0.1), sf.lit(1)))\n... ).show(truncate=False)\n+------------------------------------------------------------------------+\n|hex(count_min_sketch(id, 3.0, 0.1, 1))                                  |\n+------------------------------------------------------------------------+\n|0000000100000000000000640000000100000001000000005D8D6AB90000000000000064|\n+------------------------------------------------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.range(100).select(\n...     sf.hex(sf.count_min_sketch(\"id\", 1.0, 0.3, 2))\n... ).show(truncate=False)\n+----------------------------------------------------------------------------------------+\n|hex(count_min_sketch(id, 1.0, 0.3, 2))                                                  |\n+----------------------------------------------------------------------------------------+\n|0000000100000000000000640000000100000002000000005D96391C00000000000000320000000000000032|\n+----------------------------------------------------------------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.range(100).select(\n...     sf.hex(sf.count_min_sketch(\"id\", sf.lit(1.5), 0.2, 1111111111111111111))\n... ).show(truncate=False)\n+----------------------------------------------------------------------------------------+\n|hex(count_min_sketch(id, 1.5, 0.2, 1111111111111111111))                                |\n+----------------------------------------------------------------------------------------+\n|00000001000000000000006400000001000000020000000044078BA100000000000000320000000000000032|\n+----------------------------------------------------------------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.range(100).select(\n...     sf.hex(sf.count_min_sketch(\"id\", sf.lit(1.5), 0.6))\n... ).show(truncate=False) \n+----------------------------------------------------------------------------------------------------------------------------------------+\n|hex(count_min_sketch(id, 1.5, 0.6, 2120704260))                                                                                         |\n+----------------------------------------------------------------------------------------------------------------------------------------+\n|0000000100000000000000640000000200000002000000005ADECCEE00000000153EBE090000000000000033000000000000003100000000000000320000000000000032|\n+----------------------------------------------------------------------------------------------------------------------------------------+"], "Parameters": [["col Column or column name", "target column to compute on."], ["eps Column or float", "relative error, must be positive Changed in version 4.0.0: eps now accepts float value."], ["confidence Column or float", "confidence, must be positive and less than 1.0 Changed in version 4.0.0: confidence now accepts float value."], ["seed Column or int, optional", "random seed Changed in version 4.0.0: seed now accepts int value."]], "Returns": [["Column", "count-min sketch of the column"]], "Category": ["Functions"], "index": 411}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.localCheckpoint.html#pyspark.sql.DataFrame.localCheckpoint"], "Title": ["DataFrame.localCheckpoint"], "Feature": ["DataFrame.localCheckpoint"], "Description": "Returns a locally checkpointed version of thisDataFrame. Checkpointing can\nbe used to truncate the logical plan of thisDataFrame, which is especially\nuseful in iterative algorithms where the plan may grow exponentially. Local checkpoints\nare stored in the executors using the caching subsystem and therefore they are not\nreliable.\nAdded storageLevel parameter.\nNotes\nThis API is experimental.", "Examples": [">>> df = spark.createDataFrame([\n...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.localCheckpoint(False)\nDataFrame[age: bigint, name: string]"], "Parameters": [["eager bool, optional, default True", "Whether to checkpoint this DataFrame immediately."], ["storageLevel StorageLevel , optional, default None", "The StorageLevel with which the checkpoint will be stored.\nIf not specified, default for RDD local checkpoints."]], "Returns": [["DataFrame", "Checkpointed DataFrame."]], "Category": ["DataFrame"], "index": 412}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.covar_pop.html#pyspark.sql.functions.covar_pop"], "Title": ["covar_pop"], "Feature": ["covar_pop"], "Description": "Returns a newColumnfor the population covariance ofcol1andcol2.\nSee alsopyspark.sql.functions.covar_samp()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> a = [1] * 10\n>>> b = [1] * 10\n>>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n>>> df.agg(sf.covar_pop(\"a\", df.b)).show()\n+---------------+\n|covar_pop(a, b)|\n+---------------+\n|            0.0|\n+---------------+"], "Parameters": [["col1 Column or column name", "first column to calculate covariance."], ["col2 Column or column name", "second column to calculate covariance."]], "Returns": [["Column", "covariance of these two column values."]], "Category": ["Functions"], "index": 413}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.covar_samp.html#pyspark.sql.functions.covar_samp"], "Title": ["covar_samp"], "Feature": ["covar_samp"], "Description": "Returns a newColumnfor the sample covariance ofcol1andcol2.\nSee alsopyspark.sql.functions.covar_pop()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> a = [1] * 10\n>>> b = [1] * 10\n>>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n>>> df.agg(sf.covar_samp(\"a\", df.b)).show()\n+----------------+\n|covar_samp(a, b)|\n+----------------+\n|             0.0|\n+----------------+"], "Parameters": [["col1 Column or column name", "first column to calculate covariance."], ["col2 Column or column name", "second column to calculate covariance."]], "Returns": [["Column", "sample covariance of these two column values."]], "Category": ["Functions"], "index": 414}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.every.html#pyspark.sql.functions.every"], "Title": ["every"], "Feature": ["every"], "Description": "Aggregate function: returns true if all values ofcolare true.\nSee alsopyspark.sql.functions.some()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [[True], [True], [True]], [\"flag\"]\n... ).select(sf.every(\"flag\")).show()\n+-----------+\n|every(flag)|\n+-----------+\n|       true|\n+-----------+", ">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [[True], [False], [True]], [\"flag\"]\n... ).select(sf.every(\"flag\")).show()\n+-----------+\n|every(flag)|\n+-----------+\n|      false|\n+-----------+", ">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [[False], [False], [False]], [\"flag\"]\n... ).select(sf.every(\"flag\")).show()\n+-----------+\n|every(flag)|\n+-----------+\n|      false|\n+-----------+"], "Parameters": [["col Column or column name", "column to check if all values are true."]], "Returns": [["Column", "true if all values of col are true, false otherwise."]], "Category": ["Functions"], "index": 415}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.first.html#pyspark.sql.functions.first"], "Title": ["first"], "Feature": ["first"], "Description": "Aggregate function: returns the first value in a group.\nThe function by default returns the first values it sees. It will return the first non-null\nvalue it sees when ignoreNulls is set to true. If all values are null, then null is returned.\nNotes\nThe function is non-deterministic because its results depends on the order of the\nrows which may be non-deterministic after a shuffle.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5), (\"Alice\", None)], (\"name\", \"age\"))\n>>> df = df.orderBy(df.age)\n>>> df.groupby(\"name\").agg(sf.first(\"age\")).orderBy(\"name\").show()\n+-----+----------+\n| name|first(age)|\n+-----+----------+\n|Alice|      NULL|\n|  Bob|         5|\n+-----+----------+", ">>> df.groupby(\"name\").agg(sf.first(\"age\", ignorenulls=True)).orderBy(\"name\").show()\n+-----+----------+\n| name|first(age)|\n+-----+----------+\n|Alice|         2|\n|  Bob|         5|\n+-----+----------+"], "Parameters": [["col Column or column name", "column to fetch first value for."], ["ignorenulls bool", "if first value is null then look for first non-null value. False` by default."]], "Returns": [["Column", "first value of the group."]], "Category": ["Functions"], "index": 416}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.first_value.html#pyspark.sql.functions.first_value"], "Title": ["first_value"], "Feature": ["first_value"], "Description": "Returns the first value ofcolfor a group of rows. It will return the first non-null\nvalue it sees whenignoreNullsis set to true. If all values are null, then null is returned.\nSee alsopyspark.sql.functions.last_value()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(None, 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"a\", \"b\"]\n... ).select(sf.first_value('a'), sf.first_value('b')).show()\n+--------------+--------------+\n|first_value(a)|first_value(b)|\n+--------------+--------------+\n|          NULL|             1|\n+--------------+--------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(None, 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"a\", \"b\"]\n... ).select(sf.first_value('a', True), sf.first_value('b', True)).show()\n+--------------+--------------+\n|first_value(a)|first_value(b)|\n+--------------+--------------+\n|             a|             1|\n+--------------+--------------+"], "Parameters": [["col Column or column name", "target column to work on."], ["ignoreNulls Column or bool, optional", "if first value is null then look for first non-null value."]], "Returns": [["Column", "some value of col for a group of rows."]], "Category": ["Functions"], "index": 417}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.grouping.html#pyspark.sql.functions.grouping"], "Title": ["grouping"], "Feature": ["grouping"], "Description": "Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated\nor not, returns 1 for aggregated or 0 for not aggregated in the result set.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n>>> df.cube(\"name\").agg(sf.grouping(\"name\"), sf.sum(\"age\")).orderBy(\"name\").show()\n+-----+--------------+--------+\n| name|grouping(name)|sum(age)|\n+-----+--------------+--------+\n| NULL|             1|       7|\n|Alice|             0|       2|\n|  Bob|             0|       5|\n+-----+--------------+--------+"], "Parameters": [["col Column or column name", "column to check if it’s aggregated."]], "Returns": [["Column", "returns 1 for aggregated or 0 for not aggregated in the result set."]], "Category": ["Functions"], "index": 418}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.grouping_id.html#pyspark.sql.functions.grouping_id"], "Title": ["grouping_id"], "Feature": ["grouping_id"], "Description": "Aggregate function: returns the level of grouping, equals to\nNotes\nThe list of columns should match with grouping columns exactly, or empty (means all\nthe grouping columns).", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...     [(1, \"a\", \"a\"), (3, \"a\", \"a\"), (4, \"b\", \"c\")], [\"c1\", \"c2\", \"c3\"])\n>>> df.cube(\"c2\", \"c3\").agg(sf.grouping_id(), sf.sum(\"c1\")).orderBy(\"c2\", \"c3\").show()\n+----+----+-------------+-------+\n|  c2|  c3|grouping_id()|sum(c1)|\n+----+----+-------------+-------+\n|NULL|NULL|            3|      8|\n|NULL|   a|            2|      4|\n|NULL|   c|            2|      4|\n|   a|NULL|            1|      4|\n|   a|   a|            0|      4|\n|   b|NULL|            1|      4|\n|   b|   c|            0|      4|\n+----+----+-------------+-------+"], "Parameters": [["cols Column or column name", "columns to check for."]], "Returns": [["Column", "returns level of the grouping it relates to."]], "Category": ["Functions"], "index": 419}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.histogram_numeric.html#pyspark.sql.functions.histogram_numeric"], "Title": ["histogram_numeric"], "Feature": ["histogram_numeric"], "Description": "Computes a histogram on numeric ‘col’ using nb bins.\nThe return value is an array of (x,y) pairs representing the centers of the\nhistogram’s bins. As the value of ‘nb’ is increased, the histogram approximation\ngets finer-grained, but may yield artifacts around outliers. In practice, 20-40\nhistogram bins appear to work well, with more bins being required for skewed or\nsmaller datasets. Note that this function creates a histogram with non-uniform\nbin widths. It offers no guarantees in terms of the mean-squared-error of the\nhistogram, but in practice is comparable to the histograms produced by the R/S-Plus\nstatistical computing packages. Note: the output type of the ‘x’ field in the return value is\npropagated from the input value consumed in the aggregate function.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.range(100, numPartitions=1)\n>>> df.select(sf.histogram_numeric('id', sf.lit(5))).show(truncate=False)\n+-----------------------------------------------------------+\n|histogram_numeric(id, 5)                                   |\n+-----------------------------------------------------------+\n|[{11, 25.0}, {36, 24.0}, {59, 23.0}, {84, 25.0}, {98, 3.0}]|\n+-----------------------------------------------------------+"], "Parameters": [["col Column or column name", "target column to work on."], ["nBins Column", "number of Histogram columns."]], "Returns": [["Column", "a histogram on numeric ‘col’ using nb bins."]], "Category": ["Functions"], "index": 420}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.hll_sketch_agg.html#pyspark.sql.functions.hll_sketch_agg"], "Title": ["hll_sketch_agg"], "Feature": ["hll_sketch_agg"], "Description": "Aggregate function: returns the updatable binary representation of the Datasketches\nHllSketch configured with lgConfigK arg.\nSee alsopyspark.sql.functions.hll_union()pyspark.sql.functions.hll_union_agg()pyspark.sql.functions.hll_sketch_estimate()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([1,2,2,3], \"INT\")\n>>> df.agg(sf.hll_sketch_estimate(sf.hll_sketch_agg(\"value\"))).show()\n+----------------------------------------------+\n|hll_sketch_estimate(hll_sketch_agg(value, 12))|\n+----------------------------------------------+\n|                                             3|\n+----------------------------------------------+", ">>> df.agg(sf.hll_sketch_estimate(sf.hll_sketch_agg(\"value\", 12))).show()\n+----------------------------------------------+\n|hll_sketch_estimate(hll_sketch_agg(value, 12))|\n+----------------------------------------------+\n|                                             3|\n+----------------------------------------------+"], "Parameters": [["col Column or column name", ""], ["lgConfigK Column or int, optional", "The log-base-2 of K, where K is the number of buckets or slots for the HllSketch"]], "Returns": [["Column", "The binary representation of the HllSketch."]], "Category": ["Functions"], "index": 421}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.hll_union_agg.html#pyspark.sql.functions.hll_union_agg"], "Title": ["hll_union_agg"], "Feature": ["hll_union_agg"], "Description": "Aggregate function: returns the updatable binary representation of the Datasketches\nHllSketch, generated by merging previously created Datasketches HllSketch instances\nvia a Datasketches Union instance. Throws an exception if sketches have different\nlgConfigK values and allowDifferentLgConfigK is unset or set to false.\nSee alsopyspark.sql.functions.hll_union()pyspark.sql.functions.hll_sketch_agg()pyspark.sql.functions.hll_sketch_estimate()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df1 = spark.createDataFrame([1,2,2,3], \"INT\")\n>>> df1 = df1.agg(sf.hll_sketch_agg(\"value\").alias(\"sketch\"))\n>>> df2 = spark.createDataFrame([4,5,5,6], \"INT\")\n>>> df2 = df2.agg(sf.hll_sketch_agg(\"value\").alias(\"sketch\"))\n>>> df3 = df1.union(df2)\n>>> df3.agg(sf.hll_sketch_estimate(sf.hll_union_agg(\"sketch\"))).show()\n+-------------------------------------------------+\n|hll_sketch_estimate(hll_union_agg(sketch, false))|\n+-------------------------------------------------+\n|                                                6|\n+-------------------------------------------------+", ">>> df3.agg(sf.hll_sketch_estimate(sf.hll_union_agg(\"sketch\", False))).show()\n+-------------------------------------------------+\n|hll_sketch_estimate(hll_union_agg(sketch, false))|\n+-------------------------------------------------+\n|                                                6|\n+-------------------------------------------------+"], "Parameters": [["col Column or column name", ""], ["allowDifferentLgConfigK Column or bool, optional", "Allow sketches with different lgConfigK values to be merged (defaults to false)."]], "Returns": [["Column", "The binary representation of the merged HllSketch."]], "Category": ["Functions"], "index": 422}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.mapInPandas.html#pyspark.sql.DataFrame.mapInPandas"], "Title": ["DataFrame.mapInPandas"], "Feature": ["DataFrame.mapInPandas"], "Description": "Maps an iterator of batches in the currentDataFrameusing a Python native\nfunction that is performed on pandas DataFrames both as input and output,\nand returns the result as aDataFrame.\nThis method applies the specified Python function to an iterator ofpandas.DataFrames, each representing a batch of rows from the original DataFrame.\nThe returned iterator ofpandas.DataFrames are combined as aDataFrame.\nThe size of the function’s input and output can be different. Eachpandas.DataFramesize can be controlled byspark.sql.execution.arrow.maxRecordsPerBatch.\nSee alsopyspark.sql.functions.pandas_udfDataFrame.mapInArrow", "Examples": [">>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))", ">>> def filter_func(iterator):\n...     for pdf in iterator:\n...         yield pdf[pdf.id == 1]\n...\n>>> df.mapInPandas(filter_func, df.schema).show()  \n+---+---+\n| id|age|\n+---+---+\n|  1| 21|\n+---+---+", ">>> def mean_age(iterator):\n...     for pdf in iterator:\n...         yield pdf.groupby(\"id\").mean().reset_index()\n...\n>>> df.mapInPandas(mean_age, \"id: bigint, age: double\").show()  \n+---+----+\n| id| age|\n+---+----+\n|  1|21.0|\n|  2|30.0|\n+---+----+", ">>> def double_age(iterator):\n...     for pdf in iterator:\n...         pdf[\"double_age\"] = pdf[\"age\"] * 2\n...         yield pdf\n...\n>>> df.mapInPandas(\n...     double_age, \"id: bigint, age: bigint, double_age: bigint\").show()  \n+---+---+----------+\n| id|age|double_age|\n+---+---+----------+\n|  1| 21|        42|\n|  2| 30|        60|\n+---+---+----------+", ">>> df.mapInPandas(filter_func, df.schema, barrier=True).show()  \n+---+---+\n| id|age|\n+---+---+\n|  1| 21|\n+---+---+"], "Parameters": [["func function", "a Python native function that takes an iterator of pandas.DataFrame s, and\noutputs an iterator of pandas.DataFrame s."], ["schema pyspark.sql.types.DataType or str", "the return type of the func in PySpark. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string."], ["barrier bool, optional, default False", "Use barrier mode execution, ensuring that all Python workers in the stage will be\nlaunched concurrently."], ["profile pyspark.resource.ResourceProfile . The optional ResourceProfile", "to be used for mapInPandas."]], "Returns": [], "Category": ["DataFrame"], "index": 423}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.kurtosis.html#pyspark.sql.functions.kurtosis"], "Title": ["kurtosis"], "Feature": ["kurtosis"], "Description": "Aggregate function: returns the kurtosis of the values in a group.\nSee alsopyspark.sql.functions.std()pyspark.sql.functions.stddev()pyspark.sql.functions.variance()pyspark.sql.functions.skewness()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n>>> df.select(sf.kurtosis(df.c)).show()\n+-----------+\n|kurtosis(c)|\n+-----------+\n|       -1.5|\n+-----------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "kurtosis of given column."]], "Category": ["Functions"], "index": 424}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.last.html#pyspark.sql.functions.last"], "Title": ["last"], "Feature": ["last"], "Description": "Aggregate function: returns the last value in a group.\nThe function by default returns the last values it sees. It will return the last non-null\nvalue it sees when ignoreNulls is set to true. If all values are null, then null is returned.\nNotes\nThe function is non-deterministic because its results depends on the order of the\nrows which may be non-deterministic after a shuffle.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5), (\"Alice\", None)], (\"name\", \"age\"))\n>>> df = df.orderBy(df.age.desc())\n>>> df.groupby(\"name\").agg(sf.last(\"age\")).orderBy(\"name\").show()\n+-----+---------+\n| name|last(age)|\n+-----+---------+\n|Alice|     NULL|\n|  Bob|        5|\n+-----+---------+", ">>> df.groupby(\"name\").agg(sf.last(\"age\", ignorenulls=True)).orderBy(\"name\").show()\n+-----+---------+\n| name|last(age)|\n+-----+---------+\n|Alice|        2|\n|  Bob|        5|\n+-----+---------+"], "Parameters": [["col Column or column name", "column to fetch last value for."], ["ignorenulls bool", "if last value is null then look for non-null value. False` by default."]], "Returns": [["Column", "last value of the group."]], "Category": ["Functions"], "index": 425}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.last_value.html#pyspark.sql.functions.last_value"], "Title": ["last_value"], "Feature": ["last_value"], "Description": "Returns the last value ofcolfor a group of rows. It will return the last non-null\nvalue it sees whenignoreNullsis set to true. If all values are null, then null is returned.\nSee alsopyspark.sql.functions.first_value()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (None, 2)], [\"a\", \"b\"]\n... ).select(sf.last_value('a'), sf.last_value('b')).show()\n+-------------+-------------+\n|last_value(a)|last_value(b)|\n+-------------+-------------+\n|         NULL|            2|\n+-------------+-------------+", ">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (None, 2)], [\"a\", \"b\"]\n... ).select(sf.last_value('a', True), sf.last_value('b', True)).show()\n+-------------+-------------+\n|last_value(a)|last_value(b)|\n+-------------+-------------+\n|            b|            2|\n+-------------+-------------+"], "Parameters": [["col Column or column name", "target column to work on."], ["ignoreNulls Column or bool, optional", "if first value is null then look for first non-null value."]], "Returns": [["Column", "some value of col for a group of rows."]], "Category": ["Functions"], "index": 426}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.listagg.html#pyspark.sql.functions.listagg"], "Title": ["listagg"], "Feature": ["listagg"], "Description": "Aggregate function: returns the concatenation of non-null input values,\nseparated by the delimiter.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('a',), ('b',), (None,), ('c',)], ['strings'])\n>>> df.select(sf.listagg('strings')).show()\n+----------------------+\n|listagg(strings, NULL)|\n+----------------------+\n|                   abc|\n+----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('a',), ('b',), (None,), ('c',)], ['strings'])\n>>> df.select(sf.listagg('strings', ', ')).show()\n+--------------------+\n|listagg(strings, , )|\n+--------------------+\n|             a, b, c|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(b'\u0001',), (b'\u0002',), (None,), (b'\u0003',)], ['bytes'])\n>>> df.select(sf.listagg('bytes', b'B')).show()\n+---------------------+\n|listagg(bytes, X'42')|\n+---------------------+\n|     [01 42 02 42 03]|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import StructType, StructField, StringType\n>>> schema = StructType([StructField(\"strings\", StringType(), True)])\n>>> df = spark.createDataFrame([(None,), (None,), (None,), (None,)], schema=schema)\n>>> df.select(sf.listagg('strings')).show()\n+----------------------+\n|listagg(strings, NULL)|\n+----------------------+\n|                  NULL|\n+----------------------+"], "Parameters": [["col Column or column name", "target column to compute on."], ["delimiter Column , literal string or bytes, optional", "the delimiter to separate the values. The default value is None."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 427}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.listagg_distinct.html#pyspark.sql.functions.listagg_distinct"], "Title": ["listagg_distinct"], "Feature": ["listagg_distinct"], "Description": "Aggregate function: returns the concatenation of distinct non-null input values,\nseparated by the delimiter.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('a',), ('b',), (None,), ('c',), ('b',)], ['strings'])\n>>> df.select(sf.listagg_distinct('strings')).show()\n+-------------------------------+\n|listagg(DISTINCT strings, NULL)|\n+-------------------------------+\n|                            abc|\n+-------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('a',), ('b',), (None,), ('c',), ('b',)], ['strings'])\n>>> df.select(sf.listagg_distinct('strings', ', ')).show()\n+-----------------------------+\n|listagg(DISTINCT strings, , )|\n+-----------------------------+\n|                      a, b, c|\n+-----------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(b'\u0001',), (b'\u0002',), (None,), (b'\u0003',), (b'\u0002',)],\n...                            ['bytes'])\n>>> df.select(sf.listagg_distinct('bytes', b'B')).show()\n+------------------------------+\n|listagg(DISTINCT bytes, X'42')|\n+------------------------------+\n|              [01 42 02 42 03]|\n+------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import StructType, StructField, StringType\n>>> schema = StructType([StructField(\"strings\", StringType(), True)])\n>>> df = spark.createDataFrame([(None,), (None,), (None,), (None,)], schema=schema)\n>>> df.select(sf.listagg_distinct('strings')).show()\n+-------------------------------+\n|listagg(DISTINCT strings, NULL)|\n+-------------------------------+\n|                           NULL|\n+-------------------------------+"], "Parameters": [["col Column or column name", "target column to compute on."], ["delimiter Column , literal string or bytes, optional", "the delimiter to separate the values. The default value is None."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 428}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.max.html#pyspark.sql.functions.max"], "Title": ["max"], "Feature": ["max"], "Description": "Aggregate function: returns the maximum value of the expression in a group.\nSee alsopyspark.sql.functions.min()pyspark.sql.functions.avg()pyspark.sql.functions.sum()\nNotes\nNull values are ignored during the computation.NaN values are larger than any other numeric value.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.range(10)\n>>> df.select(sf.max(df.id)).show()\n+-------+\n|max(id)|\n+-------+\n|      9|\n+-------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\"A\",), (\"B\",), (\"C\",)], [\"value\"])\n>>> df.select(sf.max(df.value)).show()\n+----------+\n|max(value)|\n+----------+\n|         C|\n+----------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\"A\", 1), (\"A\", 2), (\"B\", 3), (\"B\", 4)], [\"key\", \"value\"])\n>>> df.groupBy(\"key\").agg(sf.max(df.value)).show()\n+---+----------+\n|key|max(value)|\n+---+----------+\n|  A|         2|\n|  B|         4|\n+---+----------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame(\n...     [(\"A\", 1, 2), (\"A\", 2, 3), (\"B\", 3, 4), (\"B\", 4, 5)], [\"key\", \"value1\", \"value2\"])\n>>> df.groupBy(\"key\").agg(sf.max(\"value1\"), sf.max(\"value2\")).show()\n+---+-----------+-----------+\n|key|max(value1)|max(value2)|\n+---+-----------+-----------+\n|  A|          2|          3|\n|  B|          4|          5|\n+---+-----------+-----------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1,), (2,), (None,)], [\"value\"])\n>>> df.select(sf.max(df.value)).show()\n+----------+\n|max(value)|\n+----------+\n|         2|\n+----------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1.1,), (float(\"nan\"),), (3.3,)], [\"value\"])\n>>> df.select(sf.max(df.value)).show()\n+----------+\n|max(value)|\n+----------+\n|       NaN|\n+----------+"], "Parameters": [["col Column or column name", "The target column on which the maximum value is computed."]], "Returns": [["Column", "A column that contains the maximum value computed."]], "Category": ["Functions"], "index": 429}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.max_by.html#pyspark.sql.functions.max_by"], "Title": ["max_by"], "Feature": ["max_by"], "Description": "Returns the value from thecolparameter that is associated with the maximum value\nfrom theordparameter. This function is often used to find thecolparameter value\ncorresponding to the maximumordparameter value within each group when used with groupBy().\nNotes\nThe function is non-deterministic so the output order can be different for those\nassociated the same values ofcol.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([\n...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n...     schema=(\"course\", \"year\", \"earnings\"))\n>>> df.groupby(\"course\").agg(sf.max_by(\"year\", \"earnings\")).sort(\"course\").show()\n+------+----------------------+\n|course|max_by(year, earnings)|\n+------+----------------------+\n|  Java|                  2013|\n|dotNET|                  2013|\n+------+----------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([\n...     (\"Marketing\", \"Anna\", 4), (\"IT\", \"Bob\", 2),\n...     (\"IT\", \"Charlie\", 3), (\"Marketing\", \"David\", 1)],\n...     schema=(\"department\", \"name\", \"years_in_dept\"))\n>>> df.groupby(\"department\").agg(\n...     sf.max_by(\"name\", \"years_in_dept\")\n... ).sort(\"department\").show()\n+----------+---------------------------+\n|department|max_by(name, years_in_dept)|\n+----------+---------------------------+\n|        IT|                    Charlie|\n| Marketing|                       Anna|\n+----------+---------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([\n...     (\"Consult\", \"Eva\", 6), (\"Finance\", \"Frank\", 5),\n...     (\"Finance\", \"George\", 9), (\"Consult\", \"Henry\", 7)],\n...     schema=(\"department\", \"name\", \"years_in_dept\"))\n>>> df.groupby(\"department\").agg(\n...     sf.max_by(\"name\", \"years_in_dept\")\n... ).sort(\"department\").show()\n+----------+---------------------------+\n|department|max_by(name, years_in_dept)|\n+----------+---------------------------+\n|   Consult|                      Henry|\n|   Finance|                     George|\n+----------+---------------------------+"], "Parameters": [["col Column or column name", "The column representing the values to be returned. This could be the column instance\nor the column name as string."], ["ord Column or column name", "The column that needs to be maximized. This could be the column instance\nor the column name as string."]], "Returns": [["Column", "A column object representing the value from col that is associated with\nthe maximum value from ord ."]], "Category": ["Functions"], "index": 430}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.mean.html#pyspark.sql.functions.mean"], "Title": ["mean"], "Feature": ["mean"], "Description": "Aggregate function: returns the average of the values in a group.\nAn alias ofavg().", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1982, 15), (1990, 2)], [\"birth\", \"age\"])\n>>> df.select(sf.mean(\"age\")).show()\n+--------+\n|avg(age)|\n+--------+\n|     8.5|\n+--------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1982, None), (1990, 2), (2000, 4)], [\"birth\", \"age\"])\n>>> df.select(sf.mean(\"age\")).show()\n+--------+\n|avg(age)|\n+--------+\n|     3.0|\n+--------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 431}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.median.html#pyspark.sql.functions.median"], "Title": ["median"], "Feature": ["median"], "Description": "Returns the median of the values in a group.\npyspark.sql.functions.percentile()pyspark.sql.functions.approx_percentile()pyspark.sql.functions.percentile_approx()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n...     (\"Java\", 2012, 22000), (\"dotNET\", 2012, 10000),\n...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n...     schema=(\"course\", \"year\", \"earnings\"))\n>>> df.groupby(\"course\").agg(sf.median(\"earnings\")).show()\n+------+----------------+\n|course|median(earnings)|\n+------+----------------+\n|  Java|         22000.0|\n|dotNET|         10000.0|\n+------+----------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the median of the values in a group."]], "Category": ["Functions"], "index": 432}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.min.html#pyspark.sql.functions.min"], "Title": ["min"], "Feature": ["min"], "Description": "Aggregate function: returns the minimum value of the expression in a group.\nSee alsopyspark.sql.functions.max()pyspark.sql.functions.avg()pyspark.sql.functions.sum()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.range(10)\n>>> df.select(sf.min(df.id)).show()\n+-------+\n|min(id)|\n+-------+\n|      0|\n+-------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\"Alice\",), (\"Bob\",), (\"Charlie\",)], [\"name\"])\n>>> df.select(sf.min(\"name\")).show()\n+---------+\n|min(name)|\n+---------+\n|    Alice|\n+---------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1,), (None,), (3,)], [\"value\"])\n>>> df.select(sf.min(\"value\")).show()\n+----------+\n|min(value)|\n+----------+\n|         1|\n+----------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\"Alice\", 1), (\"Alice\", 2), (\"Bob\", 3)], [\"name\", \"value\"])\n>>> df.groupBy(\"name\").agg(sf.min(\"value\")).show()\n+-----+----------+\n| name|min(value)|\n+-----+----------+\n|Alice|         1|\n|  Bob|         3|\n+-----+----------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame(\n...     [(\"Alice\", 1, 100), (\"Bob\", 2, 200), (\"Charlie\", 3, 300)],\n...     [\"name\", \"value1\", \"value2\"])\n>>> df.select(sf.min(\"value1\"), sf.min(\"value2\")).show()\n+-----------+-----------+\n|min(value1)|min(value2)|\n+-----------+-----------+\n|          1|        100|\n+-----------+-----------+"], "Parameters": [["col Column or column name", "The target column on which the minimum value is computed."]], "Returns": [["Column", "A column that contains the minimum value computed."]], "Category": ["Functions"], "index": 433}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.mapInArrow.html#pyspark.sql.DataFrame.mapInArrow"], "Title": ["DataFrame.mapInArrow"], "Feature": ["DataFrame.mapInArrow"], "Description": "Maps an iterator of batches in the currentDataFrameusing a Python native\nfunction that is performed onpyarrow.RecordBatchs both as input and output,\nand returns the result as aDataFrame.\nThis method applies the specified Python function to an iterator ofpyarrow.RecordBatchs, each representing a batch of rows from the original DataFrame.\nThe returned iterator ofpyarrow.RecordBatchs are combined as aDataFrame.\nThe size of the function’s input and output can be different. Eachpyarrow.RecordBatchsize can be controlled byspark.sql.execution.arrow.maxRecordsPerBatch.\nSee alsopyspark.sql.functions.pandas_udfDataFrame.mapInPandas", "Examples": [">>> import pyarrow  \n>>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n>>> def filter_func(iterator):\n...     for batch in iterator:\n...         pdf = batch.to_pandas()\n...         yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])\n>>> df.mapInArrow(filter_func, df.schema).show()  \n+---+---+\n| id|age|\n+---+---+\n|  1| 21|\n+---+---+", ">>> df.mapInArrow(filter_func, df.schema, barrier=True).show()  \n+---+---+\n| id|age|\n+---+---+\n|  1| 21|\n+---+---+"], "Parameters": [["func function", "a Python native function that takes an iterator of pyarrow.RecordBatch s, and\noutputs an iterator of pyarrow.RecordBatch s."], ["schema pyspark.sql.types.DataType or str", "the return type of the func in PySpark. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string."], ["barrier bool, optional, default False", "Use barrier mode execution, ensuring that all Python workers in the stage will be\nlaunched concurrently."], ["profile pyspark.resource.ResourceProfile . The optional ResourceProfile", "to be used for mapInArrow."]], "Returns": [], "Category": ["DataFrame"], "index": 434}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.min_by.html#pyspark.sql.functions.min_by"], "Title": ["min_by"], "Feature": ["min_by"], "Description": "Returns the value from thecolparameter that is associated with the minimum value\nfrom theordparameter. This function is often used to find thecolparameter value\ncorresponding to the minimumordparameter value within each group when used with groupBy().\nNotes\nThe function is non-deterministic so the output order can be different for those\nassociated the same values ofcol.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([\n...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n...     schema=(\"course\", \"year\", \"earnings\"))\n>>> df.groupby(\"course\").agg(sf.min_by(\"year\", \"earnings\")).sort(\"course\").show()\n+------+----------------------+\n|course|min_by(year, earnings)|\n+------+----------------------+\n|  Java|                  2012|\n|dotNET|                  2012|\n+------+----------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([\n...     (\"Marketing\", \"Anna\", 4), (\"IT\", \"Bob\", 2),\n...     (\"IT\", \"Charlie\", 3), (\"Marketing\", \"David\", 1)],\n...     schema=(\"department\", \"name\", \"years_in_dept\"))\n>>> df.groupby(\"department\").agg(\n...     sf.min_by(\"name\", \"years_in_dept\")\n... ).sort(\"department\").show()\n+----------+---------------------------+\n|department|min_by(name, years_in_dept)|\n+----------+---------------------------+\n|        IT|                        Bob|\n| Marketing|                      David|\n+----------+---------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([\n...     (\"Consult\", \"Eva\", 6), (\"Finance\", \"Frank\", 5),\n...     (\"Finance\", \"George\", 9), (\"Consult\", \"Henry\", 7)],\n...     schema=(\"department\", \"name\", \"years_in_dept\"))\n>>> df.groupby(\"department\").agg(\n...     sf.min_by(\"name\", \"years_in_dept\")\n... ).sort(\"department\").show()\n+----------+---------------------------+\n|department|min_by(name, years_in_dept)|\n+----------+---------------------------+\n|   Consult|                        Eva|\n|   Finance|                      Frank|\n+----------+---------------------------+"], "Parameters": [["col Column or column name", "The column representing the values that will be returned. This could be the column instance\nor the column name as string."], ["ord Column or column name", "The column that needs to be minimized. This could be the column instance\nor the column name as string."]], "Returns": [["Column", "Column object that represents the value from col associated with\nthe minimum value from ord ."]], "Category": ["Functions"], "index": 435}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.mode.html#pyspark.sql.functions.mode"], "Title": ["mode"], "Feature": ["mode"], "Description": "Returns the most frequent value in a group.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n...     schema=(\"course\", \"year\", \"earnings\"))\n>>> df.groupby(\"course\").agg(sf.mode(\"year\")).sort(\"course\").show()\n+------+----------+\n|course|mode(year)|\n+------+----------+\n|  Java|      2012|\n|dotNET|      2012|\n+------+----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(-10,), (0,), (10,)], [\"col\"])\n>>> df.select(sf.mode(\"col\", False)).show() \n+---------+\n|mode(col)|\n+---------+\n|        0|\n+---------+", ">>> df.select(sf.mode(\"col\", True)).show()\n+---------------------------------------+\n|mode() WITHIN GROUP (ORDER BY col DESC)|\n+---------------------------------------+\n|                                    -10|\n+---------------------------------------+"], "Parameters": [["col Column or column name", "target column to compute on."], ["deterministic bool, optional", "if there are multiple equally-frequent results then return the lowest (defaults to false)."]], "Returns": [["Column", "the most frequent value in a group."]], "Category": ["Functions"], "index": 436}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.percentile.html#pyspark.sql.functions.percentile"], "Title": ["percentile"], "Feature": ["percentile"], "Description": "Returns the exact percentile(s) of numeric columnexprat the given percentage(s)\nwith value range in [0.0, 1.0].\nSee alsopyspark.sql.functions.median()pyspark.sql.functions.approx_percentile()pyspark.sql.functions.percentile_approx()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> key = (sf.col(\"id\") % 3).alias(\"key\")\n>>> value = (sf.randn(42) + key * 10).alias(\"value\")\n>>> df = spark.range(0, 1000, 1, 1).select(key, value)\n>>> df.select(\n...     sf.percentile(\"value\", [0.25, 0.5, 0.75], sf.lit(1))\n... ).show(truncate=False)\n+--------------------------------------------------------+\n|percentile(value, array(0.25, 0.5, 0.75), 1)            |\n+--------------------------------------------------------+\n|[0.7441991494121..., 9.9900713756..., 19.33740203080...]|\n+--------------------------------------------------------+", ">>> df.groupBy(\"key\").agg(\n...     sf.percentile(\"value\", sf.lit(0.5), sf.lit(1))\n... ).sort(\"key\").show()\n+---+-------------------------+\n|key|percentile(value, 0.5, 1)|\n+---+-------------------------+\n|  0|     -0.03449962216667901|\n|  1|        9.990389751837329|\n|  2|       19.967859769284075|\n+---+-------------------------+"], "Parameters": [["col Column or column name", ""], ["percentage Column , float, list of floats or tuple of floats", "percentage in decimal (must be between 0.0 and 1.0)."], ["frequency Column or int is a positive numeric literal which", "controls frequency."]], "Returns": [["Column", "the exact percentile of the numeric column."]], "Category": ["Functions"], "index": 437}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.percentile_approx.html#pyspark.sql.functions.percentile_approx"], "Title": ["percentile_approx"], "Feature": ["percentile_approx"], "Description": "Returns the approximatepercentileof the numeric columncolwhich is the smallest value\nin the orderedcolvalues (sorted from least to greatest) such that no more thanpercentageofcolvalues is less than the value or equal to that value.\nSee alsopyspark.sql.functions.median()pyspark.sql.functions.percentile()pyspark.sql.functions.approx_percentile()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> key = (sf.col(\"id\") % 3).alias(\"key\")\n>>> value = (sf.randn(42) + key * 10).alias(\"value\")\n>>> df = spark.range(0, 1000, 1, 1).select(key, value)\n>>> df.select(\n...     sf.percentile_approx(\"value\", [0.25, 0.5, 0.75], 1000000)\n... ).show(truncate=False)\n+----------------------------------------------------------+\n|percentile_approx(value, array(0.25, 0.5, 0.75), 1000000) |\n+----------------------------------------------------------+\n|[0.7264430125286..., 9.98975299938..., 19.335304783039...]|\n+----------------------------------------------------------+", ">>> df.groupBy(\"key\").agg(\n...     sf.percentile_approx(\"value\", sf.lit(0.5), sf.lit(1000000))\n... ).sort(\"key\").show()\n+---+--------------------------------------+\n|key|percentile_approx(value, 0.5, 1000000)|\n+---+--------------------------------------+\n|  0|                  -0.03519435193070...|\n|  1|                     9.990389751837...|\n|  2|                    19.967859769284...|\n+---+--------------------------------------+"], "Parameters": [["col Column or column name", "input column."], ["percentage Column , float, list of floats or tuple of floats", "percentage in decimal (must be between 0.0 and 1.0).\nWhen percentage is an array, each value of the percentage array must be between 0.0 and 1.0.\nIn this case, returns the approximate percentile array of column col\nat the given percentage array."], ["accuracy Column or int", "is a positive numeric literal which controls approximation accuracy\nat the cost of memory. Higher value of accuracy yields better accuracy,\n1.0/accuracy is the relative error of the approximation. (default: 10000)."]], "Returns": [["Column", "approximate percentile of the numeric column."]], "Category": ["Functions"], "index": 438}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.product.html#pyspark.sql.functions.product"], "Title": ["product"], "Feature": ["product"], "Description": "Aggregate function: returns the product of the values in a group.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql(\"SELECT id % 3 AS mod3, id AS value FROM RANGE(10)\")\n>>> df.groupBy('mod3').agg(sf.product('value')).orderBy('mod3').show()\n+----+--------------+\n|mod3|product(value)|\n+----+--------------+\n|   0|           0.0|\n|   1|          28.0|\n|   2|          80.0|\n+----+--------------+"], "Parameters": [["col Column or column name", "column containing values to be multiplied together"]], "Returns": [["Column or column name", "the column for computed results."]], "Category": ["Functions"], "index": 439}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regr_avgx.html#pyspark.sql.functions.regr_avgx"], "Title": ["regr_avgx"], "Feature": ["regr_avgx"], "Description": "Aggregate function: returns the average of the independent variable for non-null pairs\nin a group, whereyis the dependent variable andxis the independent variable.\nSee alsopyspark.sql.functions.regr_avgy()pyspark.sql.functions.regr_count()pyspark.sql.functions.regr_intercept()pyspark.sql.functions.regr_r2()pyspark.sql.functions.regr_slope()pyspark.sql.functions.regr_sxy()pyspark.sql.functions.regr_syy()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_avgx(\"y\", \"x\"), sf.avg(\"x\")).show()\n+---------------+------+\n|regr_avgx(y, x)|avg(x)|\n+---------------+------+\n|           2.75|  2.75|\n+---------------+------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, null) AS tab(y, x)\")\n>>> df.select(sf.regr_avgx(\"y\", \"x\"), sf.avg(\"x\")).show()\n+---------------+------+\n|regr_avgx(y, x)|avg(x)|\n+---------------+------+\n|           NULL|  NULL|\n+---------------+------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (null, 1) AS tab(y, x)\")\n>>> df.select(sf.regr_avgx(\"y\", \"x\"), sf.avg(\"x\")).show()\n+---------------+------+\n|regr_avgx(y, x)|avg(x)|\n+---------------+------+\n|           NULL|   1.0|\n+---------------+------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_avgx(\"y\", \"x\"), sf.avg(\"x\")).show()\n+---------------+------+\n|regr_avgx(y, x)|avg(x)|\n+---------------+------+\n|            3.0|   3.0|\n+---------------+------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_avgx(\"y\", \"x\"), sf.avg(\"x\")).show()\n+---------------+------+\n|regr_avgx(y, x)|avg(x)|\n+---------------+------+\n|            3.0|   3.0|\n+---------------+------+"], "Parameters": [["y Column or column name", "the dependent variable."], ["x Column or column name", "the independent variable."]], "Returns": [["Column", "the average of the independent variable for non-null pairs in a group."]], "Category": ["Functions"], "index": 440}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regr_avgy.html#pyspark.sql.functions.regr_avgy"], "Title": ["regr_avgy"], "Feature": ["regr_avgy"], "Description": "Aggregate function: returns the average of the dependent variable for non-null pairs\nin a group, whereyis the dependent variable andxis the independent variable.\nSee alsopyspark.sql.functions.regr_avgx()pyspark.sql.functions.regr_count()pyspark.sql.functions.regr_intercept()pyspark.sql.functions.regr_r2()pyspark.sql.functions.regr_slope()pyspark.sql.functions.regr_sxy()pyspark.sql.functions.regr_syy()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_avgy(\"y\", \"x\"), sf.avg(\"y\")).show()\n+---------------+------+\n|regr_avgy(y, x)|avg(y)|\n+---------------+------+\n|           1.75|  1.75|\n+---------------+------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, null) AS tab(y, x)\")\n>>> df.select(sf.regr_avgy(\"y\", \"x\"), sf.avg(\"y\")).show()\n+---------------+------+\n|regr_avgy(y, x)|avg(y)|\n+---------------+------+\n|           NULL|   1.0|\n+---------------+------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (null, 1) AS tab(y, x)\")\n>>> df.select(sf.regr_avgy(\"y\", \"x\"), sf.avg(\"y\")).show()\n+---------------+------+\n|regr_avgy(y, x)|avg(y)|\n+---------------+------+\n|           NULL|  NULL|\n+---------------+------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_avgy(\"y\", \"x\"), sf.avg(\"y\")).show()\n+------------------+------+\n|   regr_avgy(y, x)|avg(y)|\n+------------------+------+\n|1.6666666666666...|  1.75|\n+------------------+------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_avgy(\"y\", \"x\"), sf.avg(\"y\")).show()\n+---------------+------------------+\n|regr_avgy(y, x)|            avg(y)|\n+---------------+------------------+\n|            1.5|1.6666666666666...|\n+---------------+------------------+"], "Parameters": [["y Column or column name", "the dependent variable."], ["x Column or column name", "the independent variable."]], "Returns": [["Column", "the average of the dependent variable for non-null pairs in a group."]], "Category": ["Functions"], "index": 441}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regr_count.html#pyspark.sql.functions.regr_count"], "Title": ["regr_count"], "Feature": ["regr_count"], "Description": "Aggregate function: returns the number of non-null number pairs\nin a group, whereyis the dependent variable andxis the independent variable.\nSee alsopyspark.sql.functions.regr_avgx()pyspark.sql.functions.regr_avgy()pyspark.sql.functions.regr_intercept()pyspark.sql.functions.regr_r2()pyspark.sql.functions.regr_slope()pyspark.sql.functions.regr_sxy()pyspark.sql.functions.regr_syy()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_count(\"y\", \"x\"), sf.count(sf.lit(0))).show()\n+----------------+--------+\n|regr_count(y, x)|count(0)|\n+----------------+--------+\n|               4|       4|\n+----------------+--------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, null) AS tab(y, x)\")\n>>> df.select(sf.regr_count(\"y\", \"x\"), sf.count(sf.lit(0))).show()\n+----------------+--------+\n|regr_count(y, x)|count(0)|\n+----------------+--------+\n|               0|       1|\n+----------------+--------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (null, 1) AS tab(y, x)\")\n>>> df.select(sf.regr_count(\"y\", \"x\"), sf.count(sf.lit(0))).show()\n+----------------+--------+\n|regr_count(y, x)|count(0)|\n+----------------+--------+\n|               0|       1|\n+----------------+--------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_count(\"y\", \"x\"), sf.count(sf.lit(0))).show()\n+----------------+--------+\n|regr_count(y, x)|count(0)|\n+----------------+--------+\n|               3|       4|\n+----------------+--------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_count(\"y\", \"x\"), sf.count(sf.lit(0))).show()\n+----------------+--------+\n|regr_count(y, x)|count(0)|\n+----------------+--------+\n|               2|       4|\n+----------------+--------+"], "Parameters": [["y Column or column name", "the dependent variable."], ["x Column or column name", "the independent variable."]], "Returns": [["Column", "the number of non-null number pairs in a group."]], "Category": ["Functions"], "index": 442}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regr_intercept.html#pyspark.sql.functions.regr_intercept"], "Title": ["regr_intercept"], "Feature": ["regr_intercept"], "Description": "Aggregate function: returns the intercept of the univariate linear regression line\nfor non-null pairs in a group, whereyis the dependent variable andxis the independent variable.\nSee alsopyspark.sql.functions.regr_avgx()pyspark.sql.functions.regr_avgy()pyspark.sql.functions.regr_count()pyspark.sql.functions.regr_r2()pyspark.sql.functions.regr_slope()pyspark.sql.functions.regr_sxy()pyspark.sql.functions.regr_syy()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, 2), (3, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_intercept(\"y\", \"x\")).show()\n+--------------------+\n|regr_intercept(y, x)|\n+--------------------+\n|                 0.0|\n+--------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, null) AS tab(y, x)\")\n>>> df.select(sf.regr_intercept(\"y\", \"x\")).show()\n+--------------------+\n|regr_intercept(y, x)|\n+--------------------+\n|                NULL|\n+--------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (null, 1) AS tab(y, x)\")\n>>> df.select(sf.regr_intercept(\"y\", \"x\")).show()\n+--------------------+\n|regr_intercept(y, x)|\n+--------------------+\n|                NULL|\n+--------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, null), (3, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_intercept(\"y\", \"x\")).show()\n+--------------------+\n|regr_intercept(y, x)|\n+--------------------+\n|                 0.0|\n+--------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, null), (null, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_intercept(\"y\", \"x\")).show()\n+--------------------+\n|regr_intercept(y, x)|\n+--------------------+\n|                 0.0|\n+--------------------+"], "Parameters": [["y Column or column name", "the dependent variable."], ["x Column or column name", "the independent variable."]], "Returns": [["Column", "the intercept of the univariate linear regression line for non-null pairs in a group."]], "Category": ["Functions"], "index": 443}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regr_r2.html#pyspark.sql.functions.regr_r2"], "Title": ["regr_r2"], "Feature": ["regr_r2"], "Description": "Aggregate function: returns the coefficient of determination for non-null pairs\nin a group, whereyis the dependent variable andxis the independent variable.\nSee alsopyspark.sql.functions.regr_avgx()pyspark.sql.functions.regr_avgy()pyspark.sql.functions.regr_count()pyspark.sql.functions.regr_intercept()pyspark.sql.functions.regr_slope()pyspark.sql.functions.regr_sxy()pyspark.sql.functions.regr_syy()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, 2), (3, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_r2(\"y\", \"x\")).show()\n+-------------+\n|regr_r2(y, x)|\n+-------------+\n|          1.0|\n+-------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, null) AS tab(y, x)\")\n>>> df.select(sf.regr_r2(\"y\", \"x\")).show()\n+-------------+\n|regr_r2(y, x)|\n+-------------+\n|         NULL|\n+-------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (null, 1) AS tab(y, x)\")\n>>> df.select(sf.regr_r2(\"y\", \"x\")).show()\n+-------------+\n|regr_r2(y, x)|\n+-------------+\n|         NULL|\n+-------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, null), (3, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_r2(\"y\", \"x\")).show()\n+-------------+\n|regr_r2(y, x)|\n+-------------+\n|          1.0|\n+-------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, null), (null, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_r2(\"y\", \"x\")).show()\n+-------------+\n|regr_r2(y, x)|\n+-------------+\n|          1.0|\n+-------------+"], "Parameters": [["y Column or column name", "the dependent variable."], ["x Column or column name", "the independent variable."]], "Returns": [["Column", "the coefficient of determination for non-null pairs in a group."]], "Category": ["Functions"], "index": 444}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.asTable.html#pyspark.sql.DataFrame.asTable"], "Title": ["DataFrame.asTable"], "Feature": ["DataFrame.asTable"], "Description": "Converts the DataFrame into atable_arg.TableArgobject, which can\nbe used as a table argument in a TVF(Table-Valued Function) including UDTF\n(User-Defined Table Function).\nAfter obtaining a TableArg from a DataFrame using this method, you can specify partitioning\nand ordering for the table argument by calling methods such aspartitionBy,orderBy, andwithSinglePartitionon theTableArginstance.\n- partitionBy: Partitions the data based on the specified columns. This method cannot\nbe called after withSinglePartition() has been called.\n- orderBy: Orders the data within partitions based on the specified columns.\n- withSinglePartition: Indicates that the data should be treated as a single partition.\nThis method cannot be called after partitionBy() has been called.", "Examples": [], "Parameters": [], "Returns": [["table_arg.TableArg", "A TableArg object representing a table argument."]], "Category": ["DataFrame"], "index": 445}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.metadataColumn.html#pyspark.sql.DataFrame.metadataColumn"], "Title": ["DataFrame.metadataColumn"], "Feature": ["DataFrame.metadataColumn"], "Description": "Selects a metadata column based on its logical column name and returns it as aColumn.\nA metadata column can be accessed this way even if the underlying data source defines a data\ncolumn with a conflicting name.", "Examples": [], "Parameters": [["colName str", "string, metadata column name"]], "Returns": [["Column", ""]], "Category": ["DataFrame"], "index": 446}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regr_slope.html#pyspark.sql.functions.regr_slope"], "Title": ["regr_slope"], "Feature": ["regr_slope"], "Description": "Aggregate function: returns the slope of the linear regression line for non-null pairs\nin a group, whereyis the dependent variable andxis the independent variable.\nSee alsopyspark.sql.functions.regr_avgx()pyspark.sql.functions.regr_avgy()pyspark.sql.functions.regr_count()pyspark.sql.functions.regr_intercept()pyspark.sql.functions.regr_r2()pyspark.sql.functions.regr_sxy()pyspark.sql.functions.regr_syy()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, 2), (3, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_slope(\"y\", \"x\")).show()\n+----------------+\n|regr_slope(y, x)|\n+----------------+\n|             1.0|\n+----------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, null) AS tab(y, x)\")\n>>> df.select(sf.regr_slope(\"y\", \"x\")).show()\n+----------------+\n|regr_slope(y, x)|\n+----------------+\n|            NULL|\n+----------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (null, 1) AS tab(y, x)\")\n>>> df.select(sf.regr_slope(\"y\", \"x\")).show()\n+----------------+\n|regr_slope(y, x)|\n+----------------+\n|            NULL|\n+----------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, null), (3, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_slope(\"y\", \"x\")).show()\n+----------------+\n|regr_slope(y, x)|\n+----------------+\n|             1.0|\n+----------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, null), (null, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_slope(\"y\", \"x\")).show()\n+----------------+\n|regr_slope(y, x)|\n+----------------+\n|             1.0|\n+----------------+"], "Parameters": [["y Column or column name", "the dependent variable."], ["x Column or column name", "the independent variable."]], "Returns": [["Column", "the slope of the linear regression line for non-null pairs in a group."]], "Category": ["Functions"], "index": 447}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regr_sxx.html#pyspark.sql.functions.regr_sxx"], "Title": ["regr_sxx"], "Feature": ["regr_sxx"], "Description": "Aggregate function: returns REGR_COUNT(y, x) * VAR_POP(x) for non-null pairs\nin a group, whereyis the dependent variable andxis the independent variable.\nSee alsopyspark.sql.functions.regr_avgx()pyspark.sql.functions.regr_avgy()pyspark.sql.functions.regr_count()pyspark.sql.functions.regr_intercept()pyspark.sql.functions.regr_r2()pyspark.sql.functions.regr_sxy()pyspark.sql.functions.regr_syy()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, 2), (3, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_sxx(\"y\", \"x\")).show()\n+--------------+\n|regr_sxx(y, x)|\n+--------------+\n|           5.0|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, null) AS tab(y, x)\")\n>>> df.select(sf.regr_sxx(\"y\", \"x\")).show()\n+--------------+\n|regr_sxx(y, x)|\n+--------------+\n|          NULL|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (null, 1) AS tab(y, x)\")\n>>> df.select(sf.regr_sxx(\"y\", \"x\")).show()\n+--------------+\n|regr_sxx(y, x)|\n+--------------+\n|          NULL|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, null), (3, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_sxx(\"y\", \"x\")).show()\n+-----------------+\n|   regr_sxx(y, x)|\n+-----------------+\n|4.666666666666...|\n+-----------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, null), (null, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_sxx(\"y\", \"x\")).show()\n+--------------+\n|regr_sxx(y, x)|\n+--------------+\n|           4.5|\n+--------------+"], "Parameters": [["y Column or column name", "the dependent variable."], ["x Column or column name", "the independent variable."]], "Returns": [["Column", "REGR_COUNT(y, x) * VAR_POP(x) for non-null pairs in a group."]], "Category": ["Functions"], "index": 448}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regr_sxy.html#pyspark.sql.functions.regr_sxy"], "Title": ["regr_sxy"], "Feature": ["regr_sxy"], "Description": "Aggregate function: returns REGR_COUNT(y, x) * COVAR_POP(y, x) for non-null pairs\nin a group, whereyis the dependent variable andxis the independent variable.\nSee alsopyspark.sql.functions.regr_avgx()pyspark.sql.functions.regr_avgy()pyspark.sql.functions.regr_count()pyspark.sql.functions.regr_intercept()pyspark.sql.functions.regr_r2()pyspark.sql.functions.regr_slope()pyspark.sql.functions.regr_syy()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, 2), (3, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_sxy(\"y\", \"x\")).show()\n+--------------+\n|regr_sxy(y, x)|\n+--------------+\n|           5.0|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, null) AS tab(y, x)\")\n>>> df.select(sf.regr_sxy(\"y\", \"x\")).show()\n+--------------+\n|regr_sxy(y, x)|\n+--------------+\n|          NULL|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (null, 1) AS tab(y, x)\")\n>>> df.select(sf.regr_sxy(\"y\", \"x\")).show()\n+--------------+\n|regr_sxy(y, x)|\n+--------------+\n|          NULL|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, null), (3, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_sxy(\"y\", \"x\")).show()\n+-----------------+\n|   regr_sxy(y, x)|\n+-----------------+\n|4.666666666666...|\n+-----------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, null), (null, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_sxy(\"y\", \"x\")).show()\n+--------------+\n|regr_sxy(y, x)|\n+--------------+\n|           4.5|\n+--------------+"], "Parameters": [["y Column or column name", "the dependent variable."], ["x Column or column name", "the independent variable."]], "Returns": [["Column", "REGR_COUNT(y, x) * COVAR_POP(y, x) for non-null pairs in a group."]], "Category": ["Functions"], "index": 449}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regr_syy.html#pyspark.sql.functions.regr_syy"], "Title": ["regr_syy"], "Feature": ["regr_syy"], "Description": "Aggregate function: returns REGR_COUNT(y, x) * VAR_POP(y) for non-null pairs\nin a group, whereyis the dependent variable andxis the independent variable.\nSee alsopyspark.sql.functions.regr_avgx()pyspark.sql.functions.regr_avgy()pyspark.sql.functions.regr_count()pyspark.sql.functions.regr_intercept()pyspark.sql.functions.regr_r2()pyspark.sql.functions.regr_slope()pyspark.sql.functions.regr_sxy()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, 2), (3, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_syy(\"y\", \"x\")).show()\n+--------------+\n|regr_syy(y, x)|\n+--------------+\n|           5.0|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, null) AS tab(y, x)\")\n>>> df.select(sf.regr_syy(\"y\", \"x\")).show()\n+--------------+\n|regr_syy(y, x)|\n+--------------+\n|          NULL|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (null, 1) AS tab(y, x)\")\n>>> df.select(sf.regr_syy(\"y\", \"x\")).show()\n+--------------+\n|regr_syy(y, x)|\n+--------------+\n|          NULL|\n+--------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, null), (3, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_syy(\"y\", \"x\")).show()\n+-----------------+\n|   regr_syy(y, x)|\n+-----------------+\n|4.666666666666...|\n+-----------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT * FROM VALUES (1, 1), (2, null), (null, 3), (4, 4) AS tab(y, x)\")\n>>> df.select(sf.regr_syy(\"y\", \"x\")).show()\n+--------------+\n|regr_syy(y, x)|\n+--------------+\n|           4.5|\n+--------------+"], "Parameters": [["y Column or column name", "the dependent variable."], ["x Column or column name", "the independent variable."]], "Returns": [["Column", "REGR_COUNT(y, x) * VAR_POP(y) for non-null pairs in a group."]], "Category": ["Functions"], "index": 450}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.skewness.html#pyspark.sql.functions.skewness"], "Title": ["skewness"], "Feature": ["skewness"], "Description": "Aggregate function: returns the skewness of the values in a group.\nSee alsopyspark.sql.functions.std()pyspark.sql.functions.stddev()pyspark.sql.functions.variance()pyspark.sql.functions.kurtosis()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n>>> df.select(sf.skewness(df.c)).show()\n+------------------+\n|       skewness(c)|\n+------------------+\n|0.7071067811865...|\n+------------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "skewness of given column."]], "Category": ["Functions"], "index": 451}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.some.html#pyspark.sql.functions.some"], "Title": ["some"], "Feature": ["some"], "Description": "Aggregate function: returns true if at least one value ofcolis true.\nSee alsopyspark.sql.functions.every()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [[True], [True], [True]], [\"flag\"]\n... ).select(sf.some(\"flag\")).show()\n+----------+\n|some(flag)|\n+----------+\n|      true|\n+----------+", ">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [[True], [False], [True]], [\"flag\"]\n... ).select(sf.some(\"flag\")).show()\n+----------+\n|some(flag)|\n+----------+\n|      true|\n+----------+", ">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [[False], [False], [False]], [\"flag\"]\n... ).select(sf.some(\"flag\")).show()\n+----------+\n|some(flag)|\n+----------+\n|     false|\n+----------+"], "Parameters": [["col Column or column name", "column to check if at least one value is true."]], "Returns": [["Column", "true if at least one value of col is true, false otherwise."]], "Category": ["Functions"], "index": 452}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.std.html#pyspark.sql.functions.std"], "Title": ["std"], "Feature": ["std"], "Description": "Aggregate function: alias for stddev_samp.\nSee alsopyspark.sql.functions.stddev()pyspark.sql.functions.stddev_pop()pyspark.sql.functions.stddev_samp()pyspark.sql.functions.variance()pyspark.sql.functions.skewness()pyspark.sql.functions.kurtosis()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(6).select(sf.std(\"id\")).show()\n+------------------+\n|           std(id)|\n+------------------+\n|1.8708286933869...|\n+------------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "standard deviation of given column."]], "Category": ["Functions"], "index": 453}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.stddev.html#pyspark.sql.functions.stddev"], "Title": ["stddev"], "Feature": ["stddev"], "Description": "Aggregate function: alias for stddev_samp.\nSee alsopyspark.sql.functions.std()pyspark.sql.functions.stddev_pop()pyspark.sql.functions.stddev_samp()pyspark.sql.functions.variance()pyspark.sql.functions.skewness()pyspark.sql.functions.kurtosis()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(6).select(sf.stddev(\"id\")).show()\n+------------------+\n|        stddev(id)|\n+------------------+\n|1.8708286933869...|\n+------------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "standard deviation of given column."]], "Category": ["Functions"], "index": 454}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.stddev_pop.html#pyspark.sql.functions.stddev_pop"], "Title": ["stddev_pop"], "Feature": ["stddev_pop"], "Description": "Aggregate function: returns population standard deviation of\nthe expression in a group.\nSee alsopyspark.sql.functions.std()pyspark.sql.functions.stddev()pyspark.sql.functions.stddev_samp()pyspark.sql.functions.var_pop()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(6).select(sf.stddev_pop(\"id\")).show()\n+-----------------+\n|   stddev_pop(id)|\n+-----------------+\n|1.707825127659...|\n+-----------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "standard deviation of given column."]], "Category": ["Functions"], "index": 455}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.stddev_samp.html#pyspark.sql.functions.stddev_samp"], "Title": ["stddev_samp"], "Feature": ["stddev_samp"], "Description": "Aggregate function: returns the unbiased sample standard deviation of\nthe expression in a group.\nSee alsopyspark.sql.functions.std()pyspark.sql.functions.stddev()pyspark.sql.functions.stddev_pop()pyspark.sql.functions.var_samp()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(6).select(sf.stddev_samp(\"id\")).show()\n+------------------+\n|   stddev_samp(id)|\n+------------------+\n|1.8708286933869...|\n+------------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "standard deviation of given column."]], "Category": ["Functions"], "index": 456}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.melt.html#pyspark.sql.DataFrame.melt"], "Title": ["DataFrame.melt"], "Feature": ["DataFrame.melt"], "Description": "Unpivot a DataFrame from wide format to long format, optionally leaving\nidentifier columns set. This is the reverse togroupBy(…).pivot(…).agg(…),\nexcept for the aggregation, which cannot be reversed.\nmelt()is an alias forunpivot().\nSee alsoDataFrame.unpivot\nNotes\nSupports Spark Connect.", "Examples": [], "Parameters": [["ids str, Column, tuple, list, optional", "Column(s) to use as identifiers. Can be a single column or column name,\nor a list or tuple for multiple columns."], ["values str, Column, tuple, list, optional", "Column(s) to unpivot. Can be a single column or column name, or a list or tuple\nfor multiple columns. If not specified or empty, use all columns that\nare not set as ids ."], ["variableColumnName str", "Name of the variable column."], ["valueColumnName str", "Name of the value column."]], "Returns": [["DataFrame", "Unpivoted DataFrame."]], "Category": ["DataFrame"], "index": 457}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.string_agg.html#pyspark.sql.functions.string_agg"], "Title": ["string_agg"], "Feature": ["string_agg"], "Description": "Aggregate function: returns the concatenation of non-null input values,\nseparated by the delimiter.\nAn alias oflistagg().", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('a',), ('b',), (None,), ('c',)], ['strings'])\n>>> df.select(sf.string_agg('strings')).show()\n+-------------------------+\n|string_agg(strings, NULL)|\n+-------------------------+\n|                      abc|\n+-------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('a',), ('b',), (None,), ('c',)], ['strings'])\n>>> df.select(sf.string_agg('strings', ', ')).show()\n+-----------------------+\n|string_agg(strings, , )|\n+-----------------------+\n|                a, b, c|\n+-----------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(b'\u0001',), (b'\u0002',), (None,), (b'\u0003',)], ['bytes'])\n>>> df.select(sf.string_agg('bytes', b'B')).show()\n+------------------------+\n|string_agg(bytes, X'42')|\n+------------------------+\n|        [01 42 02 42 03]|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import StructType, StructField, StringType\n>>> schema = StructType([StructField(\"strings\", StringType(), True)])\n>>> df = spark.createDataFrame([(None,), (None,), (None,), (None,)], schema=schema)\n>>> df.select(sf.string_agg('strings')).show()\n+-------------------------+\n|string_agg(strings, NULL)|\n+-------------------------+\n|                     NULL|\n+-------------------------+"], "Parameters": [["col Column or column name", "target column to compute on."], ["delimiter Column , literal string or bytes, optional", "the delimiter to separate the values. The default value is None."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 458}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.string_agg_distinct.html#pyspark.sql.functions.string_agg_distinct"], "Title": ["string_agg_distinct"], "Feature": ["string_agg_distinct"], "Description": "Aggregate function: returns the concatenation of distinct non-null input values,\nseparated by the delimiter.\nAn alias oflistagg_distinct().", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('a',), ('b',), (None,), ('c',), ('b',)], ['strings'])\n>>> df.select(sf.string_agg_distinct('strings')).show()\n+----------------------------------+\n|string_agg(DISTINCT strings, NULL)|\n+----------------------------------+\n|                               abc|\n+----------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('a',), ('b',), (None,), ('c',), ('b',)], ['strings'])\n>>> df.select(sf.string_agg_distinct('strings', ', ')).show()\n+--------------------------------+\n|string_agg(DISTINCT strings, , )|\n+--------------------------------+\n|                         a, b, c|\n+--------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(b'\u0001',), (b'\u0002',), (None,), (b'\u0003',), (b'\u0002',)],\n...                            ['bytes'])\n>>> df.select(sf.string_agg_distinct('bytes', b'B')).show()\n+---------------------------------+\n|string_agg(DISTINCT bytes, X'42')|\n+---------------------------------+\n|                 [01 42 02 42 03]|\n+---------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import StructType, StructField, StringType\n>>> schema = StructType([StructField(\"strings\", StringType(), True)])\n>>> df = spark.createDataFrame([(None,), (None,), (None,), (None,)], schema=schema)\n>>> df.select(sf.string_agg_distinct('strings')).show()\n+----------------------------------+\n|string_agg(DISTINCT strings, NULL)|\n+----------------------------------+\n|                              NULL|\n+----------------------------------+"], "Parameters": [["col Column or column name", "target column to compute on."], ["delimiter Column , literal string or bytes, optional", "the delimiter to separate the values. The default value is None."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 459}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sum.html#pyspark.sql.functions.sum"], "Title": ["sum"], "Feature": ["sum"], "Description": "Aggregate function: returns the sum of all values in the expression.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.range(10)\n>>> df.select(sf.sum(df[\"id\"])).show()\n+-------+\n|sum(id)|\n+-------+\n|     45|\n+-------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, 2), (3, 4)], [\"A\", \"B\"])\n>>> df.select(sf.sum(sf.col(\"A\") + sf.col(\"B\"))).show()\n+------------+\n|sum((A + B))|\n+------------+\n|          10|\n+------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1982, None), (1990, 2), (2000, 4)], [\"birth\", \"age\"])\n>>> df.select(sf.sum(\"age\")).show()\n+--------+\n|sum(age)|\n+--------+\n|       6|\n+--------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 460}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sum_distinct.html#pyspark.sql.functions.sum_distinct"], "Title": ["sum_distinct"], "Feature": ["sum_distinct"], "Description": "Aggregate function: returns the sum of distinct values in the expression.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1,), (2,), (3,), (4,)], [\"numbers\"])\n>>> df.select(sf.sum_distinct('numbers')).show()\n+---------------------+\n|sum(DISTINCT numbers)|\n+---------------------+\n|                   10|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1,), (1,), (1,), (1,)], [\"numbers\"])\n>>> df.select(sf.sum_distinct('numbers')).show()\n+---------------------+\n|sum(DISTINCT numbers)|\n+---------------------+\n|                    1|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(None,), (1,), (1,), (2,)], [\"numbers\"])\n>>> df.select(sf.sum_distinct('numbers')).show()\n+---------------------+\n|sum(DISTINCT numbers)|\n+---------------------+\n|                    3|\n+---------------------+", ">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import StructType, StructField, IntegerType\n>>> schema = StructType([StructField(\"numbers\", IntegerType(), True)])\n>>> df = spark.createDataFrame([(None,), (None,), (None,), (None,)], schema=schema)\n>>> df.select(sf.sum_distinct('numbers')).show()\n+---------------------+\n|sum(DISTINCT numbers)|\n+---------------------+\n|                 NULL|\n+---------------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "the column for computed results."]], "Category": ["Functions"], "index": 461}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_avg.html#pyspark.sql.functions.try_avg"], "Title": ["try_avg"], "Feature": ["try_avg"], "Description": "Returns the mean calculated from values of a group and the result is null on overflow.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1982, 15), (1990, 2)], [\"birth\", \"age\"])\n>>> df.select(sf.try_avg(\"age\")).show()\n+------------+\n|try_avg(age)|\n+------------+\n|         8.5|\n+------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1982, None), (1990, 2), (2000, 4)], [\"birth\", \"age\"])\n>>> df.select(sf.try_avg(\"age\")).show()\n+------------+\n|try_avg(age)|\n+------------+\n|         3.0|\n+------------+", ">>> from decimal import Decimal\n>>> import pyspark.sql.functions as sf\n>>> origin = spark.conf.get(\"spark.sql.ansi.enabled\")\n>>> spark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\n>>> try:\n...     df = spark.createDataFrame(\n...         [(Decimal(\"1\" * 38),), (Decimal(0),)], \"number DECIMAL(38, 0)\")\n...     df.select(sf.try_avg(df.number)).show()\n... finally:\n...     spark.conf.set(\"spark.sql.ansi.enabled\", origin)\n+---------------+\n|try_avg(number)|\n+---------------+\n|           NULL|\n+---------------+"], "Parameters": [["col Column or column name", ""]], "Returns": [], "Category": ["Functions"], "index": 462}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_sum.html#pyspark.sql.functions.try_sum"], "Title": ["try_sum"], "Feature": ["try_sum"], "Description": "Returns the sum calculated from values of a group and the result is null on overflow.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(10).select(sf.try_sum(\"id\")).show()\n+-----------+\n|try_sum(id)|\n+-----------+\n|         45|\n+-----------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, 2), (3, 4)], [\"A\", \"B\"])\n>>> df.select(sf.try_sum(sf.col(\"A\") + sf.col(\"B\"))).show()\n+----------------+\n|try_sum((A + B))|\n+----------------+\n|              10|\n+----------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1982, None), (1990, 2), (2000, 4)], [\"birth\", \"age\"])\n>>> df.select(sf.try_sum(\"age\")).show()\n+------------+\n|try_sum(age)|\n+------------+\n|           6|\n+------------+", ">>> from decimal import Decimal\n>>> import pyspark.sql.functions as sf\n>>> origin = spark.conf.get(\"spark.sql.ansi.enabled\")\n>>> spark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\n>>> try:\n...     df = spark.createDataFrame([(Decimal(\"1\" * 38),)] * 10, \"number DECIMAL(38, 0)\")\n...     df.select(sf.try_sum(df.number)).show()\n... finally:\n...     spark.conf.set(\"spark.sql.ansi.enabled\", origin)\n+---------------+\n|try_sum(number)|\n+---------------+\n|           NULL|\n+---------------+"], "Parameters": [["col Column or column name", ""]], "Returns": [], "Category": ["Functions"], "index": 463}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.var_pop.html#pyspark.sql.functions.var_pop"], "Title": ["var_pop"], "Feature": ["var_pop"], "Description": "Aggregate function: returns the population variance of the values in a group.\nSee alsopyspark.sql.functions.variance()pyspark.sql.functions.var_samp()pyspark.sql.functions.std_pop()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.range(6)\n>>> df.select(sf.var_pop(df.id)).show()\n+------------------+\n|       var_pop(id)|\n+------------------+\n|2.9166666666666...|\n+------------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "variance of given column."]], "Category": ["Functions"], "index": 464}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.var_samp.html#pyspark.sql.functions.var_samp"], "Title": ["var_samp"], "Feature": ["var_samp"], "Description": "Aggregate function: returns the unbiased sample variance of\nthe values in a group.\nSee alsopyspark.sql.functions.variance()pyspark.sql.functions.var_pop()pyspark.sql.functions.std_samp()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.range(6)\n>>> df.select(sf.var_samp(df.id)).show()\n+------------+\n|var_samp(id)|\n+------------+\n|         3.5|\n+------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "variance of given column."]], "Category": ["Functions"], "index": 465}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.variance.html#pyspark.sql.functions.variance"], "Title": ["variance"], "Feature": ["variance"], "Description": "Aggregate function: alias for var_samp\nSee alsopyspark.sql.functions.var_pop()pyspark.sql.functions.var_samp()pyspark.sql.functions.stddev()pyspark.sql.functions.skewness()pyspark.sql.functions.kurtosis()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.range(6)\n>>> df.select(sf.variance(df.id)).show()\n+------------+\n|variance(id)|\n+------------+\n|         3.5|\n+------------+"], "Parameters": [["col Column or column name", "target column to compute on."]], "Returns": [["Column", "variance of given column."]], "Category": ["Functions"], "index": 466}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.cume_dist.html#pyspark.sql.functions.cume_dist"], "Title": ["cume_dist"], "Feature": ["cume_dist"], "Description": "Window function: returns the cumulative distribution of values within a window partition,\ni.e. the fraction of rows that are below the current row.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql import Window\n>>> df = spark.createDataFrame([1, 2, 3, 3, 4], \"int\")\n>>> w = Window.orderBy(\"value\")\n>>> df.withColumn(\"cd\", sf.cume_dist().over(w)).show()\n+-----+---+\n|value| cd|\n+-----+---+\n|    1|0.2|\n|    2|0.4|\n|    3|0.8|\n|    3|0.8|\n|    4|1.0|\n+-----+---+"], "Parameters": [], "Returns": [["Column", "the column for calculating cumulative distribution."]], "Category": ["Functions"], "index": 467}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.na.html#pyspark.sql.DataFrame.na"], "Title": ["DataFrame.na"], "Feature": ["DataFrame.na"], "Description": "Returns aDataFrameNaFunctionsfor handling missing values.", "Examples": [">>> df = spark.sql(\"SELECT 1 AS c1, int(NULL) AS c2\")\n>>> type(df.na)\n<class '...dataframe.DataFrameNaFunctions'>", ">>> df.na.fill(2).show()\n+---+---+\n| c1| c2|\n+---+---+\n|  1|  2|\n+---+---+"], "Parameters": [], "Returns": [["DataFrameNaFunctions", ""]], "Category": ["DataFrame"], "index": 468}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.dense_rank.html#pyspark.sql.functions.dense_rank"], "Title": ["dense_rank"], "Feature": ["dense_rank"], "Description": "Window function: returns the rank of rows within a window partition, without any gaps.\nThe difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\nsequence when there are ties. That is, if you were ranking a competition using dense_rank\nand had three people tie for second place, you would say that all three were in second\nplace and that the next person came in third. Rank would give me sequential numbers, making\nthe person that came in third place (after the ties) would register as coming in fifth.\nThis is equivalent to the DENSE_RANK function in SQL.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql import Window\n>>> df = spark.createDataFrame([1, 1, 2, 3, 3, 4], \"int\")\n>>> w = Window.orderBy(\"value\")\n>>> df.withColumn(\"drank\", sf.dense_rank().over(w)).show()\n+-----+-----+\n|value|drank|\n+-----+-----+\n|    1|    1|\n|    1|    1|\n|    2|    2|\n|    3|    3|\n|    3|    3|\n|    4|    4|\n+-----+-----+"], "Parameters": [], "Returns": [["Column", "the column for calculating ranks."]], "Category": ["Functions"], "index": 469}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.lag.html#pyspark.sql.functions.lag"], "Title": ["lag"], "Feature": ["lag"], "Description": "Window function: returns the value that isoffsetrows before the current row, anddefaultif there is less thanoffsetrows before the current row. For example,\nanoffsetof one will return the previous row at any given point in the window partition.\nThis is equivalent to the LAG function in SQL.\nSee alsopyspark.sql.functions.lead()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql import Window\n>>> df = spark.createDataFrame(\n...     [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"c1\", \"c2\"])\n>>> df.show()\n+---+---+\n| c1| c2|\n+---+---+\n|  a|  1|\n|  a|  2|\n|  a|  3|\n|  b|  8|\n|  b|  2|\n+---+---+", ">>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n>>> df.withColumn(\"previous_value\", sf.lag(\"c2\").over(w)).show()\n+---+---+--------------+\n| c1| c2|previous_value|\n+---+---+--------------+\n|  a|  1|          NULL|\n|  a|  2|             1|\n|  a|  3|             2|\n|  b|  2|          NULL|\n|  b|  8|             2|\n+---+---+--------------+", ">>> df.withColumn(\"previous_value\", sf.lag(\"c2\", 1, 0).over(w)).show()\n+---+---+--------------+\n| c1| c2|previous_value|\n+---+---+--------------+\n|  a|  1|             0|\n|  a|  2|             1|\n|  a|  3|             2|\n|  b|  2|             0|\n|  b|  8|             2|\n+---+---+--------------+", ">>> df.withColumn(\"previous_value\", sf.lag(\"c2\", 2, -1).over(w)).show()\n+---+---+--------------+\n| c1| c2|previous_value|\n+---+---+--------------+\n|  a|  1|            -1|\n|  a|  2|            -1|\n|  a|  3|             1|\n|  b|  2|            -1|\n|  b|  8|            -1|\n+---+---+--------------+"], "Parameters": [["col Column or column name", "name of column or expression"], ["offset int, optional default 1", "number of row to extend"], ["default optional", "default value"]], "Returns": [["Column", "value before current row based on offset ."]], "Category": ["Functions"], "index": 470}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.lead.html#pyspark.sql.functions.lead"], "Title": ["lead"], "Feature": ["lead"], "Description": "Window function: returns the value that isoffsetrows after the current row, anddefaultif there is less thanoffsetrows after the current row. For example,\nanoffsetof one will return the next row at any given point in the window partition.\nThis is equivalent to the LEAD function in SQL.\nSee alsopyspark.sql.functions.lag()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql import Window\n>>> df = spark.createDataFrame(\n...     [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"c1\", \"c2\"])\n>>> df.show()\n+---+---+\n| c1| c2|\n+---+---+\n|  a|  1|\n|  a|  2|\n|  a|  3|\n|  b|  8|\n|  b|  2|\n+---+---+", ">>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n>>> df.withColumn(\"next_value\", sf.lead(\"c2\").over(w)).show()\n+---+---+----------+\n| c1| c2|next_value|\n+---+---+----------+\n|  a|  1|         2|\n|  a|  2|         3|\n|  a|  3|      NULL|\n|  b|  2|         8|\n|  b|  8|      NULL|\n+---+---+----------+", ">>> df.withColumn(\"next_value\", sf.lead(\"c2\", 1, 0).over(w)).show()\n+---+---+----------+\n| c1| c2|next_value|\n+---+---+----------+\n|  a|  1|         2|\n|  a|  2|         3|\n|  a|  3|         0|\n|  b|  2|         8|\n|  b|  8|         0|\n+---+---+----------+", ">>> df.withColumn(\"next_value\", sf.lead(\"c2\", 2, -1).over(w)).show()\n+---+---+----------+\n| c1| c2|next_value|\n+---+---+----------+\n|  a|  1|         3|\n|  a|  2|        -1|\n|  a|  3|        -1|\n|  b|  2|        -1|\n|  b|  8|        -1|\n+---+---+----------+"], "Parameters": [["col Column or column name", "name of column or expression"], ["offset int, optional default 1", "number of row to extend"], ["default optional", "default value"]], "Returns": [["Column", "value after current row based on offset ."]], "Category": ["Functions"], "index": 471}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.nth_value.html#pyspark.sql.functions.nth_value"], "Title": ["nth_value"], "Feature": ["nth_value"], "Description": "Window function: returns the value that is theoffsetth row of the window frame\n(counting from 1), andnullif the size of window frame is less thanoffsetrows.\nIt will return theoffsetth non-null value it sees whenignoreNullsis set to\ntrue. If all values are null, then null is returned.\nThis is equivalent to the nth_value function in SQL.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql import Window\n>>> df = spark.createDataFrame(\n...     [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"c1\", \"c2\"])\n>>> df.show()\n+---+---+\n| c1| c2|\n+---+---+\n|  a|  1|\n|  a|  2|\n|  a|  3|\n|  b|  8|\n|  b|  2|\n+---+---+", ">>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n>>> df.withColumn(\"nth_value\", sf.nth_value(\"c2\", 1).over(w)).show()\n+---+---+---------+\n| c1| c2|nth_value|\n+---+---+---------+\n|  a|  1|        1|\n|  a|  2|        1|\n|  a|  3|        1|\n|  b|  2|        2|\n|  b|  8|        2|\n+---+---+---------+", ">>> df.withColumn(\"nth_value\", sf.nth_value(\"c2\", 2).over(w)).show()\n+---+---+---------+\n| c1| c2|nth_value|\n+---+---+---------+\n|  a|  1|     NULL|\n|  a|  2|        2|\n|  a|  3|        2|\n|  b|  2|     NULL|\n|  b|  8|        8|\n+---+---+---------+"], "Parameters": [["col Column or column name", "name of column or expression"], ["offset int", "number of row to use as the value"], ["ignoreNulls bool, optional", "indicates the Nth value should skip null in the\ndetermination of which row to use"]], "Returns": [["Column", "value of nth row."]], "Category": ["Functions"], "index": 472}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ntile.html#pyspark.sql.functions.ntile"], "Title": ["ntile"], "Feature": ["ntile"], "Description": "Window function: returns the ntile group id (from 1 toninclusive)\nin an ordered window partition. For example, ifnis 4, the first\nquarter of the rows will get value 1, the second quarter will get 2,\nthe third quarter will get 3, and the last quarter will get 4.\nThis is equivalent to the NTILE function in SQL.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql import Window\n>>> df = spark.createDataFrame(\n...     [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"c1\", \"c2\"])\n>>> df.show()\n+---+---+\n| c1| c2|\n+---+---+\n|  a|  1|\n|  a|  2|\n|  a|  3|\n|  b|  8|\n|  b|  2|\n+---+---+", ">>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n>>> df.withColumn(\"ntile\", sf.ntile(2).over(w)).show()\n+---+---+-----+\n| c1| c2|ntile|\n+---+---+-----+\n|  a|  1|    1|\n|  a|  2|    1|\n|  a|  3|    2|\n|  b|  2|    1|\n|  b|  8|    2|\n+---+---+-----+"], "Parameters": [["n int", "an integer"]], "Returns": [["Column", "portioned group id."]], "Category": ["Functions"], "index": 473}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.percent_rank.html#pyspark.sql.functions.percent_rank"], "Title": ["percent_rank"], "Feature": ["percent_rank"], "Description": "Window function: returns the relative rank (i.e. percentile) of rows within a window partition.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql import Window\n>>> df = spark.createDataFrame([1, 1, 2, 3, 3, 4], \"int\")\n>>> w = Window.orderBy(\"value\")\n>>> df.withColumn(\"pr\", sf.percent_rank().over(w)).show()\n+-----+---+\n|value| pr|\n+-----+---+\n|    1|0.0|\n|    1|0.0|\n|    2|0.4|\n|    3|0.6|\n|    3|0.6|\n|    4|1.0|\n+-----+---+"], "Parameters": [], "Returns": [["Column", "the column for calculating relative rank."]], "Category": ["Functions"], "index": 474}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.rank.html#pyspark.sql.functions.rank"], "Title": ["rank"], "Feature": ["rank"], "Description": "Window function: returns the rank of rows within a window partition.\nThe difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\nsequence when there are ties. That is, if you were ranking a competition using dense_rank\nand had three people tie for second place, you would say that all three were in second\nplace and that the next person came in third. Rank would give me sequential numbers, making\nthe person that came in third place (after the ties) would register as coming in fifth.\nThis is equivalent to the RANK function in SQL.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql import Window\n>>> df = spark.createDataFrame([1, 1, 2, 3, 3, 4], \"int\")\n>>> w = Window.orderBy(\"value\")\n>>> df.withColumn(\"drank\", sf.rank().over(w)).show()\n+-----+-----+\n|value|drank|\n+-----+-----+\n|    1|    1|\n|    1|    1|\n|    2|    3|\n|    3|    4|\n|    3|    4|\n|    4|    6|\n+-----+-----+"], "Parameters": [], "Returns": [["Column", "the column for calculating ranks."]], "Category": ["Functions"], "index": 475}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.row_number.html#pyspark.sql.functions.row_number"], "Title": ["row_number"], "Feature": ["row_number"], "Description": "Window function: returns a sequential number starting at 1 within a window partition.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql import Window\n>>> df = spark.range(3)\n>>> w = Window.orderBy(df.id.desc())\n>>> df.withColumn(\"desc_order\", sf.row_number().over(w)).show()\n+---+----------+\n| id|desc_order|\n+---+----------+\n|  2|         1|\n|  1|         2|\n|  0|         3|\n+---+----------+"], "Parameters": [], "Returns": [["Column", "the column for calculating row numbers."]], "Category": ["Functions"], "index": 476}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.explode.html#pyspark.sql.functions.explode"], "Title": ["explode"], "Feature": ["explode"], "Description": "Returns a new row for each element in the given array or map.\nUses the default column namecolfor elements in the array andkeyandvaluefor elements in the map unless specified otherwise.\nSee alsopyspark.sql.functions.posexplode()pyspark.sql.functions.explode_outer()pyspark.sql.functions.posexplode_outer()pyspark.sql.functions.inline()pyspark.sql.functions.inline_outer()\nNotes\nOnly one explode is allowed per SELECT clause.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql('SELECT * FROM VALUES (1,ARRAY(1,2,3,NULL)), (2,ARRAY()), (3,NULL) AS t(i,a)')\n>>> df.show()\n+---+---------------+\n|  i|              a|\n+---+---------------+\n|  1|[1, 2, 3, NULL]|\n|  2|             []|\n|  3|           NULL|\n+---+---------------+", ">>> df.select('*', sf.explode('a')).show()\n+---+---------------+----+\n|  i|              a| col|\n+---+---------------+----+\n|  1|[1, 2, 3, NULL]|   1|\n|  1|[1, 2, 3, NULL]|   2|\n|  1|[1, 2, 3, NULL]|   3|\n|  1|[1, 2, 3, NULL]|NULL|\n+---+---------------+----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql('SELECT * FROM VALUES (1,MAP(1,2,3,4,5,NULL)), (2,MAP()), (3,NULL) AS t(i,m)')\n>>> df.show(truncate=False)\n+---+---------------------------+\n|i  |m                          |\n+---+---------------------------+\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|\n|2  |{}                         |\n|3  |NULL                       |\n+---+---------------------------+", ">>> df.select('*', sf.explode('m')).show(truncate=False)\n+---+---------------------------+---+-----+\n|i  |m                          |key|value|\n+---+---------------------------+---+-----+\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|1  |2    |\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|3  |4    |\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|5  |NULL |\n+---+---------------------------+---+-----+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql('SELECT ARRAY(1,2) AS a1, ARRAY(3,4,5) AS a2')\n>>> df.select(\n...     '*', sf.explode('a1').alias('v1')\n... ).select('*', sf.explode('a2').alias('v2')).show()\n+------+---------+---+---+\n|    a1|       a2| v1| v2|\n+------+---------+---+---+\n|[1, 2]|[3, 4, 5]|  1|  3|\n|[1, 2]|[3, 4, 5]|  1|  4|\n|[1, 2]|[3, 4, 5]|  1|  5|\n|[1, 2]|[3, 4, 5]|  2|  3|\n|[1, 2]|[3, 4, 5]|  2|  4|\n|[1, 2]|[3, 4, 5]|  2|  5|\n+------+---------+---+---+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql('SELECT ARRAY(NAMED_STRUCT(\"a\",1,\"b\",2), NAMED_STRUCT(\"a\",3,\"b\",4)) AS a')\n>>> df.select(sf.explode('a').alias(\"s\")).select(\"s.*\").show()\n+---+---+\n|  a|  b|\n+---+---+\n|  1|  2|\n|  3|  4|\n+---+---+"], "Parameters": [["col Column or column name", "Target column to work on."]], "Returns": [["Column", "One row per array item or map key value."]], "Category": ["Functions"], "index": 477}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.explode_outer.html#pyspark.sql.functions.explode_outer"], "Title": ["explode_outer"], "Feature": ["explode_outer"], "Description": "Returns a new row for each element in the given array or map.\nUnlike explode, if the array/map is null or empty then null is produced.\nUses the default column namecolfor elements in the array andkeyandvaluefor elements in the map unless specified otherwise.\nSee alsopyspark.sql.functions.explode()pyspark.sql.functions.posexplode()pyspark.sql.functions.posexplode_outer()pyspark.sql.functions.inline()pyspark.sql.functions.inline_outer()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql('SELECT * FROM VALUES (1,ARRAY(1,2,3,NULL)), (2,ARRAY()), (3,NULL) AS t(i,a)')\n>>> df.select('*', sf.explode_outer('a')).show()\n+---+---------------+----+\n|  i|              a| col|\n+---+---------------+----+\n|  1|[1, 2, 3, NULL]|   1|\n|  1|[1, 2, 3, NULL]|   2|\n|  1|[1, 2, 3, NULL]|   3|\n|  1|[1, 2, 3, NULL]|NULL|\n|  2|             []|NULL|\n|  3|           NULL|NULL|\n+---+---------------+----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql('SELECT * FROM VALUES (1,MAP(1,2,3,4,5,NULL)), (2,MAP()), (3,NULL) AS t(i,m)')\n>>> df.select('*', sf.explode_outer('m')).show(truncate=False)\n+---+---------------------------+----+-----+\n|i  |m                          |key |value|\n+---+---------------------------+----+-----+\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|1   |2    |\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|3   |4    |\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|5   |NULL |\n|2  |{}                         |NULL|NULL |\n|3  |NULL                       |NULL|NULL |\n+---+---------------------------+----+-----+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "one row per array item or map key value."]], "Category": ["Functions"], "index": 478}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.observe.html#pyspark.sql.DataFrame.observe"], "Title": ["DataFrame.observe"], "Feature": ["DataFrame.observe"], "Description": "Define (named) metrics to observe on the DataFrame. This method returns an ‘observed’\nDataFrame that returns the same result as the input, with the following guarantees:\nIt will compute the defined aggregates (metrics) on all the data that is flowing throughthe Dataset at that point.It will report the value of the defined aggregate columns as soon as we reach a completionpoint. A completion point is either the end of a query (batch mode) or the end of a\nstreaming epoch. The value of the aggregates only reflects the data processed since\nthe previous completion point.\nThe metrics columns must either contain a literal (e.g. lit(42)), or should contain one or\nmore aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that\ncontain references to the input Dataset’s columns must always be wrapped in an aggregate\nfunction.\nA user can observe these metrics by adding\nPython’sStreamingQueryListener,\nScala/Java’sorg.apache.spark.sql.streaming.StreamingQueryListeneror Scala/Java’sorg.apache.spark.sql.util.QueryExecutionListenerto the spark session.\nNotes\nWhenobservationisObservation, this method only supports batch queries.\nWhenobservationis a string, this method works for both batch and streaming queries.\nContinuous execution is currently not supported yet.", "Examples": [">>> from pyspark.sql.functions import col, count, lit, max\n>>> from pyspark.sql import Observation\n>>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> observation = Observation(\"my metrics\")\n>>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n>>> observed_df.count()\n2\n>>> observation.get\n{'count': 2, 'max(age)': 5}", ">>> from pyspark.sql.streaming import StreamingQueryListener\n>>> import time\n>>> class MyErrorListener(StreamingQueryListener):\n...    def onQueryStarted(self, event):\n...        pass\n...\n...    def onQueryProgress(self, event):\n...        row = event.progress.observedMetrics.get(\"my_event\")\n...        # Trigger if the number of errors exceeds 5 percent\n...        num_rows = row.rc\n...        num_error_rows = row.erc\n...        ratio = num_error_rows / num_rows\n...        if ratio > 0.05:\n...            # Trigger alert\n...            pass\n...\n...    def onQueryIdle(self, event):\n...        pass\n...\n...    def onQueryTerminated(self, event):\n...        pass\n...\n>>> error_listener = MyErrorListener()\n>>> spark.streams.addListener(error_listener)\n>>> sdf = spark.readStream.format(\"rate\").load().withColumn(\n...     \"error\", col(\"value\")\n... )\n>>> # Observe row count (rc) and error row count (erc) in the streaming Dataset\n... observed_ds = sdf.observe(\n...     \"my_event\",\n...     count(lit(1)).alias(\"rc\"),\n...     count(col(\"error\")).alias(\"erc\"))\n>>> try:\n...     q = observed_ds.writeStream.format(\"console\").start()\n...     time.sleep(5)\n...\n... finally:\n...     q.stop()\n...     spark.streams.removeListener(error_listener)\n..."], "Parameters": [["observation Observation or str", "str to specify the name, or an Observation instance to obtain the metric. Changed in version 3.4.0: Added support for str in this parameter."], ["exprs Column", "column expressions ( Column )."]], "Returns": [["DataFrame", "the observed DataFrame ."]], "Category": ["DataFrame"], "index": 479}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.inline.html#pyspark.sql.functions.inline"], "Title": ["inline"], "Feature": ["inline"], "Description": "Explodes an array of structs into a table.\nThis function takes an input column containing an array of structs and returns a\nnew column where each struct in the array is exploded into a separate row.\nSee alsopyspark.sql.functions.explode()pyspark.sql.functions.explode_outer()pyspark.sql.functions.posexplode()pyspark.sql.functions.posexplode_outer()pyspark.sql.functions.inline_outer()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.sql('SELECT ARRAY(NAMED_STRUCT(\"a\",1,\"b\",2), NAMED_STRUCT(\"a\",3,\"b\",4)) AS a')\n>>> df.select('*', sf.inline(df.a)).show()\n+----------------+---+---+\n|               a|  a|  b|\n+----------------+---+---+\n|[{1, 2}, {3, 4}]|  1|  2|\n|[{1, 2}, {3, 4}]|  3|  4|\n+----------------+---+---+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql('SELECT ARRAY(NAMED_STRUCT(\"a\",1,\"b\",2), NAMED_STRUCT(\"a\",3,\"b\",4)) AS a')\n>>> df.select('*', sf.inline('a')).show()\n+----------------+---+---+\n|               a|  a|  b|\n+----------------+---+---+\n|[{1, 2}, {3, 4}]|  1|  2|\n|[{1, 2}, {3, 4}]|  3|  4|\n+----------------+---+---+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql('SELECT ARRAY(NAMED_STRUCT(\"a\",1,\"b\",2), NAMED_STRUCT(\"a\",3,\"b\",4)) AS a')\n>>> df.select('*', sf.inline('a').alias(\"c1\", \"c2\")).show()\n+----------------+---+---+\n|               a| c1| c2|\n+----------------+---+---+\n|[{1, 2}, {3, 4}]|  1|  2|\n|[{1, 2}, {3, 4}]|  3|  4|\n+----------------+---+---+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql('SELECT ARRAY(NAMED_STRUCT(\"a\",1,\"b\",2), NAMED_STRUCT(\"a\",3,\"b\",4)) AS a1, ARRAY(NAMED_STRUCT(\"c\",5,\"d\",6), NAMED_STRUCT(\"c\",7,\"d\",8)) AS a2')\n>>> df.select(\n...     '*', sf.inline('a1')\n... ).select('*', sf.inline('a2')).show()\n+----------------+----------------+---+---+---+---+\n|              a1|              a2|  a|  b|  c|  d|\n+----------------+----------------+---+---+---+---+\n|[{1, 2}, {3, 4}]|[{5, 6}, {7, 8}]|  1|  2|  5|  6|\n|[{1, 2}, {3, 4}]|[{5, 6}, {7, 8}]|  1|  2|  7|  8|\n|[{1, 2}, {3, 4}]|[{5, 6}, {7, 8}]|  3|  4|  5|  6|\n|[{1, 2}, {3, 4}]|[{5, 6}, {7, 8}]|  3|  4|  7|  8|\n+----------------+----------------+---+---+---+---+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql('SELECT NAMED_STRUCT(\"a\",1,\"b\",2,\"c\",ARRAY(NAMED_STRUCT(\"c\",3,\"d\",4), NAMED_STRUCT(\"c\",5,\"d\",6))) AS s')\n>>> df.select('*', sf.inline('s.c')).show(truncate=False)\n+------------------------+---+---+\n|s                       |c  |d  |\n+------------------------+---+---+\n|{1, 2, [{3, 4}, {5, 6}]}|3  |4  |\n|{1, 2, [{3, 4}, {5, 6}]}|5  |6  |\n+------------------------+---+---+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql('SELECT * FROM VALUES (1,ARRAY(NAMED_STRUCT(\"a\",1,\"b\",2), NULL, NAMED_STRUCT(\"a\",3,\"b\",4))), (2,ARRAY()), (3,NULL) AS t(i,s)')\n>>> df.show(truncate=False)\n+---+----------------------+\n|i  |s                     |\n+---+----------------------+\n|1  |[{1, 2}, NULL, {3, 4}]|\n|2  |[]                    |\n|3  |NULL                  |\n+---+----------------------+", ">>> df.select('*', sf.inline('s')).show(truncate=False)\n+---+----------------------+----+----+\n|i  |s                     |a   |b   |\n+---+----------------------+----+----+\n|1  |[{1, 2}, NULL, {3, 4}]|1   |2   |\n|1  |[{1, 2}, NULL, {3, 4}]|NULL|NULL|\n|1  |[{1, 2}, NULL, {3, 4}]|3   |4   |\n+---+----------------------+----+----+"], "Parameters": [["col Column or column name", "Input column of values to explode."]], "Returns": [["Column", "Generator expression with the inline exploded result."]], "Category": ["Functions"], "index": 480}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.inline_outer.html#pyspark.sql.functions.inline_outer"], "Title": ["inline_outer"], "Feature": ["inline_outer"], "Description": "Explodes an array of structs into a table.\nUnlike inline, if the array is null or empty then null is produced for each nested column.\nSee alsopyspark.sql.functions.explode()pyspark.sql.functions.explode_outer()pyspark.sql.functions.posexplode()pyspark.sql.functions.posexplode_outer()pyspark.sql.functions.inline()\nNotes\nSupports Spark Connect.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql('SELECT * FROM VALUES (1,ARRAY(NAMED_STRUCT(\"a\",1,\"b\",2), NULL, NAMED_STRUCT(\"a\",3,\"b\",4))), (2,ARRAY()), (3,NULL) AS t(i,s)')\n>>> df.printSchema()\nroot\n |-- i: integer (nullable = false)\n |-- s: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- a: integer (nullable = false)\n |    |    |-- b: integer (nullable = false)", ">>> df.select('*', sf.inline_outer('s')).show(truncate=False)\n+---+----------------------+----+----+\n|i  |s                     |a   |b   |\n+---+----------------------+----+----+\n|1  |[{1, 2}, NULL, {3, 4}]|1   |2   |\n|1  |[{1, 2}, NULL, {3, 4}]|NULL|NULL|\n|1  |[{1, 2}, NULL, {3, 4}]|3   |4   |\n|2  |[]                    |NULL|NULL|\n|3  |NULL                  |NULL|NULL|\n+---+----------------------+----+----+"], "Parameters": [["col Column or column name", "input column of values to explode."]], "Returns": [["Column", "generator expression with the inline exploded result."]], "Category": ["Functions"], "index": 481}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.posexplode.html#pyspark.sql.functions.posexplode"], "Title": ["posexplode"], "Feature": ["posexplode"], "Description": "Returns a new row for each element with position in the given array or map.\nUses the default column nameposfor position, andcolfor elements in the\narray andkeyandvaluefor elements in the map unless specified otherwise.\nSee alsopyspark.sql.functions.explode()pyspark.sql.functions.explode_outer()pyspark.sql.functions.posexplode_outer()pyspark.sql.functions.inline()pyspark.sql.functions.inline_outer()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql('SELECT * FROM VALUES (1,ARRAY(1,2,3,NULL)), (2,ARRAY()), (3,NULL) AS t(i,a)')\n>>> df.show()\n+---+---------------+\n|  i|              a|\n+---+---------------+\n|  1|[1, 2, 3, NULL]|\n|  2|             []|\n|  3|           NULL|\n+---+---------------+", ">>> df.select('*', sf.posexplode('a')).show()\n+---+---------------+---+----+\n|  i|              a|pos| col|\n+---+---------------+---+----+\n|  1|[1, 2, 3, NULL]|  0|   1|\n|  1|[1, 2, 3, NULL]|  1|   2|\n|  1|[1, 2, 3, NULL]|  2|   3|\n|  1|[1, 2, 3, NULL]|  3|NULL|\n+---+---------------+---+----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql('SELECT * FROM VALUES (1,MAP(1,2,3,4,5,NULL)), (2,MAP()), (3,NULL) AS t(i,m)')\n>>> df.show(truncate=False)\n+---+---------------------------+\n|i  |m                          |\n+---+---------------------------+\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|\n|2  |{}                         |\n|3  |NULL                       |\n+---+---------------------------+", ">>> df.select('*', sf.posexplode('m')).show(truncate=False)\n+---+---------------------------+---+---+-----+\n|i  |m                          |pos|key|value|\n+---+---------------------------+---+---+-----+\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|0  |1  |2    |\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|1  |3  |4    |\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|2  |5  |NULL |\n+---+---------------------------+---+---+-----+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "one row per array item or map key value including positions as a separate column."]], "Category": ["Functions"], "index": 482}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.posexplode_outer.html#pyspark.sql.functions.posexplode_outer"], "Title": ["posexplode_outer"], "Feature": ["posexplode_outer"], "Description": "Returns a new row for each element with position in the given array or map.\nUnlike posexplode, if the array/map is null or empty then the row (null, null) is produced.\nUses the default column nameposfor position, andcolfor elements in the\narray andkeyandvaluefor elements in the map unless specified otherwise.\nSee alsopyspark.sql.functions.explode()pyspark.sql.functions.explode_outer()pyspark.sql.functions.posexplode()pyspark.sql.functions.inline()pyspark.sql.functions.inline_outer()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.sql('SELECT * FROM VALUES (1,ARRAY(1,2,3,NULL)), (2,ARRAY()), (3,NULL) AS t(i,a)')\n>>> df.select('*', sf.posexplode_outer('a')).show()\n+---+---------------+----+----+\n|  i|              a| pos| col|\n+---+---------------+----+----+\n|  1|[1, 2, 3, NULL]|   0|   1|\n|  1|[1, 2, 3, NULL]|   1|   2|\n|  1|[1, 2, 3, NULL]|   2|   3|\n|  1|[1, 2, 3, NULL]|   3|NULL|\n|  2|             []|NULL|NULL|\n|  3|           NULL|NULL|NULL|\n+---+---------------+----+----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.sql('SELECT * FROM VALUES (1,MAP(1,2,3,4,5,NULL)), (2,MAP()), (3,NULL) AS t(i,m)')\n>>> df.select('*', sf.posexplode_outer('m')).show(truncate=False)\n+---+---------------------------+----+----+-----+\n|i  |m                          |pos |key |value|\n+---+---------------------------+----+----+-----+\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|0   |1   |2    |\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|1   |3   |4    |\n|1  |{1 -> 2, 3 -> 4, 5 -> NULL}|2   |5   |NULL |\n|2  |{}                         |NULL|NULL|NULL |\n|3  |NULL                       |NULL|NULL|NULL |\n+---+---------------------------+----+----+-----+"], "Parameters": [["col Column or column name", "target column to work on."]], "Returns": [["Column", "one row per array item or map key value including positions as a separate column."]], "Category": ["Functions"], "index": 483}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.stack.html#pyspark.sql.functions.stack"], "Title": ["stack"], "Feature": ["stack"], "Description": "Separatescol1, …,colkintonrows. Uses column names col0, col1, etc. by default\nunless specified otherwise.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1, 2, 3)], ['a', 'b', 'c'])\n>>> df.select('*', sf.stack(sf.lit(2), df.a, df.b, 'c')).show()\n+---+---+---+----+----+\n|  a|  b|  c|col0|col1|\n+---+---+---+----+----+\n|  1|  2|  3|   1|   2|\n|  1|  2|  3|   3|NULL|\n+---+---+---+----+----+", ">>> df.select('*', sf.stack(sf.lit(2), df.a, df.b, 'c').alias('x', 'y')).show()\n+---+---+---+---+----+\n|  a|  b|  c|  x|   y|\n+---+---+---+---+----+\n|  1|  2|  3|  1|   2|\n|  1|  2|  3|  3|NULL|\n+---+---+---+---+----+", ">>> df.select('*', sf.stack(sf.lit(3), df.a, df.b, 'c')).show()\n+---+---+---+----+\n|  a|  b|  c|col0|\n+---+---+---+----+\n|  1|  2|  3|   1|\n|  1|  2|  3|   2|\n|  1|  2|  3|   3|\n+---+---+---+----+", ">>> df.select('*', sf.stack(sf.lit(4), df.a, df.b, 'c')).show()\n+---+---+---+----+\n|  a|  b|  c|col0|\n+---+---+---+----+\n|  1|  2|  3|   1|\n|  1|  2|  3|   2|\n|  1|  2|  3|   3|\n|  1|  2|  3|NULL|\n+---+---+---+----+"], "Parameters": [["cols Column or column name", "the first element should be a literal int for the number of rows to be separated,\nand the remaining are input elements to be separated."]], "Returns": [], "Category": ["Functions"], "index": 484}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.partitioning.years.html#pyspark.sql.functions.partitioning.years"], "Title": ["partitioning.years"], "Feature": ["partitioning.years"], "Description": "Partition transform function: A transform for timestamps and dates\nto partition data into years.\nNotes\nThis function can be used only in combination withpartitionedBy()method of theDataFrameWriterV2.", "Examples": [">>> df.writeTo(\"catalog.db.table\").partitionedBy(  \n...     partitioning.years(\"ts\")\n... ).createOrReplace()"], "Parameters": [["col Column or str", "target date or timestamp column to work on."]], "Returns": [["Column", "data partitioned by years."]], "Category": ["Functions"], "index": 485}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.partitioning.months.html#pyspark.sql.functions.partitioning.months"], "Title": ["partitioning.months"], "Feature": ["partitioning.months"], "Description": "Partition transform function: A transform for timestamps and dates\nto partition data into months.\nNotes\nThis function can be used only in combination withpartitionedBy()method of theDataFrameWriterV2.", "Examples": [">>> df.writeTo(\"catalog.db.table\").partitionedBy(\n...     partitioning.months(\"ts\")\n... ).createOrReplace()"], "Parameters": [["col Column or str", "target date or timestamp column to work on."]], "Returns": [["Column", "data partitioned by months."]], "Category": ["Functions"], "index": 486}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.partitioning.days.html#pyspark.sql.functions.partitioning.days"], "Title": ["partitioning.days"], "Feature": ["partitioning.days"], "Description": "Partition transform function: A transform for timestamps and dates\nto partition data into days.\nNotes\nThis function can be used only in combination withpartitionedBy()method of theDataFrameWriterV2.", "Examples": [">>> df.writeTo(\"catalog.db.table\").partitionedBy(  \n...     partitioning.days(\"ts\")\n... ).createOrReplace()"], "Parameters": [["col Column or str", "target date or timestamp column to work on."]], "Returns": [["Column", "data partitioned by days."]], "Category": ["Functions"], "index": 487}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.partitioning.hours.html#pyspark.sql.functions.partitioning.hours"], "Title": ["partitioning.hours"], "Feature": ["partitioning.hours"], "Description": "Partition transform function: A transform for timestamps\nto partition data into hours.\nNotes\nThis function can be used only in combination withpartitionedBy()method of theDataFrameWriterV2.", "Examples": [">>> df.writeTo(\"catalog.db.table\").partitionedBy(   \n...     partitioning.hours(\"ts\")\n... ).createOrReplace()"], "Parameters": [["col Column or str", "target date or timestamp column to work on."]], "Returns": [["Column", "data partitioned by hours."]], "Category": ["Functions"], "index": 488}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.partitioning.bucket.html#pyspark.sql.functions.partitioning.bucket"], "Title": ["partitioning.bucket"], "Feature": ["partitioning.bucket"], "Description": "Partition transform function: A transform for any type that partitions\nby a hash of the input column.\nNotes\nThis function can be used only in combination withpartitionedBy()method of theDataFrameWriterV2.", "Examples": [">>> df.writeTo(\"catalog.db.table\").partitionedBy(  \n...     partitioning.bucket(42, \"ts\")\n... ).createOrReplace()"], "Parameters": [["numBuckets Column or int", "the number of buckets"], ["col Column or str", "target date or timestamp column to work on."]], "Returns": [["Column", "data partitioned by given columns."]], "Category": ["Functions"], "index": 489}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.persist.html#pyspark.sql.DataFrame.persist"], "Title": ["DataFrame.persist"], "Feature": ["DataFrame.persist"], "Description": "Sets the storage level to persist the contents of theDataFrameacross\noperations after the first time it is computed. This can only be used to assign\na new storage level if theDataFramedoes not have a storage level set yet.\nIf no storage level is specified defaults to (MEMORY_AND_DISK_DESER)\nNotes\nThe default storage level has changed toMEMORY_AND_DISK_DESERto match Scala in 3.0.", "Examples": [">>> df = spark.range(1)\n>>> df.persist()\nDataFrame[id: bigint]", ">>> df.explain()\n== Physical Plan ==\nInMemoryTableScan ...", ">>> from pyspark.storagelevel import StorageLevel\n>>> df.persist(StorageLevel.DISK_ONLY)\nDataFrame[id: bigint]"], "Parameters": [["storageLevel StorageLevel", "Storage level to set for persistence. Default is MEMORY_AND_DISK_DESER."]], "Returns": [["DataFrame", "Persisted DataFrame."]], "Category": ["DataFrame"], "index": 490}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.from_csv.html#pyspark.sql.functions.from_csv"], "Title": ["from_csv"], "Feature": ["from_csv"], "Description": "CSV Function: Parses a column containing a CSV string into a row with the specified schema.\nReturnsnullif the string cannot be parsed.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> data = [(\"1,2,3\",)]\n>>> df = spark.createDataFrame(data, (\"value\",))\n>>> df.select(sf.from_csv(df.value, \"a INT, b INT, c INT\")).show()\n+---------------+\n|from_csv(value)|\n+---------------+\n|      {1, 2, 3}|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> data = [(\"1,2,3\",)]\n>>> value = data[0][0]\n>>> df.select(sf.from_csv(df.value, sf.schema_of_csv(value))).show()\n+---------------+\n|from_csv(value)|\n+---------------+\n|      {1, 2, 3}|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> data = [(\"   abc\",)]\n>>> df = spark.createDataFrame(data, (\"value\",))\n>>> options = {'ignoreLeadingWhiteSpace': True}\n>>> df.select(sf.from_csv(df.value, \"s string\", options)).show()\n+---------------+\n|from_csv(value)|\n+---------------+\n|          {abc}|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> data = [(\"1,2,\",)]\n>>> df = spark.createDataFrame(data, (\"value\",))\n>>> df.select(sf.from_csv(df.value, \"a INT, b INT, c INT\")).show()\n+---------------+\n|from_csv(value)|\n+---------------+\n|   {1, 2, NULL}|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> data = [(\"1;2;3\",)]\n>>> df = spark.createDataFrame(data, (\"value\",))\n>>> options = {'delimiter': ';'}\n>>> df.select(sf.from_csv(df.value, \"a INT, b INT, c INT\", options)).show()\n+---------------+\n|from_csv(value)|\n+---------------+\n|      {1, 2, 3}|\n+---------------+"], "Parameters": [["col Column or str", "A column or column name in CSV format."], ["schema Column or str", "A column, or Python string literal with schema in DDL format, to use when parsing the CSV column."], ["options dict, optional", "Options to control parsing. Accepts the same options as the CSV datasource.\nSee Data Source Option for the version you use."]], "Returns": [["Column", "A column of parsed CSV values."]], "Category": ["Functions"], "index": 491}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.schema_of_csv.html#pyspark.sql.functions.schema_of_csv"], "Title": ["schema_of_csv"], "Feature": ["schema_of_csv"], "Description": "CSV Function: Parses a CSV string and infers its schema in DDL format.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.range(1)\n>>> df.select(sf.schema_of_csv(sf.lit('1|a|true'), {'sep':'|'})).show(truncate=False)\n+-------------------------------------------+\n|schema_of_csv(1|a|true)                    |\n+-------------------------------------------+\n|STRUCT<_c0: INT, _c1: STRING, _c2: BOOLEAN>|\n+-------------------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.range(1)\n>>> df.select(sf.schema_of_csv(sf.lit('1||true'), {'sep':'|'})).show(truncate=False)\n+-------------------------------------------+\n|schema_of_csv(1||true)                     |\n+-------------------------------------------+\n|STRUCT<_c0: INT, _c1: STRING, _c2: BOOLEAN>|\n+-------------------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.range(1)\n>>> df.select(sf.schema_of_csv(sf.lit('1;a;true'), {'sep':';'})).show(truncate=False)\n+-------------------------------------------+\n|schema_of_csv(1;a;true)                    |\n+-------------------------------------------+\n|STRUCT<_c0: INT, _c1: STRING, _c2: BOOLEAN>|\n+-------------------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.range(1)\n>>> df.select(sf.schema_of_csv(sf.lit('\"1\",\"a\",\"true\"'), {'sep':','})).show(truncate=False)\n+-------------------------------------------+\n|schema_of_csv(\"1\",\"a\",\"true\")              |\n+-------------------------------------------+\n|STRUCT<_c0: INT, _c1: STRING, _c2: BOOLEAN>|\n+-------------------------------------------+"], "Parameters": [["csv Column or str", "A CSV string or a foldable string column containing a CSV string."], ["options dict, optional", "Options to control parsing. Accepts the same options as the CSV datasource.\nSee Data Source Option for the version you use."]], "Returns": [["Column", "A string representation of a StructType parsed from the given CSV."]], "Category": ["Functions"], "index": 492}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_csv.html#pyspark.sql.functions.to_csv"], "Title": ["to_csv"], "Feature": ["to_csv"], "Description": "CSV Function: Converts a column containing aStructTypeinto a CSV string.\nThrows an exception, in the case of an unsupported type.", "Examples": [">>> from pyspark.sql import Row, functions as sf\n>>> data = [(1, Row(age=2, name='Alice'))]\n>>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n>>> df.select(sf.to_csv(df.value)).show()\n+-------------+\n|to_csv(value)|\n+-------------+\n|      2,Alice|\n+-------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> data = [(1, Row(age=2, name='Alice', scores=[100, 200, 300]))]\n>>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n>>> df.select(sf.to_csv(df.value)).show(truncate=False)\n+-------------------------+\n|to_csv(value)            |\n+-------------------------+\n|2,Alice,\"[100, 200, 300]\"|\n+-------------------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n>>> data = [(1, Row(age=None, name='Alice'))]\n>>> schema = StructType([\n...   StructField(\"key\", IntegerType(), True),\n...   StructField(\"value\", StructType([\n...     StructField(\"age\", IntegerType(), True),\n...     StructField(\"name\", StringType(), True)\n...   ]), True)\n... ])\n>>> df = spark.createDataFrame(data, schema)\n>>> df.select(sf.to_csv(df.value)).show()\n+-------------+\n|to_csv(value)|\n+-------------+\n|       ,Alice|\n+-------------+", ">>> from pyspark.sql import Row, functions as sf\n>>> data = [(1, Row(age=2, name='Alice', isStudent=True))]\n>>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n>>> df.select(sf.to_csv(df.value)).show()\n+-------------+\n|to_csv(value)|\n+-------------+\n| 2,Alice,true|\n+-------------+"], "Parameters": [["col Column or str", "Name of column containing a struct."], ["options: dict, optional", "Options to control converting. Accepts the same options as the CSV datasource.\nSee Data Source Option for the version you use."]], "Returns": [["Column", "A CSV string converted from the given StructType ."]], "Category": ["Functions"], "index": 493}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.from_json.html#pyspark.sql.functions.from_json"], "Title": ["from_json"], "Feature": ["from_json"], "Description": "Parses a column containing a JSON string into aMapTypewithStringTypeas keys type,StructTypeorArrayTypewith\nthe specified schema. Returnsnull, in the case of an unparsable string.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> from pyspark.sql.types import StructType, StructField, IntegerType\n>>> schema = StructType([StructField(\"a\", IntegerType())])\n>>> df = spark.createDataFrame([(1, '''{\"a\": 1}''')], (\"key\", \"value\"))\n>>> df.select(sf.from_json(df.value, schema).alias(\"json\")).show()\n+----+\n|json|\n+----+\n| {1}|\n+----+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1, '''{\"a\": 1}''')], (\"key\", \"value\"))\n>>> df.select(sf.from_json(df.value, \"a INT\").alias(\"json\")).show()\n+----+\n|json|\n+----+\n| {1}|\n+----+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1, '''{\"a\": 1}''')], (\"key\", \"value\"))\n>>> df.select(sf.from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).show()\n+--------+\n|    json|\n+--------+\n|{a -> 1}|\n+--------+", ">>> import pyspark.sql.functions as sf\n>>> from pyspark.sql.types import ArrayType, StructType, StructField, IntegerType\n>>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n>>> df = spark.createDataFrame([(1, '''[{\"a\": 1}]''')], (\"key\", \"value\"))\n>>> df.select(sf.from_json(df.value, schema).alias(\"json\")).show()\n+-----+\n| json|\n+-----+\n|[{1}]|\n+-----+", ">>> import pyspark.sql.functions as sf\n>>> from pyspark.sql.types import ArrayType, IntegerType\n>>> schema = ArrayType(IntegerType())\n>>> df = spark.createDataFrame([(1, '''[1, 2, 3]''')], (\"key\", \"value\"))\n>>> df.select(sf.from_json(df.value, schema).alias(\"json\")).show()\n+---------+\n|     json|\n+---------+\n|[1, 2, 3]|\n+---------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1, '''{a:123}'''), (2, '''{\"a\":456}''')], (\"key\", \"value\"))\n>>> parsed1 = sf.from_json(df.value, \"a INT\")\n>>> parsed2 = sf.from_json(df.value, \"a INT\", {\"allowUnquotedFieldNames\": \"true\"})\n>>> df.select(\"value\", parsed1, parsed2).show()\n+---------+----------------+----------------+\n|    value|from_json(value)|from_json(value)|\n+---------+----------------+----------------+\n|  {a:123}|          {NULL}|           {123}|\n|{\"a\":456}|           {456}|           {456}|\n+---------+----------------+----------------+"], "Parameters": [["col Column or str", "a column or column name in JSON format"], ["schema DataType or str", "a StructType, ArrayType of StructType or Python string literal with a DDL-formatted string\nto use when parsing the json column"], ["options dict, optional", "options to control parsing. accepts the same options as the json datasource.\nSee Data Source Option for the version you use."]], "Returns": [["Column", "a new column of complex type from given JSON object."]], "Category": ["Functions"], "index": 494}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.get_json_object.html#pyspark.sql.functions.get_json_object"], "Title": ["get_json_object"], "Feature": ["get_json_object"], "Description": "Extracts json object from a json string based on jsonpathspecified, and returns json string\nof the extracted json object. It will return null if the input json string is invalid.", "Examples": [">>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n>>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n>>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n[Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]"], "Parameters": [["col Column or str", "string column in json format"], ["path str", "path to the json object to extract"]], "Returns": [["Column", "string representation of given JSON object value."]], "Category": ["Functions"], "index": 495}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.json_array_length.html#pyspark.sql.functions.json_array_length"], "Title": ["json_array_length"], "Feature": ["json_array_length"], "Description": "Returns the number of elements in the outermost JSON array.NULLis returned in case of\nany other valid JSON string,NULLor an invalid JSON.", "Examples": [">>> df = spark.createDataFrame([(None,), ('[1, 2, 3]',), ('[]',)], ['data'])\n>>> df.select(json_array_length(df.data).alias('r')).collect()\n[Row(r=None), Row(r=3), Row(r=0)]"], "Parameters": [["col: :class:`~pyspark.sql.Column` or str", "target column to compute on."]], "Returns": [["Column", "length of json array."]], "Category": ["Functions"], "index": 496}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.json_object_keys.html#pyspark.sql.functions.json_object_keys"], "Title": ["json_object_keys"], "Feature": ["json_object_keys"], "Description": "Returns all the keys of the outermost JSON object as an array. If a valid JSON object is\ngiven, all the keys of the outermost object will be returned as an array. If it is any\nother valid JSON string, an invalid JSON string or an empty string, the function returns null.", "Examples": [">>> df = spark.createDataFrame([(None,), ('{}',), ('{\"key1\":1, \"key2\":2}',)], ['data'])\n>>> df.select(json_object_keys(df.data).alias('r')).collect()\n[Row(r=None), Row(r=[]), Row(r=['key1', 'key2'])]"], "Parameters": [["col: :class:`~pyspark.sql.Column` or str", "target column to compute on."]], "Returns": [["Column", "all the keys of the outermost JSON object."]], "Category": ["Functions"], "index": 497}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.json_tuple.html#pyspark.sql.functions.json_tuple"], "Title": ["json_tuple"], "Feature": ["json_tuple"], "Description": "Creates a new row for a json column according to the given field names.", "Examples": [">>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n>>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n>>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n[Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]"], "Parameters": [["col Column or str", "string column in json format"], ["fields str", "a field or fields to extract"]], "Returns": [["Column", "a new row for each given field value from json object"]], "Category": ["Functions"], "index": 498}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.schema_of_json.html#pyspark.sql.functions.schema_of_json"], "Title": ["schema_of_json"], "Feature": ["schema_of_json"], "Description": "Parses a JSON string and infers its schema in DDL format.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> parsed1 = sf.schema_of_json(sf.lit('{\"a\": 0}'))\n>>> parsed2 = sf.schema_of_json('{a: 1}', {'allowUnquotedFieldNames':'true'})\n>>> spark.range(1).select(parsed1, parsed2).show()\n+------------------------+----------------------+\n|schema_of_json({\"a\": 0})|schema_of_json({a: 1})|\n+------------------------+----------------------+\n|       STRUCT<a: BIGINT>|     STRUCT<a: BIGINT>|\n+------------------------+----------------------+"], "Parameters": [["json Column or str", "a JSON string or a foldable string column containing a JSON string."], ["options dict, optional", "options to control parsing. accepts the same options as the JSON datasource.\nSee Data Source Option for the version you use. Changed in version 3.0.0: It accepts options parameter to control schema inferring."]], "Returns": [["Column", "a string representation of a StructType parsed from given JSON."]], "Category": ["Functions"], "index": 499}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_json.html#pyspark.sql.functions.to_json"], "Title": ["to_json"], "Feature": ["to_json"], "Description": "Converts a column containing aStructType,ArrayTypeor aMapTypeinto a JSON string. Throws an exception, in the case of an unsupported type.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> from pyspark.sql import Row\n>>> data = [(1, Row(age=2, name='Alice'))]\n>>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n>>> df.select(sf.to_json(df.value).alias(\"json\")).show(truncate=False)\n+------------------------+\n|json                    |\n+------------------------+\n|{\"age\":2,\"name\":\"Alice\"}|\n+------------------------+", ">>> import pyspark.sql.functions as sf\n>>> from pyspark.sql import Row\n>>> data = [(1, [Row(age=2, name='Alice'), Row(age=3, name='Bob')])]\n>>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n>>> df.select(sf.to_json(df.value).alias(\"json\")).show(truncate=False)\n+-------------------------------------------------+\n|json                                             |\n+-------------------------------------------------+\n|[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]|\n+-------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1, {\"name\": \"Alice\"})], (\"key\", \"value\"))\n>>> df.select(sf.to_json(df.value).alias(\"json\")).show(truncate=False)\n+----------------+\n|json            |\n+----------------+\n|{\"name\":\"Alice\"}|\n+----------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1, [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}])], (\"key\", \"value\"))\n>>> df.select(sf.to_json(df.value).alias(\"json\")).show(truncate=False)\n+---------------------------------+\n|json                             |\n+---------------------------------+\n|[{\"name\":\"Alice\"},{\"name\":\"Bob\"}]|\n+---------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(1, [\"Alice\", \"Bob\"])], (\"key\", \"value\"))\n>>> df.select(sf.to_json(df.value).alias(\"json\")).show(truncate=False)\n+---------------+\n|json           |\n+---------------+\n|[\"Alice\",\"Bob\"]|\n+---------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.sql(\"SELECT (DATE('2022-02-22'), 1) AS date\")\n>>> json1 = sf.to_json(df.date)\n>>> json2 = sf.to_json(df.date, {\"dateFormat\": \"yyyy/MM/dd\"})\n>>> df.select(\"date\", json1, json2).show(truncate=False)\n+---------------+------------------------------+------------------------------+\n|date           |to_json(date)                 |to_json(date)                 |\n+---------------+------------------------------+------------------------------+\n|{2022-02-22, 1}|{\"col1\":\"2022-02-22\",\"col2\":1}|{\"col1\":\"2022/02/22\",\"col2\":1}|\n+---------------+------------------------------+------------------------------+"], "Parameters": [["col Column or str", "name of column containing a struct, an array or a map."], ["options dict, optional", "options to control converting. accepts the same options as the JSON datasource.\nSee Data Source Option for the version you use.\nAdditionally the function supports the pretty option which enables\npretty JSON generation."]], "Returns": [["Column", "JSON object as string column."]], "Category": ["Functions"], "index": 500}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.plot.html#pyspark.sql.DataFrame.plot"], "Title": ["DataFrame.plot"], "Feature": ["DataFrame.plot"], "Description": "Returns aplot.core.PySparkPlotAccessorfor plotting functions.\nNotes\nThis API is experimental.\nIt provides two ways to create plots:\n1. Chaining style (e.g.,df.plot.line(…)).\n2. Explicit style (e.g.,df.plot(kind=”line”, …)).", "Examples": [">>> data = [(\"A\", 10, 1.5), (\"B\", 30, 2.5), (\"C\", 20, 3.5)]\n>>> columns = [\"category\", \"int_val\", \"float_val\"]\n>>> df = spark.createDataFrame(data, columns)\n>>> type(df.plot)\n<class 'pyspark.sql.plot.core.PySparkPlotAccessor'>\n>>> df.plot.line(x=\"category\", y=[\"int_val\", \"float_val\"])  \n>>> df.plot(kind=\"line\", x=\"category\", y=[\"int_val\", \"float_val\"])"], "Parameters": [], "Returns": [["plot.core.PySparkPlotAccessor", ""]], "Category": ["DataFrame"], "index": 501}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.is_variant_null.html#pyspark.sql.functions.is_variant_null"], "Title": ["is_variant_null"], "Feature": ["is_variant_null"], "Description": "Check if a variant value is a variant null. Returns true if and only if the input is a variant\nnull and false otherwise (including in the case of SQL NULL).", "Examples": [">>> df = spark.createDataFrame([ {'json': '''{ \"a\" : 1 }'''} ])\n>>> df.select(is_variant_null(parse_json(df.json)).alias(\"r\")).collect()\n[Row(r=False)]"], "Parameters": [["v Column or str", "a variant column or column name"]], "Returns": [["Column", "a boolean column indicating whether the variant value is a variant null"]], "Category": ["Functions"], "index": 502}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.parse_json.html#pyspark.sql.functions.parse_json"], "Title": ["parse_json"], "Feature": ["parse_json"], "Description": "Parses a column containing a JSON string into aVariantType. Throws exception if a\nstring represents an invalid JSON value.", "Examples": [">>> df = spark.createDataFrame([ {'json': '''{ \"a\" : 1 }'''} ])\n>>> df.select(to_json(parse_json(df.json))).collect()\n[Row(to_json(parse_json(json))='{\"a\":1}')]"], "Parameters": [["col Column or str", "a column or column name JSON formatted strings"]], "Returns": [["Column", "a new column of VariantType."]], "Category": ["Functions"], "index": 503}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.schema_of_variant.html#pyspark.sql.functions.schema_of_variant"], "Title": ["schema_of_variant"], "Feature": ["schema_of_variant"], "Description": "Returns schema in the SQL format of a variant.", "Examples": [">>> df = spark.createDataFrame([ {'json': '''{ \"a\" : 1 }'''} ])\n>>> df.select(schema_of_variant(parse_json(df.json)).alias(\"r\")).collect()\n[Row(r='OBJECT<a: BIGINT>')]"], "Parameters": [["v Column or str", "a variant column or column name"]], "Returns": [["Column", "a string column representing the variant schema"]], "Category": ["Functions"], "index": 504}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.schema_of_variant_agg.html#pyspark.sql.functions.schema_of_variant_agg"], "Title": ["schema_of_variant_agg"], "Feature": ["schema_of_variant_agg"], "Description": "Returns the merged schema in the SQL format of a variant column.", "Examples": [">>> df = spark.createDataFrame([ {'json': '''{ \"a\" : 1 }'''} ])\n>>> df.select(schema_of_variant_agg(parse_json(df.json)).alias(\"r\")).collect()\n[Row(r='OBJECT<a: BIGINT>')]"], "Parameters": [["v Column or str", "a variant column or column name"]], "Returns": [["Column", "a string column representing the variant schema"]], "Category": ["Functions"], "index": 505}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_variant_get.html#pyspark.sql.functions.try_variant_get"], "Title": ["try_variant_get"], "Feature": ["try_variant_get"], "Description": "Extracts a sub-variant fromvaccording topath, and then cast the sub-variant totargetType. Returns null if the path does not exist or the cast fails.", "Examples": [">>> df = spark.createDataFrame([ {'json': '''{ \"a\" : 1 }''', 'path': '$.a'} ])\n>>> df.select(try_variant_get(parse_json(df.json), \"$.a\", \"int\").alias(\"r\")).collect()\n[Row(r=1)]\n>>> df.select(try_variant_get(parse_json(df.json), \"$.b\", \"int\").alias(\"r\")).collect()\n[Row(r=None)]\n>>> df.select(try_variant_get(parse_json(df.json), \"$.a\", \"binary\").alias(\"r\")).collect()\n[Row(r=None)]\n>>> df.select(try_variant_get(parse_json(df.json), df.path, \"int\").alias(\"r\")).collect()\n[Row(r=1)]"], "Parameters": [["v Column or str", "a variant column or column name"], ["path Column or str", "a column containing the extraction path strings or a string representing the extraction\npath. A valid path should start with $ and is followed by zero or more segments like [123] , .name , [‘name’] , or [“name”] ."], ["targetType str", "the target data type to cast into, in a DDL-formatted string"]], "Returns": [["Column", "a column of targetType representing the extracted result"]], "Category": ["Functions"], "index": 506}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.variant_get.html#pyspark.sql.functions.variant_get"], "Title": ["variant_get"], "Feature": ["variant_get"], "Description": "Extracts a sub-variant fromvaccording topath, and then cast the sub-variant totargetType. Returns null if the path does not exist. Throws an exception if the cast fails.", "Examples": [">>> df = spark.createDataFrame([ {'json': '''{ \"a\" : 1 }''', 'path': '$.a'} ])\n>>> df.select(variant_get(parse_json(df.json), \"$.a\", \"int\").alias(\"r\")).collect()\n[Row(r=1)]\n>>> df.select(variant_get(parse_json(df.json), \"$.b\", \"int\").alias(\"r\")).collect()\n[Row(r=None)]\n>>> df.select(variant_get(parse_json(df.json), df.path, \"int\").alias(\"r\")).collect()\n[Row(r=1)]"], "Parameters": [["v Column or str", "a variant column or column name"], ["path Column or str", "a column containing the extraction path strings or a string representing the extraction\npath. A valid path should start with $ and is followed by zero or more segments like [123] , .name , [‘name’] , or [“name”] ."], ["targetType str", "the target data type to cast into, in a DDL-formatted string"]], "Returns": [["Column", "a column of targetType representing the extracted result"]], "Category": ["Functions"], "index": 507}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_parse_json.html#pyspark.sql.functions.try_parse_json"], "Title": ["try_parse_json"], "Feature": ["try_parse_json"], "Description": "Parses a column containing a JSON string into aVariantType. Returns None if a string\ncontains an invalid JSON value.", "Examples": [">>> df = spark.createDataFrame([ {'json': '''{ \"a\" : 1 }'''}, {'json': '''{a : 1}'''} ])\n>>> df.select(to_json(try_parse_json(df.json))).collect()\n[Row(to_json(try_parse_json(json))='{\"a\":1}'), Row(to_json(try_parse_json(json))=None)]"], "Parameters": [["col Column or str", "a column or column name JSON formatted strings"]], "Returns": [["Column", "a new column of VariantType."]], "Category": ["Functions"], "index": 508}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_variant_object.html#pyspark.sql.functions.to_variant_object"], "Title": ["to_variant_object"], "Feature": ["to_variant_object"], "Description": "Converts a column containing nested inputs (array/map/struct) into a variants where maps and\nstructs are converted to variant objects which are unordered unlike SQL structs. Input maps can\nonly have string keys.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.types import ArrayType, StructType, StructField, StringType, MapType\n>>> schema = StructType([\n...     StructField(\"i\", StringType(), True),\n...     StructField(\"v\", ArrayType(StructType([\n...         StructField(\"a\", MapType(StringType(), StringType()), True)\n...     ]), True))\n... ])\n>>> data = [(\"1\", [{\"a\": {\"b\": 2}}])]\n>>> df = spark.createDataFrame(data, schema)\n>>> df.select(sf.to_variant_object(df.v))\nDataFrame[to_variant_object(v): variant]\n>>> df.select(sf.to_variant_object(df.v)).show(truncate=False)\n+--------------------+\n|to_variant_object(v)|\n+--------------------+\n|[{\"a\":{\"b\":\"2\"}}]   |\n+--------------------+"], "Parameters": [["col Column or str", "a column with a nested schema or column name"]], "Returns": [["Column", "a new column of VariantType."]], "Category": ["Functions"], "index": 509}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.from_xml.html#pyspark.sql.functions.from_xml"], "Title": ["from_xml"], "Feature": ["from_xml"], "Description": "Parses a column containing a XML string to a row with\nthe specified schema. Returnsnull, in the case of an unparsable string.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> data = [(1, '''<p><a>1</a></p>''')]\n>>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n... # Define the schema using a DDL-formatted string\n>>> schema = \"STRUCT<a: BIGINT>\"\n... # Parse the XML column using the DDL-formatted schema\n>>> df.select(sf.from_xml(df.value, schema).alias(\"xml\")).collect()\n[Row(xml=Row(a=1))]", ">>> import pyspark.sql.functions as sf\n>>> from pyspark.sql.types import StructType, LongType\n>>> data = [(1, '''<p><a>1</a></p>''')]\n>>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n>>> schema = StructType().add(\"a\", LongType())\n>>> df.select(sf.from_xml(df.value, schema)).show()\n+---------------+\n|from_xml(value)|\n+---------------+\n|            {1}|\n+---------------+", ">>> import pyspark.sql.functions as sf\n>>> data = [(1, '<p><a>1</a><a>2</a></p>')]\n>>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n... # Define the schema with an Array type\n>>> schema = \"STRUCT<a: ARRAY<BIGINT>>\"\n... # Parse the XML column using the schema with an Array\n>>> df.select(sf.from_xml(df.value, schema).alias(\"xml\")).collect()\n[Row(xml=Row(a=[1, 2]))]", ">>> import pyspark.sql.functions as sf\n>>> # Sample data with an XML column\n... data = [(1, '<p><a>1</a><a>2</a></p>')]\n>>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n... # Generate the schema from an example XML value\n>>> schema = sf.schema_of_xml(sf.lit(data[0][1]))\n... # Parse the XML column using the generated schema\n>>> df.select(sf.from_xml(df.value, schema).alias(\"xml\")).collect()\n[Row(xml=Row(a=[1, 2]))]"], "Parameters": [["col Column or str", "a column or column name in XML format"], ["schema StructType , Column or str", "a StructType, Column or Python string literal with a DDL-formatted string\nto use when parsing the Xml column"], ["options dict, optional", "options to control parsing. accepts the same options as the Xml datasource.\nSee Data Source Option for the version you use."]], "Returns": [["Column", "a new column of complex type from given XML object."]], "Category": ["Functions"], "index": 510}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.schema_of_xml.html#pyspark.sql.functions.schema_of_xml"], "Title": ["schema_of_xml"], "Feature": ["schema_of_xml"], "Description": "Parses a XML string and infers its schema in DDL format.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.range(1)\n>>> df.select(sf.schema_of_xml(sf.lit('<p><a>1</a></p>')).alias(\"xml\")).collect()\n[Row(xml='STRUCT<a: BIGINT>')]", ">>> from pyspark.sql import functions as sf\n>>> df.select(sf.schema_of_xml(sf.lit('<p><a>1</a><a>2</a></p>')).alias(\"xml\")).collect()\n[Row(xml='STRUCT<a: ARRAY<BIGINT>>')]", ">>> from pyspark.sql import functions as sf\n>>> schema = sf.schema_of_xml('<p><a attr=\"2\">1</a></p>', {'excludeAttribute':'true'})\n>>> df.select(schema.alias(\"xml\")).collect()\n[Row(xml='STRUCT<a: BIGINT>')]", ">>> from pyspark.sql import functions as sf\n>>> df.select(\n...     sf.schema_of_xml(\n...         sf.lit('<root><person><name>Alice</name><age>30</age></person></root>')\n...     ).alias(\"xml\")\n... ).collect()\n[Row(xml='STRUCT<person: STRUCT<age: BIGINT, name: STRING>>')]", ">>> from pyspark.sql import functions as sf\n>>> df.select(\n...     sf.schema_of_xml(\n...         sf.lit('<data><values><value>1</value><value>2</value></values></data>')\n...     ).alias(\"xml\")\n... ).collect()\n[Row(xml='STRUCT<values: STRUCT<value: ARRAY<BIGINT>>>')]"], "Parameters": [["xml Column or str", "a XML string or a foldable string column containing a XML string."], ["options dict, optional", "options to control parsing. accepts the same options as the XML datasource.\nSee Data Source Option for the version you use."]], "Returns": [["Column", "a string representation of a StructType parsed from given XML."]], "Category": ["Functions"], "index": 511}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.printSchema.html#pyspark.sql.DataFrame.printSchema"], "Title": ["DataFrame.printSchema"], "Feature": ["DataFrame.printSchema"], "Description": "Prints out the schema in the tree format.\nOptionally allows to specify how many levels to print if schema is nested.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.printSchema()\nroot\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)", ">>> df = spark.createDataFrame([(1, (2, 2))], [\"a\", \"b\"])\n>>> df.printSchema(1)\nroot\n |-- a: long (nullable = true)\n |-- b: struct (nullable = true)", ">>> df.printSchema(2)\nroot\n |-- a: long (nullable = true)\n |-- b: struct (nullable = true)\n |    |-- _1: long (nullable = true)\n |    |-- _2: long (nullable = true)", ">>> df = spark.range(1).selectExpr(\"id AS nonnullable\", \"NULL AS nullable\")\n>>> df.printSchema()\nroot\n |-- nonnullable: long (nullable = false)\n |-- nullable: void (nullable = true)"], "Parameters": [["level int, optional", "How many levels to print for nested schemas. New in version 3.5.0."]], "Returns": [], "Category": ["DataFrame"], "index": 512}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_xml.html#pyspark.sql.functions.to_xml"], "Title": ["to_xml"], "Feature": ["to_xml"], "Description": "Converts a column containing aStructTypeinto a XML string.\nThrows an exception, in the case of an unsupported type.", "Examples": [">>> from pyspark.sql import Row\n>>> data = [(1, Row(age=2, name='Alice'))]\n>>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n>>> df.select(to_xml(df.value, {'rowTag':'person'}).alias(\"xml\")).collect()\n[Row(xml='<person>\\n    <age>2</age>\\n    <name>Alice</name>\\n</person>')]"], "Parameters": [["col Column or str", "name of column containing a struct."], ["options: dict, optional", "options to control converting. accepts the same options as the XML datasource.\nSee Data Source Option for the version you use."]], "Returns": [["Column", "a XML string converted from given StructType ."]], "Category": ["Functions"], "index": 513}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.xpath.html#pyspark.sql.functions.xpath"], "Title": ["xpath"], "Feature": ["xpath"], "Description": "Returns a string array of values within the nodes of xml that match the XPath expression.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...     [('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>',)], ['x'])\n>>> df.select(sf.xpath(df.x, sf.lit('a/b/text()'))).show()\n+--------------------+\n|xpath(x, a/b/text())|\n+--------------------+\n|        [b1, b2, b3]|\n+--------------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 514}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.xpath_boolean.html#pyspark.sql.functions.xpath_boolean"], "Title": ["xpath_boolean"], "Feature": ["xpath_boolean"], "Description": "Returns true if the XPath expression evaluates to true, or if a matching node is found.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('<a><b>1</b></a>',)], ['x'])\n>>> df.select(sf.xpath_boolean(df.x, sf.lit('a/b'))).show()\n+---------------------+\n|xpath_boolean(x, a/b)|\n+---------------------+\n|                 true|\n+---------------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 515}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.xpath_double.html#pyspark.sql.functions.xpath_double"], "Title": ["xpath_double"], "Feature": ["xpath_double"], "Description": "Returns a double value, the value zero if no match is found,\nor NaN if a match is found but the value is non-numeric.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n>>> df.select(sf.xpath_double(df.x, sf.lit('sum(a/b)'))).show()\n+-------------------------+\n|xpath_double(x, sum(a/b))|\n+-------------------------+\n|                      3.0|\n+-------------------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 516}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.xpath_float.html#pyspark.sql.functions.xpath_float"], "Title": ["xpath_float"], "Feature": ["xpath_float"], "Description": "Returns a float value, the value zero if no match is found,\nor NaN if a match is found but the value is non-numeric.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n>>> df.select(sf.xpath_float(df.x, sf.lit('sum(a/b)'))).show()\n+------------------------+\n|xpath_float(x, sum(a/b))|\n+------------------------+\n|                     3.0|\n+------------------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 517}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.xpath_int.html#pyspark.sql.functions.xpath_int"], "Title": ["xpath_int"], "Feature": ["xpath_int"], "Description": "Returns an integer value, or the value zero if no match is found,\nor a match is found but the value is non-numeric.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n>>> df.select(sf.xpath_int(df.x, sf.lit('sum(a/b)'))).show()\n+----------------------+\n|xpath_int(x, sum(a/b))|\n+----------------------+\n|                     3|\n+----------------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 518}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.xpath_long.html#pyspark.sql.functions.xpath_long"], "Title": ["xpath_long"], "Feature": ["xpath_long"], "Description": "Returns a long integer value, or the value zero if no match is found,\nor a match is found but the value is non-numeric.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n>>> df.select(sf.xpath_long(df.x, sf.lit('sum(a/b)'))).show()\n+-----------------------+\n|xpath_long(x, sum(a/b))|\n+-----------------------+\n|                      3|\n+-----------------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 519}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.xpath_number.html#pyspark.sql.functions.xpath_number"], "Title": ["xpath_number"], "Feature": ["xpath_number"], "Description": "Returns a double value, the value zero if no match is found,\nor NaN if a match is found but the value is non-numeric.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.createDataFrame(\n...     [('<a><b>1</b><b>2</b></a>',)], ['x']\n... ).select(sf.xpath_number('x', sf.lit('sum(a/b)'))).show()\n+-------------------------+\n|xpath_number(x, sum(a/b))|\n+-------------------------+\n|                      3.0|\n+-------------------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 520}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.xpath_short.html#pyspark.sql.functions.xpath_short"], "Title": ["xpath_short"], "Feature": ["xpath_short"], "Description": "Returns a short integer value, or the value zero if no match is found,\nor a match is found but the value is non-numeric.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n>>> df.select(sf.xpath_short(df.x, sf.lit('sum(a/b)'))).show()\n+------------------------+\n|xpath_short(x, sum(a/b))|\n+------------------------+\n|                       3|\n+------------------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 521}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.xpath_string.html#pyspark.sql.functions.xpath_string"], "Title": ["xpath_string"], "Feature": ["xpath_string"], "Description": "Returns the text contents of the first xml node that matches the XPath expression.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([('<a><b>b</b><c>cc</c></a>',)], ['x'])\n>>> df.select(sf.xpath_string(df.x, sf.lit('a/c'))).show()\n+--------------------+\n|xpath_string(x, a/c)|\n+--------------------+\n|                  cc|\n+--------------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 522}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.randomSplit.html#pyspark.sql.DataFrame.randomSplit"], "Title": ["DataFrame.randomSplit"], "Feature": ["DataFrame.randomSplit"], "Description": "Randomly splits thisDataFramewith the provided weights.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([\n...     Row(age=10, height=80, name=\"Alice\"),\n...     Row(age=5, height=None, name=\"Bob\"),\n...     Row(age=None, height=None, name=\"Tom\"),\n...     Row(age=None, height=None, name=None),\n... ])", ">>> splits = df.randomSplit([1.0, 2.0], 24)\n>>> splits[0].count()\n2\n>>> splits[1].count()\n2"], "Parameters": [["weights list", "list of doubles as weights with which to split the DataFrame .\nWeights will be normalized if they don’t sum up to 1.0."], ["seed int, optional", "The seed for sampling."]], "Returns": [["list", "List of DataFrames."]], "Category": ["DataFrame"], "index": 523}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.parse_url.html#pyspark.sql.functions.parse_url"], "Title": ["parse_url"], "Feature": ["parse_url"], "Description": "URL function: Extracts a specified part from a URL. If a key is provided,\nit returns the associated query parameter value.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(\"https://spark.apache.org/path?query=1\", \"QUERY\")],\n...   [\"url\", \"part\"]\n... )\n>>> df.select(sf.parse_url(df.url, df.part)).show()\n+--------------------+\n|parse_url(url, part)|\n+--------------------+\n|             query=1|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(\"https://spark.apache.org/path?query=1\", \"QUERY\", \"query\")],\n...   [\"url\", \"part\", \"key\"]\n... )\n>>> df.select(sf.parse_url(df.url, df.part, df.key)).show()\n+-------------------------+\n|parse_url(url, part, key)|\n+-------------------------+\n|                        1|\n+-------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(\"https://spark.apache.org/path?query=1\", \"PROTOCOL\")],\n...   [\"url\", \"part\"]\n... )\n>>> df.select(sf.parse_url(df.url, df.part)).show()\n+--------------------+\n|parse_url(url, part)|\n+--------------------+\n|               https|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(\"https://spark.apache.org/path?query=1\", \"HOST\")],\n...   [\"url\", \"part\"]\n... )\n>>> df.select(sf.parse_url(df.url, df.part)).show()\n+--------------------+\n|parse_url(url, part)|\n+--------------------+\n|    spark.apache.org|\n+--------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(\"https://spark.apache.org/path?query=1\", \"PATH\")],\n...   [\"url\", \"part\"]\n... )\n>>> df.select(sf.parse_url(df.url, df.part)).show()\n+--------------------+\n|parse_url(url, part)|\n+--------------------+\n|               /path|\n+--------------------+"], "Parameters": [["url Column or str", "A column of strings, each representing a URL."], ["partToExtract Column or str", "A column of strings, each representing the part to extract from the URL."], ["key Column or str, optional", "A column of strings, each representing the key of a query parameter in the URL."]], "Returns": [["Column", "A new column of strings, each representing the value of the extracted part from the URL."]], "Category": ["Functions"], "index": 524}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_parse_url.html#pyspark.sql.functions.try_parse_url"], "Title": ["try_parse_url"], "Feature": ["try_parse_url"], "Description": "This is a special version ofparse_urlthat performs the same operation, but returns a\nNULL value instead of raising an error if the parsing cannot be performed.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(\"https://spark.apache.org/path?query=1\", \"QUERY\")],\n...   [\"url\", \"part\"]\n... )\n>>> df.select(sf.try_parse_url(df.url, df.part)).show()\n+------------------------+\n|try_parse_url(url, part)|\n+------------------------+\n|                 query=1|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(\"https://spark.apache.org/path?query=1\", \"QUERY\", \"query\")],\n...   [\"url\", \"part\", \"key\"]\n... )\n>>> df.select(sf.try_parse_url(df.url, df.part, df.key)).show()\n+-----------------------------+\n|try_parse_url(url, part, key)|\n+-----------------------------+\n|                            1|\n+-----------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(\"https://spark.apache.org/path?query=1\", \"PROTOCOL\")],\n...   [\"url\", \"part\"]\n... )\n>>> df.select(sf.try_parse_url(df.url, df.part)).show()\n+------------------------+\n|try_parse_url(url, part)|\n+------------------------+\n|                   https|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(\"https://spark.apache.org/path?query=1\", \"HOST\")],\n...   [\"url\", \"part\"]\n... )\n>>> df.select(sf.try_parse_url(df.url, df.part)).show()\n+------------------------+\n|try_parse_url(url, part)|\n+------------------------+\n|        spark.apache.org|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(\"https://spark.apache.org/path?query=1\", \"PATH\")],\n...   [\"url\", \"part\"]\n... )\n>>> df.select(sf.try_parse_url(df.url, df.part)).show()\n+------------------------+\n|try_parse_url(url, part)|\n+------------------------+\n|                   /path|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame(\n...   [(\"inva lid://spark.apache.org/path?query=1\", \"QUERY\", \"query\")],\n...   [\"url\", \"part\", \"key\"]\n... )\n>>> df.select(sf.try_parse_url(df.url, df.part, df.key)).show()\n+-----------------------------+\n|try_parse_url(url, part, key)|\n+-----------------------------+\n|                         NULL|\n+-----------------------------+"], "Parameters": [["url Column or str", "A column of strings, each representing a URL."], ["partToExtract Column or str", "A column of strings, each representing the part to extract from the URL."], ["key Column or str, optional", "A column of strings, each representing the key of a query parameter in the URL."]], "Returns": [["Column", "A new column of strings, each representing the value of the extracted part from the URL."]], "Category": ["Functions"], "index": 525}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.url_decode.html#pyspark.sql.functions.url_decode"], "Title": ["url_decode"], "Feature": ["url_decode"], "Description": "URL function: Decodes a URL-encoded string in ‘application/x-www-form-urlencoded’\nformat to its original format.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"https%3A%2F%2Fspark.apache.org\",)], [\"url\"])\n>>> df.select(sf.url_decode(df.url)).show(truncate=False)\n+------------------------+\n|url_decode(url)         |\n+------------------------+\n|https://spark.apache.org|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Hello%20World%21\",)], [\"url\"])\n>>> df.select(sf.url_decode(df.url)).show()\n+---------------+\n|url_decode(url)|\n+---------------+\n|   Hello World!|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"A%2BB%3D%3D\",)], [\"url\"])\n>>> df.select(sf.url_decode(df.url)).show()\n+---------------+\n|url_decode(url)|\n+---------------+\n|          A+B==|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"%E4%BD%A0%E5%A5%BD\",)], [\"url\"])\n>>> df.select(sf.url_decode(df.url)).show()\n+---------------+\n|url_decode(url)|\n+---------------+\n|           你好|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"%7E%21%40%23%24%25%5E%26%2A%28%29%5F%2B\",)], [\"url\"])\n>>> df.select(sf.url_decode(df.url)).show()\n+---------------+\n|url_decode(url)|\n+---------------+\n|  ~!@#$%^&*()_+|\n+---------------+"], "Parameters": [["str Column or str", "A column of strings, each representing a URL-encoded string."]], "Returns": [["Column", "A new column of strings, each representing the decoded string."]], "Category": ["Functions"], "index": 526}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.url_encode.html#pyspark.sql.functions.url_encode"], "Title": ["url_encode"], "Feature": ["url_encode"], "Description": "URL function: Encodes a string into a URL-encoded string in\n‘application/x-www-form-urlencoded’ format.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"https://spark.apache.org\",)], [\"url\"])\n>>> df.select(sf.url_encode(df.url)).show(truncate=False)\n+------------------------------+\n|url_encode(url)               |\n+------------------------------+\n|https%3A%2F%2Fspark.apache.org|\n+------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"Hello World!\",)], [\"url\"])\n>>> df.select(sf.url_encode(df.url)).show()\n+---------------+\n|url_encode(url)|\n+---------------+\n| Hello+World%21|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"A+B==\",)], [\"url\"])\n>>> df.select(sf.url_encode(df.url)).show()\n+---------------+\n|url_encode(url)|\n+---------------+\n|    A%2BB%3D%3D|\n+---------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"你好\",)], [\"url\"])\n>>> df.select(sf.url_encode(df.url)).show()\n+------------------+\n|   url_encode(url)|\n+------------------+\n|%E4%BD%A0%E5%A5%BD|\n+------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"~!@#$%^&*()_+\",)], [\"url\"])\n>>> df.select(sf.url_encode(df.url)).show(truncate=False)\n+-----------------------------------+\n|url_encode(url)                    |\n+-----------------------------------+\n|%7E%21%40%23%24%25%5E%26*%28%29_%2B|\n+-----------------------------------+"], "Parameters": [["str Column or str", "A column of strings, each representing a string to be URL-encoded."]], "Returns": [["Column", "A new column of strings, each representing the URL-encoded string."]], "Category": ["Functions"], "index": 527}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_url_decode.html#pyspark.sql.functions.try_url_decode"], "Title": ["try_url_decode"], "Feature": ["try_url_decode"], "Description": "This is a special version ofurl_decodethat performs the same operation, but returns a\nNULL value instead of raising an error if the decoding cannot be performed.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"https%3A%2F%2Fspark.apache.org\",)], [\"url\"])\n>>> df.select(sf.try_url_decode(df.url)).show(truncate=False)\n+------------------------+\n|try_url_decode(url)     |\n+------------------------+\n|https://spark.apache.org|\n+------------------------+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"https%3A%2F%2spark.apache.org\",)], [\"url\"])\n>>> df.select(sf.try_url_decode(df.url)).show()\n+-------------------+\n|try_url_decode(url)|\n+-------------------+\n|               NULL|\n+-------------------+"], "Parameters": [["str Column or str", "A column of strings, each representing a URL-encoded string."]], "Returns": [["Column", "A new column of strings, each representing the decoded string."]], "Category": ["Functions"], "index": 528}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.aes_decrypt.html#pyspark.sql.functions.aes_decrypt"], "Title": ["aes_decrypt"], "Feature": ["aes_decrypt"], "Description": "Returns a decrypted value ofinputusing AES inmodewithpadding. Key lengths of 16,\n24 and 32 bits are supported. Supported combinations of (mode,padding) are (‘ECB’,\n‘PKCS’), (‘GCM’, ‘NONE’) and (‘CBC’, ‘PKCS’). Optional additional authenticated data (AAD) is\nonly supported for GCM. If provided for encryption, the identical AAD value must be provided\nfor decryption. The default mode is GCM.\nSee alsopyspark.sql.functions.aes_encrypt()pyspark.sql.functions.try_aes_decrypt()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4\",\n...     \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n...     \"This is an AAD mixed into the input\",)],\n...     [\"input\", \"key\", \"mode\", \"padding\", \"aad\"]\n... )\n>>> df.select(sf.aes_decrypt(\n...     sf.unbase64(df.input), df.key, \"mode\", df.padding, df.aad\n... ).cast(\"STRING\")).show(truncate=False)\n+---------------------------------------------------------------------+\n|CAST(aes_decrypt(unbase64(input), key, mode, padding, aad) AS STRING)|\n+---------------------------------------------------------------------+\n|Spark                                                                |\n+---------------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=\",\n...     \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",)],\n...     [\"input\", \"key\", \"mode\", \"padding\"]\n... )\n>>> df.select(sf.aes_decrypt(\n...     sf.unbase64(df.input), df.key, \"mode\", df.padding\n... ).cast(\"STRING\")).show(truncate=False)\n+------------------------------------------------------------------+\n|CAST(aes_decrypt(unbase64(input), key, mode, padding, ) AS STRING)|\n+------------------------------------------------------------------+\n|Spark                                                             |\n+------------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=\",\n...     \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",)],\n...     [\"input\", \"key\", \"mode\", \"padding\"]\n... )\n>>> df.select(sf.aes_decrypt(\n...     sf.unbase64(df.input), df.key, \"mode\"\n... ).cast(\"STRING\")).show(truncate=False)\n+------------------------------------------------------------------+\n|CAST(aes_decrypt(unbase64(input), key, mode, DEFAULT, ) AS STRING)|\n+------------------------------------------------------------------+\n|Spark                                                             |\n+------------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94\",\n...     \"0000111122223333\",)],\n...     [\"input\", \"key\"]\n... )\n>>> df.select(sf.aes_decrypt(\n...     sf.unhex(df.input), df.key\n... ).cast(\"STRING\")).show(truncate=False)\n+--------------------------------------------------------------+\n|CAST(aes_decrypt(unhex(input), key, GCM, DEFAULT, ) AS STRING)|\n+--------------------------------------------------------------+\n|Spark                                                         |\n+--------------------------------------------------------------+"], "Parameters": [["input Column or column name", "The binary value to decrypt."], ["key Column or column name", "The passphrase to use to decrypt the data."], ["mode Column or column name, optional", "Specifies which block cipher mode should be used to decrypt messages. Valid modes: ECB,\nGCM, CBC."], ["padding Column or column name, optional", "Specifies how to pad messages whose length is not a multiple of the block size. Valid\nvalues: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS\nfor CBC."], ["aad Column or column name, optional", "Optional additional authenticated data. Only supported for GCM mode. This can be any\nfree-form input and must be provided for both encryption and decryption."]], "Returns": [["Column", "A new column that contains a decrypted value."]], "Category": ["Functions"], "index": 529}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.aes_encrypt.html#pyspark.sql.functions.aes_encrypt"], "Title": ["aes_encrypt"], "Feature": ["aes_encrypt"], "Description": "Returns an encrypted value ofinputusing AES in givenmodewith the specifiedpadding.\nKey lengths of 16, 24 and 32 bits are supported. Supported combinations of (mode,padding) are (‘ECB’, ‘PKCS’), (‘GCM’, ‘NONE’) and (‘CBC’, ‘PKCS’). Optional initialization\nvectors (IVs) are only supported for CBC and GCM modes. These must be 16 bytes for CBC and 12\nbytes for GCM. If not provided, a random vector will be generated and prepended to the\noutput. Optional additional authenticated data (AAD) is only supported for GCM. If provided\nfor encryption, the identical AAD value must be provided for decryption. The default mode is\nGCM.\nSee alsopyspark.sql.functions.aes_decrypt()pyspark.sql.functions.try_aes_decrypt()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"Spark\", \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n...     \"000000000000000000000000\", \"This is an AAD mixed into the input\",)],\n...     [\"input\", \"key\", \"mode\", \"padding\", \"iv\", \"aad\"]\n... )\n>>> df.select(sf.base64(sf.aes_encrypt(\n...     df.input, df.key, \"mode\", df.padding, sf.to_binary(df.iv, sf.lit(\"hex\")), df.aad)\n... )).show(truncate=False)\n+-----------------------------------------------------------------------+\n|base64(aes_encrypt(input, key, mode, padding, to_binary(iv, hex), aad))|\n+-----------------------------------------------------------------------+\n|AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4                           |\n+-----------------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"Spark\", \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n...     \"000000000000000000000000\", \"This is an AAD mixed into the input\",)],\n...     [\"input\", \"key\", \"mode\", \"padding\", \"iv\", \"aad\"]\n... )\n>>> df.select(sf.base64(sf.aes_encrypt(\n...     df.input, df.key, \"mode\", df.padding, sf.to_binary(df.iv, sf.lit(\"hex\")))\n... )).show(truncate=False)\n+--------------------------------------------------------------------+\n|base64(aes_encrypt(input, key, mode, padding, to_binary(iv, hex), ))|\n+--------------------------------------------------------------------+\n|AAAAAAAAAAAAAAAAQiYi+sRNYDAOTjdSEcYBFsAWPL1f                        |\n+--------------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"Spark SQL\", \"1234567890abcdef\", \"ECB\", \"PKCS\",)],\n...     [\"input\", \"key\", \"mode\", \"padding\"]\n... )\n>>> df.select(sf.aes_decrypt(sf.aes_encrypt(df.input, df.key, \"mode\", df.padding),\n...     df.key, df.mode, df.padding\n... ).cast(\"STRING\")).show(truncate=False)\n+---------------------------------------------------------------------------------------------+\n|CAST(aes_decrypt(aes_encrypt(input, key, mode, padding, , ), key, mode, padding, ) AS STRING)|\n+---------------------------------------------------------------------------------------------+\n|Spark SQL                                                                                    |\n+---------------------------------------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"Spark SQL\", \"0000111122223333\", \"ECB\",)],\n...     [\"input\", \"key\", \"mode\"]\n... )\n>>> df.select(sf.aes_decrypt(sf.aes_encrypt(df.input, df.key, \"mode\"),\n...     df.key, df.mode\n... ).cast(\"STRING\")).show(truncate=False)\n+---------------------------------------------------------------------------------------------+\n|CAST(aes_decrypt(aes_encrypt(input, key, mode, DEFAULT, , ), key, mode, DEFAULT, ) AS STRING)|\n+---------------------------------------------------------------------------------------------+\n|Spark SQL                                                                                    |\n+---------------------------------------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"Spark SQL\", \"abcdefghijklmnop\",)],\n...     [\"input\", \"key\"]\n... )\n>>> df.select(sf.aes_decrypt(\n...     sf.unbase64(sf.base64(sf.aes_encrypt(df.input, df.key))), df.key\n... ).cast(\"STRING\")).show(truncate=False)\n+-------------------------------------------------------------------------------------------------------------+\n|CAST(aes_decrypt(unbase64(base64(aes_encrypt(input, key, GCM, DEFAULT, , ))), key, GCM, DEFAULT, ) AS STRING)|\n+-------------------------------------------------------------------------------------------------------------+\n|Spark SQL                                                                                                    |\n+-------------------------------------------------------------------------------------------------------------+"], "Parameters": [["input Column or column name", "The binary value to encrypt."], ["key Column or column name", "The passphrase to use to encrypt the data."], ["mode Column or str, optional", "Specifies which block cipher mode should be used to encrypt messages. Valid modes: ECB,\nGCM, CBC."], ["padding Column or column name, optional", "Specifies how to pad messages whose length is not a multiple of the block size. Valid\nvalues: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS\nfor CBC."], ["iv Column or column name, optional", "Optional initialization vector. Only supported for CBC and GCM modes. Valid values: None or\n“”. 16-byte array for CBC mode. 12-byte array for GCM mode."], ["aad Column or column name, optional", "Optional additional authenticated data. Only supported for GCM mode. This can be any\nfree-form input and must be provided for both encryption and decryption."]], "Returns": [["Column", "A new column that contains an encrypted value."]], "Category": ["Functions"], "index": 530}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.assert_true.html#pyspark.sql.functions.assert_true"], "Title": ["assert_true"], "Feature": ["assert_true"], "Description": "Returnsnullif the input column istrue; throws an exception\nwith the provided error message otherwise.\nSee alsopyspark.sql.functions.raise_error()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(0, 1)], ['a', 'b'])\n>>> df.select('*', sf.assert_true(df.a < df.b)).show() \n+------------------------------------------------------+\n|assert_true((a < b), '(a#788L < b#789L)' is not true!)|\n+------------------------------------------------------+\n|                                                  NULL|\n+------------------------------------------------------+", ">>> df.select('*', sf.assert_true(df.a < df.b, df.a)).show()\n+---+---+-----------------------+\n|  a|  b|assert_true((a < b), a)|\n+---+---+-----------------------+\n|  0|  1|                   NULL|\n+---+---+-----------------------+", ">>> df.select('*', sf.assert_true(df.a < df.b, 'error')).show()\n+---+---+---------------------------+\n|  a|  b|assert_true((a < b), error)|\n+---+---+---------------------------+\n|  0|  1|                       NULL|\n+---+---+---------------------------+", ">>> df.select('*', sf.assert_true(df.a > df.b, 'My error msg')).show() \n...\njava.lang.RuntimeException: My error msg\n..."], "Parameters": [["col Column or column name", "column name or column that represents the input column to test"], ["errMsg Column or literal string, optional", "A Python string literal or column containing the error message"]], "Returns": [["Column", "null if the input column is true otherwise throws an error with specified message."]], "Category": ["Functions"], "index": 531}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bitmap_bit_position.html#pyspark.sql.functions.bitmap_bit_position"], "Title": ["bitmap_bit_position"], "Feature": ["bitmap_bit_position"], "Description": "Returns the bit position for the given input column.\nSee alsopyspark.sql.functions.bitmap_bucket_number()pyspark.sql.functions.bitmap_construct_agg()pyspark.sql.functions.bitmap_count()pyspark.sql.functions.bitmap_or_agg()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(123,)], ['a'])\n>>> df.select('*', sf.bitmap_bit_position('a')).show()\n+---+----------------------+\n|  a|bitmap_bit_position(a)|\n+---+----------------------+\n|123|                   122|\n+---+----------------------+"], "Parameters": [["col Column or column name", "The input column."]], "Returns": [], "Category": ["Functions"], "index": 532}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bitmap_bucket_number.html#pyspark.sql.functions.bitmap_bucket_number"], "Title": ["bitmap_bucket_number"], "Feature": ["bitmap_bucket_number"], "Description": "Returns the bucket number for the given input column.\nSee alsopyspark.sql.functions.bitmap_bit_position()pyspark.sql.functions.bitmap_construct_agg()pyspark.sql.functions.bitmap_count()pyspark.sql.functions.bitmap_or_agg()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(123,)], ['a'])\n>>> df.select('*', sf.bitmap_bucket_number('a')).show()\n+---+-----------------------+\n|  a|bitmap_bucket_number(a)|\n+---+-----------------------+\n|123|                      1|\n+---+-----------------------+"], "Parameters": [["col Column or column name", "The input column."]], "Returns": [], "Category": ["Functions"], "index": 533}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.rdd.html#pyspark.sql.DataFrame.rdd"], "Title": ["DataFrame.rdd"], "Feature": ["DataFrame.rdd"], "Description": "Returns the content as anpyspark.RDDofRow.", "Examples": [">>> df = spark.range(1)\n>>> type(df.rdd)\n<class 'pyspark.core.rdd.RDD'>"], "Parameters": [], "Returns": [["RDD", ""]], "Category": ["DataFrame"], "index": 534}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bitmap_count.html#pyspark.sql.functions.bitmap_count"], "Title": ["bitmap_count"], "Feature": ["bitmap_count"], "Description": "Returns the number of set bits in the input bitmap.\nSee alsopyspark.sql.functions.bitmap_bit_position()pyspark.sql.functions.bitmap_bucket_number()pyspark.sql.functions.bitmap_construct_agg()pyspark.sql.functions.bitmap_or_agg()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"FFFF\",)], [\"a\"])\n>>> df.select(sf.bitmap_count(sf.to_binary(df.a, sf.lit(\"hex\")))).show()\n+-------------------------------+\n|bitmap_count(to_binary(a, hex))|\n+-------------------------------+\n|                             16|\n+-------------------------------+"], "Parameters": [["col Column or column name", "The input bitmap."]], "Returns": [], "Category": ["Functions"], "index": 535}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.current_catalog.html#pyspark.sql.functions.current_catalog"], "Title": ["current_catalog"], "Feature": ["current_catalog"], "Description": "Returns the current catalog.\nSee alsopyspark.sql.functions.current_database()pyspark.sql.functions.current_schema()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.current_catalog()).show()\n+-----------------+\n|current_catalog()|\n+-----------------+\n|    spark_catalog|\n+-----------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 536}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.current_database.html#pyspark.sql.functions.current_database"], "Title": ["current_database"], "Feature": ["current_database"], "Description": "Returns the current database.\nSee alsopyspark.sql.functions.current_catalog()pyspark.sql.functions.current_schema()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.current_database()).show()\n+----------------+\n|current_schema()|\n+----------------+\n|         default|\n+----------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 537}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.current_schema.html#pyspark.sql.functions.current_schema"], "Title": ["current_schema"], "Feature": ["current_schema"], "Description": "Returns the current database.\nSee alsopyspark.sql.functions.current_catalog()pyspark.sql.functions.current_database()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.current_schema()).show()\n+----------------+\n|current_schema()|\n+----------------+\n|         default|\n+----------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 538}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.current_user.html#pyspark.sql.functions.current_user"], "Title": ["current_user"], "Feature": ["current_user"], "Description": "Returns the current database.\nSee alsopyspark.sql.functions.user()pyspark.sql.functions.session_user()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.current_user()).show() \n+--------------+\n|current_user()|\n+--------------+\n| ruifeng.zheng|\n+--------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 539}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.hll_sketch_estimate.html#pyspark.sql.functions.hll_sketch_estimate"], "Title": ["hll_sketch_estimate"], "Feature": ["hll_sketch_estimate"], "Description": "Returns the estimated number of unique values given the binary representation\nof a Datasketches HllSketch.\nSee alsopyspark.sql.functions.hll_union()pyspark.sql.functions.hll_union_agg()pyspark.sql.functions.hll_sketch_agg()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([1,2,2,3], \"INT\")\n>>> df.agg(sf.hll_sketch_estimate(sf.hll_sketch_agg(\"value\"))).show()\n+----------------------------------------------+\n|hll_sketch_estimate(hll_sketch_agg(value, 12))|\n+----------------------------------------------+\n|                                             3|\n+----------------------------------------------+"], "Parameters": [["col Column or column name", ""]], "Returns": [["Column", "The estimated number of unique values for the HllSketch."]], "Category": ["Functions"], "index": 540}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.hll_union.html#pyspark.sql.functions.hll_union"], "Title": ["hll_union"], "Feature": ["hll_union"], "Description": "Merges two binary representations of Datasketches HllSketch objects, using a\nDatasketches Union object.  Throws an exception if sketches have different\nlgConfigK values and allowDifferentLgConfigK is unset or set to false.\nSee alsopyspark.sql.functions.hll_union_agg()pyspark.sql.functions.hll_sketch_agg()pyspark.sql.functions.hll_sketch_estimate()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(1,4),(2,5),(2,5),(3,6)], \"struct<v1:int,v2:int>\")\n>>> df = df.agg(\n...     sf.hll_sketch_agg(\"v1\").alias(\"sketch1\"),\n...     sf.hll_sketch_agg(\"v2\").alias(\"sketch2\")\n... )\n>>> df.select(sf.hll_sketch_estimate(sf.hll_union(df.sketch1, \"sketch2\"))).show()\n+-------------------------------------------------------+\n|hll_sketch_estimate(hll_union(sketch1, sketch2, false))|\n+-------------------------------------------------------+\n|                                                      6|\n+-------------------------------------------------------+"], "Parameters": [["col1 Column or column name", ""], ["col2 Column or column name", ""], ["allowDifferentLgConfigK bool, optional", "Allow sketches with different lgConfigK values to be merged (defaults to false)."]], "Returns": [["Column", "The binary representation of the merged HllSketch."]], "Category": ["Functions"], "index": 541}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.input_file_block_length.html#pyspark.sql.functions.input_file_block_length"], "Title": ["input_file_block_length"], "Feature": ["input_file_block_length"], "Description": "Returns the length of the block being read, or -1 if not available.\nSee alsopyspark.sql.functions.input_file_name()pyspark.sql.functions.input_file_block_start()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.read.text(\"python/test_support/sql/ages_newlines.csv\", lineSep=\",\")\n>>> df.select(sf.input_file_block_length()).show()\n+-------------------------+\n|input_file_block_length()|\n+-------------------------+\n|                       87|\n|                       87|\n|                       87|\n|                       87|\n|                       87|\n|                       87|\n|                       87|\n|                       87|\n+-------------------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 542}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.input_file_block_start.html#pyspark.sql.functions.input_file_block_start"], "Title": ["input_file_block_start"], "Feature": ["input_file_block_start"], "Description": "Returns the start offset of the block being read, or -1 if not available.\nSee alsopyspark.sql.functions.input_file_name()pyspark.sql.functions.input_file_block_length()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.read.text(\"python/test_support/sql/ages_newlines.csv\", lineSep=\",\")\n>>> df.select(sf.input_file_block_start()).show()\n+------------------------+\n|input_file_block_start()|\n+------------------------+\n|                       0|\n|                       0|\n|                       0|\n|                       0|\n|                       0|\n|                       0|\n|                       0|\n|                       0|\n+------------------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 543}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.input_file_name.html#pyspark.sql.functions.input_file_name"], "Title": ["input_file_name"], "Feature": ["input_file_name"], "Description": "Creates a string column for the file name of the current Spark task.\nSee alsopyspark.sql.functions.input_file_block_length()pyspark.sql.functions.input_file_block_start()", "Examples": [">>> import os\n>>> from pyspark.sql import functions as sf\n>>> path = os.path.abspath(__file__)\n>>> df = spark.read.text(path)\n>>> df.select(sf.input_file_name()).first()\nRow(input_file_name()='file:///...')"], "Parameters": [], "Returns": [["Column", "file names."]], "Category": ["Functions"], "index": 544}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.registerTempTable.html#pyspark.sql.DataFrame.registerTempTable"], "Title": ["DataFrame.registerTempTable"], "Feature": ["DataFrame.registerTempTable"], "Description": "Registers thisDataFrameas a temporary table using the given name.\nThe lifetime of this temporary table is tied to theSparkSessionthat was used to create thisDataFrame.\nDeprecated since version 2.0.0:UseDataFrame.createOrReplaceTempView()instead.", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.registerTempTable(\"people\")\n>>> df2 = spark.sql(\"SELECT * FROM people\")\n>>> sorted(df.collect()) == sorted(df2.collect())\nTrue\n>>> spark.catalog.dropTempView(\"people\")\nTrue"], "Parameters": [["name str", "Name of the temporary table to register."]], "Returns": [], "Category": ["DataFrame"], "index": 545}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.java_method.html#pyspark.sql.functions.java_method"], "Title": ["java_method"], "Feature": ["java_method"], "Description": "Calls a method with reflection.\nSee alsopyspark.sql.functions.reflect()pyspark.sql.functions.try_reflect()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(\n...     sf.java_method(\n...         sf.lit(\"java.util.UUID\"),\n...         sf.lit(\"fromString\"),\n...         sf.lit(\"a5cf6c42-0c85-418f-af6c-3e4e5b1328f2\")\n...     )\n... ).show(truncate=False)\n+-----------------------------------------------------------------------------+\n|java_method(java.util.UUID, fromString, a5cf6c42-0c85-418f-af6c-3e4e5b1328f2)|\n+-----------------------------------------------------------------------------+\n|a5cf6c42-0c85-418f-af6c-3e4e5b1328f2                                         |\n+-----------------------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('a5cf6c42-0c85-418f-af6c-3e4e5b1328f2',)], ['a'])\n>>> df.select(\n...     sf.java_method(sf.lit('java.util.UUID'), sf.lit('fromString'), 'a')\n... ).show(truncate=False)\n+------------------------------------------+\n|java_method(java.util.UUID, fromString, a)|\n+------------------------------------------+\n|a5cf6c42-0c85-418f-af6c-3e4e5b1328f2      |\n+------------------------------------------+"], "Parameters": [["cols Column or column name", "the first element should be a Column representing literal string for the class name,\nand the second element should be a Column representing literal string for the method name,\nand the remaining are input arguments (Columns or column names) to the Java method."]], "Returns": [], "Category": ["Functions"], "index": 546}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.monotonically_increasing_id.html#pyspark.sql.functions.monotonically_increasing_id"], "Title": ["monotonically_increasing_id"], "Feature": ["monotonically_increasing_id"], "Description": "A column that generates monotonically increasing 64-bit integers.\nThe generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\nThe current implementation puts the partition ID in the upper 31 bits, and the record number\nwithin each partition in the lower 33 bits. The assumption is that the data frame has\nless than 1 billion partitions, and each partition has less than 8 billion records.\nNotes\nThe function is non-deterministic because its result depends on partition IDs.\nAs an example, consider aDataFramewith two partitions, each with 3 records.\nThis expression would return the following IDs:\n0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(0, 10, 1, 2).select(\n...     \"*\",\n...     sf.spark_partition_id(),\n...     sf.monotonically_increasing_id()).show()\n+---+--------------------+-----------------------------+\n| id|SPARK_PARTITION_ID()|monotonically_increasing_id()|\n+---+--------------------+-----------------------------+\n|  0|                   0|                            0|\n|  1|                   0|                            1|\n|  2|                   0|                            2|\n|  3|                   0|                            3|\n|  4|                   0|                            4|\n|  5|                   1|                   8589934592|\n|  6|                   1|                   8589934593|\n|  7|                   1|                   8589934594|\n|  8|                   1|                   8589934595|\n|  9|                   1|                   8589934596|\n+---+--------------------+-----------------------------+"], "Parameters": [], "Returns": [["Column", "last value of the group."]], "Category": ["Functions"], "index": 547}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.raise_error.html#pyspark.sql.functions.raise_error"], "Title": ["raise_error"], "Feature": ["raise_error"], "Description": "Throws an exception with the provided error message.\nSee alsopyspark.sql.functions.assert_true()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.raise_error(\"My error message\")).show() \n...\njava.lang.RuntimeException: My error message\n..."], "Parameters": [["errMsg Column or literal string", "A Python string literal or column containing the error message"]], "Returns": [["Column", "throws an error with specified message."]], "Category": ["Functions"], "index": 548}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.reflect.html#pyspark.sql.functions.reflect"], "Title": ["reflect"], "Feature": ["reflect"], "Description": "Calls a method with reflection.\nSee alsopyspark.sql.functions.java_method()pyspark.sql.functions.try_reflect()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([('a5cf6c42-0c85-418f-af6c-3e4e5b1328f2',)], ['a'])\n>>> df.select(\n...     sf.reflect(sf.lit('java.util.UUID'), sf.lit('fromString'), 'a')\n... ).show(truncate=False)\n+--------------------------------------+\n|reflect(java.util.UUID, fromString, a)|\n+--------------------------------------+\n|a5cf6c42-0c85-418f-af6c-3e4e5b1328f2  |\n+--------------------------------------+"], "Parameters": [["cols Column or column name", "the first element should be a Column representing literal string for the class name,\nand the second element should be a Column representing literal string for the method name,\nand the remaining are input arguments (Columns or column names) to the Java method."]], "Returns": [], "Category": ["Functions"], "index": 549}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.session_user.html#pyspark.sql.functions.session_user"], "Title": ["session_user"], "Feature": ["session_user"], "Description": "Returns the user name of current execution context.\nSee alsopyspark.sql.functions.user()pyspark.sql.functions.current_user()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.session_user()).show() \n+--------------+\n|session_user()|\n+--------------+\n| ruifeng.zheng|\n+--------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 550}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.spark_partition_id.html#pyspark.sql.functions.spark_partition_id"], "Title": ["spark_partition_id"], "Feature": ["spark_partition_id"], "Description": "A column for partition ID.\nNotes\nThis is non deterministic because it depends on data partitioning and task scheduling.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(10, numPartitions=5).select(\"*\", sf.spark_partition_id()).show()\n+---+--------------------+\n| id|SPARK_PARTITION_ID()|\n+---+--------------------+\n|  0|                   0|\n|  1|                   0|\n|  2|                   1|\n|  3|                   1|\n|  4|                   2|\n|  5|                   2|\n|  6|                   3|\n|  7|                   3|\n|  8|                   4|\n|  9|                   4|\n+---+--------------------+"], "Parameters": [], "Returns": [["Column", "partition id the record belongs to."]], "Category": ["Functions"], "index": 551}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_aes_decrypt.html#pyspark.sql.functions.try_aes_decrypt"], "Title": ["try_aes_decrypt"], "Feature": ["try_aes_decrypt"], "Description": "This is a special version ofaes_decryptthat performs the same operation,\nbut returns a NULL value instead of raising an error if the decryption cannot be performed.\nReturns a decrypted value ofinputusing AES inmodewithpadding. Key lengths of 16,\n24 and 32 bits are supported. Supported combinations of (mode,padding) are (‘ECB’,\n‘PKCS’), (‘GCM’, ‘NONE’) and (‘CBC’, ‘PKCS’). Optional additional authenticated data (AAD) is\nonly supported for GCM. If provided for encryption, the identical AAD value must be provided\nfor decryption. The default mode is GCM.\nSee alsopyspark.sql.functions.aes_encrypt()pyspark.sql.functions.aes_decrypt()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4\",\n...     \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n...     \"This is an AAD mixed into the input\",)],\n...     [\"input\", \"key\", \"mode\", \"padding\", \"aad\"]\n... )\n>>> df.select(sf.try_aes_decrypt(\n...     sf.unbase64(df.input), df.key, \"mode\", df.padding, df.aad\n... ).cast(\"STRING\")).show(truncate=False)\n+-------------------------------------------------------------------------+\n|CAST(try_aes_decrypt(unbase64(input), key, mode, padding, aad) AS STRING)|\n+-------------------------------------------------------------------------+\n|Spark                                                                    |\n+-------------------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4\",\n...     \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",\n...     \"This is an AAD mixed into the input\",)],\n...     [\"input\", \"key\", \"mode\", \"padding\", \"aad\"]\n... )\n>>> df.select(sf.try_aes_decrypt(\n...     sf.unbase64(df.input), df.key, \"mode\", df.padding, df.aad\n... ).cast(\"STRING\")).show(truncate=False)\n+-------------------------------------------------------------------------+\n|CAST(try_aes_decrypt(unbase64(input), key, mode, padding, aad) AS STRING)|\n+-------------------------------------------------------------------------+\n|NULL                                                                     |\n+-------------------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=\",\n...     \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",)],\n...     [\"input\", \"key\", \"mode\", \"padding\"]\n... )\n>>> df.select(sf.try_aes_decrypt(\n...     sf.unbase64(df.input), df.key, \"mode\", df.padding\n... ).cast(\"STRING\")).show(truncate=False)\n+----------------------------------------------------------------------+\n|CAST(try_aes_decrypt(unbase64(input), key, mode, padding, ) AS STRING)|\n+----------------------------------------------------------------------+\n|Spark                                                                 |\n+----------------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=\",\n...     \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",)],\n...     [\"input\", \"key\", \"mode\", \"padding\"]\n... )\n>>> df.select(sf.try_aes_decrypt(\n...     sf.unbase64(df.input), df.key, \"mode\"\n... ).cast(\"STRING\")).show(truncate=False)\n+----------------------------------------------------------------------+\n|CAST(try_aes_decrypt(unbase64(input), key, mode, DEFAULT, ) AS STRING)|\n+----------------------------------------------------------------------+\n|Spark                                                                 |\n+----------------------------------------------------------------------+", ">>> import pyspark.sql.functions as sf\n>>> df = spark.createDataFrame([(\n...     \"83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94\",\n...     \"0000111122223333\",)],\n...     [\"input\", \"key\"]\n... )\n>>> df.select(sf.try_aes_decrypt(\n...     sf.unhex(df.input), df.key\n... ).cast(\"STRING\")).show(truncate=False)\n+------------------------------------------------------------------+\n|CAST(try_aes_decrypt(unhex(input), key, GCM, DEFAULT, ) AS STRING)|\n+------------------------------------------------------------------+\n|Spark                                                             |\n+------------------------------------------------------------------+"], "Parameters": [["input Column or column name", "The binary value to decrypt."], ["key Column or column name", "The passphrase to use to decrypt the data."], ["mode Column or column name, optional", "Specifies which block cipher mode should be used to decrypt messages. Valid modes: ECB,\nGCM, CBC."], ["padding Column or column name, optional", "Specifies how to pad messages whose length is not a multiple of the block size. Valid\nvalues: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS\nfor CBC."], ["aad Column or column name, optional", "Optional additional authenticated data. Only supported for GCM mode. This can be any\nfree-form input and must be provided for both encryption and decryption."]], "Returns": [["Column", "A new column that contains a decrypted value or a NULL value."]], "Category": ["Functions"], "index": 552}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.try_reflect.html#pyspark.sql.functions.try_reflect"], "Title": ["try_reflect"], "Feature": ["try_reflect"], "Description": "This is a special version ofreflectthat performs the same operation, but returns a NULL\nvalue instead of raising an error if the invoke method thrown exception.\nSee alsopyspark.sql.functions.reflect()pyspark.sql.functions.java_method()", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(\"a5cf6c42-0c85-418f-af6c-3e4e5b1328f2\",)], [\"a\"])\n>>> df.select(\n...     sf.try_reflect(sf.lit(\"java.util.UUID\"), sf.lit(\"fromString\"), \"a\")\n... ).show(truncate=False)\n+------------------------------------------+\n|try_reflect(java.util.UUID, fromString, a)|\n+------------------------------------------+\n|a5cf6c42-0c85-418f-af6c-3e4e5b1328f2      |\n+------------------------------------------+", ">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(\n...     sf.try_reflect(sf.lit(\"scala.Predef\"), sf.lit(\"require\"), sf.lit(False))\n... ).show(truncate=False)\n+-----------------------------------------+\n|try_reflect(scala.Predef, require, false)|\n+-----------------------------------------+\n|NULL                                     |\n+-----------------------------------------+"], "Parameters": [["cols Column or column name", "the first element should be a Column representing literal string for the class name,\nand the second element should be a Column representing literal string for the method name,\nand the remaining are input arguments (Columns or column names) to the Java method."]], "Returns": [], "Category": ["Functions"], "index": 553}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.typeof.html#pyspark.sql.functions.typeof"], "Title": ["typeof"], "Feature": ["typeof"], "Description": "Return DDL-formatted type string for the data type of the input.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(True, 1, 1.0, 'xyz',)], ['a', 'b', 'c', 'd'])\n>>> df.select(sf.typeof(df.a), sf.typeof(df.b), sf.typeof('c'), sf.typeof('d')).show()\n+---------+---------+---------+---------+\n|typeof(a)|typeof(b)|typeof(c)|typeof(d)|\n+---------+---------+---------+---------+\n|  boolean|   bigint|   double|   string|\n+---------+---------+---------+---------+"], "Parameters": [["col Column or column name", ""]], "Returns": [], "Category": ["Functions"], "index": 554}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.user.html#pyspark.sql.functions.user"], "Title": ["user"], "Feature": ["user"], "Description": "Returns the current database.\nSee alsopyspark.sql.functions.current_user()pyspark.sql.functions.session_user()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.range(1).select(sf.user()).show() \n+--------------+\n|        user()|\n+--------------+\n| ruifeng.zheng|\n+--------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 555}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.cache.html#pyspark.sql.DataFrame.cache"], "Title": ["DataFrame.cache"], "Feature": ["DataFrame.cache"], "Description": "Persists theDataFramewith the default storage level (MEMORY_AND_DISK_DESER).\nNotes\nThe default storage level has changed toMEMORY_AND_DISK_DESERto match Scala in 3.0.", "Examples": [">>> df = spark.range(1)\n>>> df.cache()\nDataFrame[id: bigint]", ">>> df.explain()\n== Physical Plan ==\nInMemoryTableScan ..."], "Parameters": [], "Returns": [["DataFrame", "Cached DataFrame."]], "Category": ["DataFrame"], "index": 556}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartitionByRange.html#pyspark.sql.DataFrame.repartitionByRange"], "Title": ["DataFrame.repartitionByRange"], "Feature": ["DataFrame.repartitionByRange"], "Description": "Returns a newDataFramepartitioned by the given partitioning expressions. The\nresultingDataFrameis range partitioned.\nNotes\nAt least one partition-by expression must be specified.\nWhen no explicit sort order is specified, “ascending nulls first” is assumed.\nDue to performance reasons this method uses sampling to estimate the ranges.\nHence, the output may not be consistent, since sampling can return different values.\nThe sample size can be controlled by the configspark.sql.execution.rangeExchange.sampleSizePerPartition.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"]\n... ).repartitionByRange(2, \"age\").select(\n...     \"age\", \"name\", sf.spark_partition_id()\n... ).show()\n+---+-----+--------------------+\n|age| name|SPARK_PARTITION_ID()|\n+---+-----+--------------------+\n| 14|  Tom|                   0|\n| 16|  Bob|                   0|\n| 23|Alice|                   1|\n+---+-----+--------------------+"], "Parameters": [["numPartitions int", "can be an int to specify the target number of partitions or a Column.\nIf it is a Column, it will be used as the first partitioning column. If not specified,\nthe default number of partitions is used."], ["cols str or Column", "partitioning columns."]], "Returns": [["DataFrame", "Repartitioned DataFrame."]], "Category": ["DataFrame"], "index": 557}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.version.html#pyspark.sql.functions.version"], "Title": ["version"], "Feature": ["version"], "Description": "Returns the Spark version. The string contains 2 fields, the first being a release version\nand the second being a git revision.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(1).select(sf.version()).show(truncate=False) \n+----------------------------------------------+\n|version()                                     |\n+----------------------------------------------+\n|4.0.0 4f8d1f575e99aeef8990c63a9614af0fc5479330|\n+----------------------------------------------+"], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 558}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.call_udf.html#pyspark.sql.functions.call_udf"], "Title": ["call_udf"], "Feature": ["call_udf"], "Description": "Call a user-defined function.", "Examples": [">>> from pyspark.sql.functions import call_udf, col\n>>> from pyspark.sql.types import IntegerType, StringType\n>>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n>>> _ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n>>> df.select(call_udf(\"intX2\", \"id\")).show()\n+---------+\n|intX2(id)|\n+---------+\n|        2|\n|        4|\n|        6|\n+---------+\n>>> _ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n>>> df.select(call_udf(\"strX2\", col(\"name\"))).show()\n+-----------+\n|strX2(name)|\n+-----------+\n|         aa|\n|         bb|\n|         cc|\n+-----------+"], "Parameters": [["udfName str", "name of the user defined function (UDF)"], ["cols Column or str", "column names or Column s to be used in the UDF"]], "Returns": [["Column", "result of executed udf."]], "Category": ["Functions"], "index": 559}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf"], "Title": ["pandas_udf"], "Feature": ["pandas_udf"], "Description": "Creates a pandas user defined function (a.k.a. vectorized user defined function).\nPandas UDFs are user defined functions that are executed by Spark using Arrow to transfer\ndata and Pandas to work with the data, which allows vectorized operations. A Pandas UDF\nis defined using thepandas_udfas a decorator or to wrap the function, and no\nadditional configuration is required. A Pandas UDF behaves as a regular PySpark function\nAPI in general.\nSee alsopyspark.sql.GroupedData.aggpyspark.sql.DataFrame.mapInPandaspyspark.sql.GroupedData.applyInPandaspyspark.sql.PandasCogroupedOps.applyInPandaspyspark.sql.UDFRegistration.register\nNotes\nThe user-defined functions do not support conditional expressions or short circuiting\nin boolean expressions and it ends up with being executed all internally. If the functions\ncan fail on special rows, the workaround is to incorporate the condition into the functions.\nThe user-defined functions do not take keyword arguments on the calling side.\nThe data type of returnedpandas.Seriesfrom the user-defined functions should be\nmatched with definedreturnType(seetypes.to_arrow_type()andtypes.from_arrow_type()). When there is mismatch between them, Spark might do\nconversion on returned data. The conversion is not guaranteed to be correct and results\nshould be checked for accuracy by users.\nCurrently,pyspark.sql.types.ArrayTypeofpyspark.sql.types.TimestampTypeand\nnestedpyspark.sql.types.StructTypeare currently not supported as output types.", "Examples": [">>> import pandas as pd\n>>> from pyspark.sql.functions import pandas_udf", ">>> @pandas_udf(IntegerType())\n... def slen(s: pd.Series) -> pd.Series:\n...     return s.str.len()", ">>> from pyspark.sql.functions import PandasUDFType\n>>> from pyspark.sql.types import IntegerType\n>>> @pandas_udf(IntegerType(), PandasUDFType.SCALAR)\n... def slen(s):\n...     return s.str.len()", ">>> @pandas_udf(\"col1 string, col2 long\")\n>>> def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n...     s3['col2'] = s1 + s2.str.len()\n...     return s3\n...\n>>> # Create a Spark DataFrame that has three columns including a struct column.\n... df = spark.createDataFrame(\n...     [[1, \"a string\", (\"a nested string\",)]],\n...     \"long_col long, string_col string, struct_col struct<col1:string>\")\n>>> df.printSchema()\nroot\n|-- long_column: long (nullable = true)\n|-- string_column: string (nullable = true)\n|-- struct_column: struct (nullable = true)\n|    |-- col1: string (nullable = true)\n>>> df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()\n|-- func(long_col, string_col, struct_col): struct (nullable = true)\n|    |-- col1: string (nullable = true)\n|    |-- col2: long (nullable = true)", ">>> @pandas_udf(\"string\")\n... def to_upper(s: pd.Series) -> pd.Series:\n...     return s.str.upper()\n...\n>>> df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n>>> df.select(to_upper(\"name\")).show()\n+--------------+\n|to_upper(name)|\n+--------------+\n|      JOHN DOE|\n+--------------+", ">>> @pandas_udf(\"first string, last string\")\n... def split_expand(s: pd.Series) -> pd.DataFrame:\n...     return s.str.split(expand=True)\n...\n>>> df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n>>> df.select(split_expand(\"name\")).show()\n+------------------+\n|split_expand(name)|\n+------------------+\n|       {John, Doe}|\n+------------------+", ">>> @pandas_udf(returnType=IntegerType())\n... def calc(a: pd.Series, b: pd.Series) -> pd.Series:\n...     return a + 10 * b\n...\n>>> spark.range(2).select(calc(b=col(\"id\") * 10, a=col(\"id\"))).show()\n+-----------------------------+\n|calc(b => (id * 10), a => id)|\n+-----------------------------+\n|                            0|\n|                          101|\n+-----------------------------+", "@pandas_udf(\"long\")\ndef calculate(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n    # Do some expensive initialization with a state\n    state = very_expensive_initialization()\n    for x in iterator:\n        # Use that state for whole iterator.\n        yield calculate_with_state(x, state)\ndf.select(calculate(\"value\")).show()", ">>> from typing import Iterator\n>>> @pandas_udf(\"long\")\n... def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n...     for s in iterator:\n...         yield s + 1\n...\n>>> df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n>>> df.select(plus_one(df.v)).show()\n+-----------+\n|plus_one(v)|\n+-----------+\n|          2|\n|          3|\n|          4|\n+-----------+", ">>> from typing import Iterator, Tuple\n>>> from pyspark.sql.functions import struct, col\n>>> @pandas_udf(\"long\")\n... def multiply(iterator: Iterator[Tuple[pd.Series, pd.DataFrame]]) -> Iterator[pd.Series]:\n...     for s1, df in iterator:\n...         yield s1 * df.v\n...\n>>> df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n>>> df.withColumn('output', multiply(col(\"v\"), struct(col(\"v\")))).show()\n+---+------+\n|  v|output|\n+---+------+\n|  1|     1|\n|  2|     4|\n|  3|     9|\n+---+------+", ">>> @pandas_udf(\"double\")\n... def mean_udf(v: pd.Series) -> float:\n...     return v.mean()\n...\n>>> df = spark.createDataFrame(\n...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n>>> df.groupby(\"id\").agg(mean_udf(df['v'])).show()\n+---+-----------+\n| id|mean_udf(v)|\n+---+-----------+\n|  1|        1.5|\n|  2|        6.0|\n+---+-----------+", ">>> @pandas_udf(\"double\")\n... def weighted_mean_udf(v: pd.Series, w: pd.Series) -> float:\n...     import numpy as np\n...     return np.average(v, weights=w)\n...\n>>> df = spark.createDataFrame(\n...     [(1, 1.0, 1.0), (1, 2.0, 2.0), (2, 3.0, 1.0), (2, 5.0, 2.0), (2, 10.0, 3.0)],\n...     (\"id\", \"v\", \"w\"))\n>>> df.groupby(\"id\").agg(weighted_mean_udf(w=df[\"w\"], v=df[\"v\"])).show()\n+---+---------------------------------+\n| id|weighted_mean_udf(w => w, v => v)|\n+---+---------------------------------+\n|  1|               1.6666666666666667|\n|  2|                7.166666666666667|\n+---+---------------------------------+", ">>> from pyspark.sql import Window\n>>> @pandas_udf(\"double\")\n... def mean_udf(v: pd.Series) -> float:\n...     return v.mean()\n...\n>>> df = spark.createDataFrame(\n...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n>>> w = Window.partitionBy('id').orderBy('v').rowsBetween(-1, 0)\n>>> df.withColumn('mean_v', mean_udf(\"v\").over(w)).show()\n+---+----+------+\n| id|   v|mean_v|\n+---+----+------+\n|  1| 1.0|   1.0|\n|  1| 2.0|   1.5|\n|  2| 3.0|   3.0|\n|  2| 5.0|   4.0|\n|  2|10.0|   7.5|\n+---+----+------+"], "Parameters": [["f function, optional", "user-defined function. A python function if used as a standalone function"], ["returnType pyspark.sql.types.DataType or str, optional", "the return type of the user-defined function. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string."], ["functionType int, optional", "an enum value in pyspark.sql.functions.PandasUDFType .\nDefault: SCALAR. This parameter exists for compatibility.\nUsing Python type hints is encouraged."]], "Returns": [], "Category": ["Functions"], "index": 560}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html#pyspark.sql.functions.udf"], "Title": ["udf"], "Feature": ["udf"], "Description": "Creates a user defined function (UDF).\nNotes\nThe user-defined functions are considered deterministic by default. Due to\noptimization, duplicate invocations may be eliminated or the function may even be invoked\nmore times than it is present in the query. If your function is not deterministic, callasNondeterministicon the user defined function. E.g.:\n>>>frompyspark.sql.typesimportIntegerType>>>importrandom>>>random_udf=udf(lambda:int(random.random()*100),IntegerType()).asNondeterministic()Copy to clipboard\nThe user-defined functions do not support conditional expressions or short circuiting\nin boolean expressions and it ends up with being executed all internally. If the functions\ncan fail on special rows, the workaround is to incorporate the condition into the functions.\nThe user-defined functions do not take keyword arguments on the calling side.", "Examples": [">>> from pyspark.sql.types import IntegerType\n>>> slen = udf(lambda s: len(s), IntegerType())\n>>> @udf\n... def to_upper(s):\n...     if s is not None:\n...         return s.upper()\n...\n>>> @udf(returnType=IntegerType())\n... def add_one(x):\n...     if x is not None:\n...         return x + 1\n...\n>>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n>>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n+----------+--------------+------------+\n|slen(name)|to_upper(name)|add_one(age)|\n+----------+--------------+------------+\n|         8|      JOHN DOE|          22|\n+----------+--------------+------------+", ">>> @udf(returnType=IntegerType())\n... def calc(a, b):\n...     return a + 10 * b\n...\n>>> spark.range(2).select(calc(b=col(\"id\") * 10, a=col(\"id\"))).show()\n+-----------------------------+\n|calc(b => (id * 10), a => id)|\n+-----------------------------+\n|                            0|\n|                          101|\n+-----------------------------+"], "Parameters": [["f function, optional", "python function if used as a standalone function"], ["returnType pyspark.sql.types.DataType or str, optional", "the return type of the user-defined function. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\nDefaults to StringType ."], ["useArrow bool, optional", "whether to use Arrow to optimize the (de)serialization. When it is None, the\nSpark config “spark.sql.execution.pythonUDF.arrow.enabled” takes effect."]], "Returns": [], "Category": ["Functions"], "index": 561}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udtf.html#pyspark.sql.functions.udtf"], "Title": ["udtf"], "Feature": ["udtf"], "Description": "Creates a user defined table function (UDTF).\nNotes\nUser-defined table functions (UDTFs) are considered non-deterministic by default.\nUseasDeterministic()to mark a function as deterministic. E.g.:\n>>>classPlusOne:...defeval(self,a:int):...yielda+1,>>>plus_one=udtf(PlusOne,returnType=\"r: int\").asDeterministic()Copy to clipboard\nUse “yield” to produce one row for the UDTF result relation as many times\nas needed. In the context of a lateral join, each such result row will be\nassociated with the most recent input row consumed from the “eval” method.\nUser-defined table functions are considered opaque to the optimizer by default.\nAs a result, operations like filters from WHERE clauses or limits from\nLIMIT/OFFSET clauses that appear after the UDTF call will execute on the\nUDTF’s result relation. By the same token, any relations forwarded as input\nto UDTFs will plan as full table scans in the absence of any explicit such\nfiltering or other logic explicitly written in a table subquery surrounding the\nprovided input relation.\nUser-defined table functions do not accept keyword arguments on the calling side.", "Examples": [">>> class TestUDTF:\n...     def eval(self, *args: Any):\n...         yield \"hello\", \"world\"\n...\n>>> from pyspark.sql.functions import udtf\n>>> test_udtf = udtf(TestUDTF, returnType=\"c1: string, c2: string\")\n>>> test_udtf().show()\n+-----+-----+\n|   c1|   c2|\n+-----+-----+\n|hello|world|\n+-----+-----+", ">>> @udtf(returnType=\"c1: int, c2: int\")\n... class PlusOne:\n...     def eval(self, x: int):\n...         yield x, x + 1\n...\n>>> from pyspark.sql.functions import lit\n>>> PlusOne(lit(1)).show()\n+---+---+\n| c1| c2|\n+---+---+\n|  1|  2|\n+---+---+", ">>> from pyspark.sql.udtf import AnalyzeArgument, AnalyzeResult\n>>> # or from pyspark.sql.functions import AnalyzeArgument, AnalyzeResult\n>>> @udtf\n... class TestUDTFWithAnalyze:\n...     @staticmethod\n...     def analyze(a: AnalyzeArgument, b: AnalyzeArgument) -> AnalyzeResult:\n...         return AnalyzeResult(StructType().add(\"a\", a.dataType).add(\"b\", b.dataType))\n...\n...     def eval(self, a, b):\n...         yield a, b\n...\n>>> TestUDTFWithAnalyze(lit(1), lit(\"x\")).show()\n+---+---+\n|  a|  b|\n+---+---+\n|  1|  x|\n+---+---+", ">>> @udtf\n... class TestUDTFWithKwargs:\n...     @staticmethod\n...     def analyze(\n...         a: AnalyzeArgument, b: AnalyzeArgument, **kwargs: AnalyzeArgument\n...     ) -> AnalyzeResult:\n...         return AnalyzeResult(\n...             StructType().add(\"a\", a.dataType)\n...                 .add(\"b\", b.dataType)\n...                 .add(\"x\", kwargs[\"x\"].dataType)\n...         )\n...\n...     def eval(self, a, b, **kwargs):\n...         yield a, b, kwargs[\"x\"]\n...\n>>> TestUDTFWithKwargs(lit(1), x=lit(\"x\"), b=lit(\"b\")).show()\n+---+---+---+\n|  a|  b|  x|\n+---+---+---+\n|  1|  b|  x|\n+---+---+---+", ">>> _ = spark.udtf.register(\"test_udtf\", TestUDTFWithKwargs)\n>>> spark.sql(\"SELECT * FROM test_udtf(1, x => 'x', b => 'b')\").show()\n+---+---+---+\n|  a|  b|  x|\n+---+---+---+\n|  1|  b|  x|\n+---+---+---+", ">>> @udtf(returnType=\"c1: int, c2: int\", useArrow=True)\n... class ArrowPlusOne:\n...     def eval(self, x: int):\n...         yield x, x + 1\n...\n>>> ArrowPlusOne(lit(1)).show()\n+---+---+\n| c1| c2|\n+---+---+\n|  1|  2|\n+---+---+"], "Parameters": [["cls class, optional", "the Python user-defined table function handler class."], ["returnType pyspark.sql.types.StructType or str, optional", "the return type of the user-defined table function. The value can be either a pyspark.sql.types.StructType object or a DDL-formatted struct type string.\nIf None, the handler class must provide analyze static method."], ["useArrow bool, optional", "whether to use Arrow to optimize the (de)serializations. When it’s set to None, the\nSpark config “spark.sql.execution.pythonUDTF.arrow.enabled” is used."]], "Returns": [], "Category": ["Functions"], "index": 562}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.unwrap_udt.html#pyspark.sql.functions.unwrap_udt"], "Title": ["unwrap_udt"], "Feature": ["unwrap_udt"], "Description": "Unwrap UDT data type column into its underlying type.", "Examples": [], "Parameters": [], "Returns": [], "Category": ["Functions"], "index": 563}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.collations.html#pyspark.sql.tvf.TableValuedFunction.collations"], "Title": ["TableValuedFunction.collations"], "Feature": ["TableValuedFunction.collations"], "Description": "Get all of the Spark SQL string collations.", "Examples": [">>> spark.tvf.collations().show()\n+-------+-------+-------------------+...\n|CATALOG| SCHEMA|               NAME|...\n+-------+-------+-------------------+...\n...\n+-------+-------+-------------------+..."], "Parameters": [], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 564}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.explode.html#pyspark.sql.tvf.TableValuedFunction.explode"], "Title": ["TableValuedFunction.explode"], "Feature": ["TableValuedFunction.explode"], "Description": "Returns aDataFramecontaining a new row for each element\nin the given array or map.\nUses the default column namecolfor elements in the array andkeyandvaluefor elements in the map unless specified otherwise.\nSee alsopyspark.sql.functions.explode()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.tvf.explode(sf.array(sf.lit(1), sf.lit(2), sf.lit(3))).show()\n+---+\n|col|\n+---+\n|  1|\n|  2|\n|  3|\n+---+", ">>> import pyspark.sql.functions as sf\n>>> spark.tvf.explode(\n...     sf.create_map(sf.lit(\"a\"), sf.lit(\"b\"), sf.lit(\"c\"), sf.lit(\"d\"))\n... ).show()\n+---+-----+\n|key|value|\n+---+-----+\n|  a|    b|\n|  c|    d|\n+---+-----+", ">>> import pyspark.sql.functions as sf\n>>> spark.tvf.explode(sf.array(\n...     sf.named_struct(sf.lit(\"a\"), sf.lit(1), sf.lit(\"b\"), sf.lit(2)),\n...     sf.named_struct(sf.lit(\"a\"), sf.lit(3), sf.lit(\"b\"), sf.lit(4))\n... )).select(\"col.*\").show()\n+---+---+\n|  a|  b|\n+---+---+\n|  1|  2|\n|  3|  4|\n+---+---+", ">>> import pyspark.sql.functions as sf\n>>> spark.tvf.explode(sf.array()).show()\n+---+\n|col|\n+---+\n+---+", ">>> import pyspark.sql.functions as sf\n>>> spark.tvf.explode(sf.create_map()).show()\n+---+-----+\n|key|value|\n+---+-----+\n+---+-----+"], "Parameters": [["collection Column", "Target column to work on."]], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 565}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.explode_outer.html#pyspark.sql.tvf.TableValuedFunction.explode_outer"], "Title": ["TableValuedFunction.explode_outer"], "Feature": ["TableValuedFunction.explode_outer"], "Description": "Returns aDataFramecontaining a new row for each element with position\nin the given array or map.\nUnlike explode, if the array/map is null or empty then null is produced.\nUses the default column namecolfor elements in the array andkeyandvaluefor elements in the map unless specified otherwise.\nSee alsopyspark.sql.functions.explode_outer()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.tvf.explode_outer(sf.array(sf.lit(\"foo\"), sf.lit(\"bar\"))).show()\n+---+\n|col|\n+---+\n|foo|\n|bar|\n+---+\n>>> spark.tvf.explode_outer(sf.array()).show()\n+----+\n| col|\n+----+\n|NULL|\n+----+\n>>> spark.tvf.explode_outer(sf.create_map(sf.lit(\"x\"), sf.lit(1.0))).show()\n+---+-----+\n|key|value|\n+---+-----+\n|  x|  1.0|\n+---+-----+\n>>> spark.tvf.explode_outer(sf.create_map()).show()\n+----+-----+\n| key|value|\n+----+-----+\n|NULL| NULL|\n+----+-----+"], "Parameters": [["collection Column", "target column to work on."]], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 566}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.inline.html#pyspark.sql.tvf.TableValuedFunction.inline"], "Title": ["TableValuedFunction.inline"], "Feature": ["TableValuedFunction.inline"], "Description": "Explodes an array of structs into a table.\nThis function takes an input column containing an array of structs and returns a\nnew column where each struct in the array is exploded into a separate row.\nSee alsopyspark.sql.functions.inline()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.tvf.inline(sf.array(\n...     sf.named_struct(sf.lit(\"a\"), sf.lit(1), sf.lit(\"b\"), sf.lit(2)),\n...     sf.named_struct(sf.lit(\"a\"), sf.lit(3), sf.lit(\"b\"), sf.lit(4))\n... )).show()\n+---+---+\n|  a|  b|\n+---+---+\n|  1|  2|\n|  3|  4|\n+---+---+", ">>> import pyspark.sql.functions as sf\n>>> spark.tvf.inline(sf.array().astype(\"array<struct<a:int,b:int>>\")).show()\n+---+---+\n|  a|  b|\n+---+---+\n+---+---+", ">>> import pyspark.sql.functions as sf\n>>> spark.tvf.inline(sf.array(\n...     sf.named_struct(sf.lit(\"a\"), sf.lit(1), sf.lit(\"b\"), sf.lit(2)),\n...     sf.lit(None),\n...     sf.named_struct(sf.lit(\"a\"), sf.lit(3), sf.lit(\"b\"), sf.lit(4))\n... )).show()\n+----+----+\n|   a|   b|\n+----+----+\n|   1|   2|\n|NULL|NULL|\n|   3|   4|\n+----+----+"], "Parameters": [["input Column", "Input column of values to explode."]], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 567}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.replace.html#pyspark.sql.DataFrame.replace"], "Title": ["DataFrame.replace"], "Feature": ["DataFrame.replace"], "Description": "Returns a newDataFramereplacing a value with another value.DataFrame.replace()andDataFrameNaFunctions.replace()are\naliases of each other.\nValues to_replace and value must have the same type and can only be numerics, booleans,\nor strings. Value can have None. When replacing, the new value will be cast\nto the type of the existing column.\nFor numeric replacements all values to be replaced should have unique\nfloating point representation. In case of conflicts (for example with{42: -1, 42.0: 1})\nand arbitrary replacement will be used.", "Examples": [">>> df = spark.createDataFrame([\n...     (10, 80, \"Alice\"),\n...     (5, None, \"Bob\"),\n...     (None, 10, \"Tom\"),\n...     (None, None, None)],\n...     schema=[\"age\", \"height\", \"name\"])", ">>> df.na.replace(10, 20).show()\n+----+------+-----+\n| age|height| name|\n+----+------+-----+\n|  20|    80|Alice|\n|   5|  NULL|  Bob|\n|NULL|    20|  Tom|\n|NULL|  NULL| NULL|\n+----+------+-----+", ">>> df.na.replace('Alice', None).show()\n+----+------+----+\n| age|height|name|\n+----+------+----+\n|  10|    80|NULL|\n|   5|  NULL| Bob|\n|NULL|    10| Tom|\n|NULL|  NULL|NULL|\n+----+------+----+", ">>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n+----+------+----+\n| age|height|name|\n+----+------+----+\n|  10|    80|   A|\n|   5|  NULL|   B|\n|NULL|    10| Tom|\n|NULL|  NULL|NULL|\n+----+------+----+", ">>> df.na.replace(10, 18, 'age').show()\n+----+------+-----+\n| age|height| name|\n+----+------+-----+\n|  18|    80|Alice|\n|   5|  NULL|  Bob|\n|NULL|    10|  Tom|\n|NULL|  NULL| NULL|\n+----+------+-----+"], "Parameters": [["to_replace bool, int, float, string, list or dict, the value to be replaced.", "If the value is a dict, then value is ignored or can be omitted, and to_replace must be a mapping between a value and a replacement."], ["value bool, int, float, string or None, optional", "The replacement value must be a bool, int, float, string or None. If value is a\nlist, value should be of the same length and type as to_replace .\nIf value is a scalar and to_replace is a sequence, then value is\nused as a replacement for each item in to_replace ."], ["subset list, optional", "optional list of column names to consider.\nColumns specified in subset that do not have matching data types are ignored.\nFor example, if value is a string, and subset contains a non-string column,\nthen the non-string column is simply ignored."]], "Returns": [["DataFrame", "DataFrame with replaced values."]], "Category": ["DataFrame"], "index": 568}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.inline_outer.html#pyspark.sql.tvf.TableValuedFunction.inline_outer"], "Title": ["TableValuedFunction.inline_outer"], "Feature": ["TableValuedFunction.inline_outer"], "Description": "Explodes an array of structs into a table.\nUnlike inline, if the array is null or empty then null is produced for each nested column.\nSee alsopyspark.sql.functions.inline_outer()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.tvf.inline_outer(sf.array(\n...     sf.named_struct(sf.lit(\"a\"), sf.lit(1), sf.lit(\"b\"), sf.lit(2)),\n...     sf.named_struct(sf.lit(\"a\"), sf.lit(3), sf.lit(\"b\"), sf.lit(4))\n... )).show()\n+---+---+\n|  a|  b|\n+---+---+\n|  1|  2|\n|  3|  4|\n+---+---+\n>>> spark.tvf.inline_outer(sf.array().astype(\"array<struct<a:int,b:int>>\")).show()\n+----+----+\n|   a|   b|\n+----+----+\n|NULL|NULL|\n+----+----+"], "Parameters": [["input Column", "input column of values to explode."]], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 569}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.json_tuple.html#pyspark.sql.tvf.TableValuedFunction.json_tuple"], "Title": ["TableValuedFunction.json_tuple"], "Feature": ["TableValuedFunction.json_tuple"], "Description": "Creates a new row for a json column according to the given field names.\nSee alsopyspark.sql.functions.json_tuple()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.tvf.json_tuple(\n...     sf.lit('{\"f1\": \"value1\", \"f2\": \"value2\"}'), sf.lit(\"f1\"), sf.lit(\"f2\")\n... ).show()\n+------+------+\n|    c0|    c1|\n+------+------+\n|value1|value2|\n+------+------+"], "Parameters": [["input Column", "string column in json format"], ["fields Column", "a field or fields to extract"]], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 570}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.posexplode.html#pyspark.sql.tvf.TableValuedFunction.posexplode"], "Title": ["TableValuedFunction.posexplode"], "Feature": ["TableValuedFunction.posexplode"], "Description": "Returns aDataFramecontaining a new row for each element with position\nin the given array or map.\nUses the default column nameposfor position, andcolfor elements in the\narray andkeyandvaluefor elements in the map unless specified otherwise.\nSee alsopyspark.sql.functions.posexplode()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.tvf.posexplode(sf.array(sf.lit(1), sf.lit(2), sf.lit(3))).show()\n+---+---+\n|pos|col|\n+---+---+\n|  0|  1|\n|  1|  2|\n|  2|  3|\n+---+---+\n>>> spark.tvf.posexplode(sf.create_map(sf.lit(\"a\"), sf.lit(\"b\"))).show()\n+---+---+-----+\n|pos|key|value|\n+---+---+-----+\n|  0|  a|    b|\n+---+---+-----+"], "Parameters": [["collection Column", "target column to work on."]], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 571}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.posexplode_outer.html#pyspark.sql.tvf.TableValuedFunction.posexplode_outer"], "Title": ["TableValuedFunction.posexplode_outer"], "Feature": ["TableValuedFunction.posexplode_outer"], "Description": "Returns aDataFramecontaining a new row for each element with position\nin the given array or map.\nUnlike posexplode, if the array/map is null or empty then the row (null, null) is produced.\nUses the default column nameposfor position, andcolfor elements in the\narray andkeyandvaluefor elements in the map unless specified otherwise.\nSee alsopyspark.sql.functions.posexplode_outer()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.tvf.posexplode_outer(sf.array(sf.lit(\"foo\"), sf.lit(\"bar\"))).show()\n+---+---+\n|pos|col|\n+---+---+\n|  0|foo|\n|  1|bar|\n+---+---+\n>>> spark.tvf.posexplode_outer(sf.array()).show()\n+----+----+\n| pos| col|\n+----+----+\n|NULL|NULL|\n+----+----+\n>>> spark.tvf.posexplode_outer(sf.create_map(sf.lit(\"x\"), sf.lit(1.0))).show()\n+---+---+-----+\n|pos|key|value|\n+---+---+-----+\n|  0|  x|  1.0|\n+---+---+-----+\n>>> spark.tvf.posexplode_outer(sf.create_map()).show()\n+----+----+-----+\n| pos| key|value|\n+----+----+-----+\n|NULL|NULL| NULL|\n+----+----+-----+"], "Parameters": [["collection Column", "target column to work on."]], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 572}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.range.html#pyspark.sql.tvf.TableValuedFunction.range"], "Title": ["TableValuedFunction.range"], "Feature": ["TableValuedFunction.range"], "Description": "Create aDataFramewith singlepyspark.sql.types.LongTypecolumn namedid, containing elements in a range fromstarttoend(exclusive) with\nstep valuestep.", "Examples": [">>> spark.tvf.range(1, 7, 2).show()\n+---+\n| id|\n+---+\n|  1|\n|  3|\n|  5|\n+---+", ">>> spark.tvf.range(3).show()\n+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n+---+"], "Parameters": [["start int", "the start value"], ["end int, optional", "the end value (exclusive)"], ["step int, optional", "the incremental step (default: 1)"], ["numPartitions int, optional", "the number of partitions of the DataFrame"]], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 573}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.sql_keywords.html#pyspark.sql.tvf.TableValuedFunction.sql_keywords"], "Title": ["TableValuedFunction.sql_keywords"], "Feature": ["TableValuedFunction.sql_keywords"], "Description": "Get Spark SQL keywords.", "Examples": [">>> spark.tvf.sql_keywords().show()\n+-------------+--------+\n|      keyword|reserved|\n+-------------+--------+\n...\n+-------------+--------+..."], "Parameters": [], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 574}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.stack.html#pyspark.sql.tvf.TableValuedFunction.stack"], "Title": ["TableValuedFunction.stack"], "Feature": ["TableValuedFunction.stack"], "Description": "Separatescol1, …,colkintonrows. Uses column names col0, col1, etc. by default\nunless specified otherwise.\nSee alsopyspark.sql.functions.stack()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.tvf.stack(sf.lit(2), sf.lit(1), sf.lit(2), sf.lit(3)).show()\n+----+----+\n|col0|col1|\n+----+----+\n|   1|   2|\n|   3|NULL|\n+----+----+"], "Parameters": [["n Column", "the number of rows to separate"], ["fields Column", "input elements to be separated"]], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 575}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.variant_explode.html#pyspark.sql.tvf.TableValuedFunction.variant_explode"], "Title": ["TableValuedFunction.variant_explode"], "Feature": ["TableValuedFunction.variant_explode"], "Description": "Separates a variant object/array into multiple rows containing its fields/elements.\nIts result schema isstruct<pos int, key string, value variant>.posis the position of\nthe field/element in its parent object/array, andvalueis the field/element value.keyis the field name when exploding a variant object, or is NULL when exploding a variant\narray. It ignores any input that is not a variant array/object, including SQL NULL, variant\nnull, and any other variant values.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.tvf.variant_explode(sf.parse_json(sf.lit('[\"hello\", \"world\"]'))).show()\n+---+----+-------+\n|pos| key|  value|\n+---+----+-------+\n|  0|NULL|\"hello\"|\n|  1|NULL|\"world\"|\n+---+----+-------+", ">>> import pyspark.sql.functions as sf\n>>> spark.tvf.variant_explode(sf.parse_json(sf.lit('{\"a\": true, \"b\": 3.14}'))).show()\n+---+---+-----+\n|pos|key|value|\n+---+---+-----+\n|  0|  a| true|\n|  1|  b| 3.14|\n+---+---+-----+", ">>> import pyspark.sql.functions as sf\n>>> spark.tvf.variant_explode(sf.parse_json(sf.lit('[]'))).show()\n+---+---+-----+\n|pos|key|value|\n+---+---+-----+\n+---+---+-----+", ">>> import pyspark.sql.functions as sf\n>>> spark.tvf.variant_explode(sf.parse_json(sf.lit('{}'))).show()\n+---+---+-----+\n|pos|key|value|\n+---+---+-----+\n+---+---+-----+"], "Parameters": [["input Column", "input column of values to explode."]], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 576}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.tvf.TableValuedFunction.variant_explode_outer.html#pyspark.sql.tvf.TableValuedFunction.variant_explode_outer"], "Title": ["TableValuedFunction.variant_explode_outer"], "Feature": ["TableValuedFunction.variant_explode_outer"], "Description": "Separates a variant object/array into multiple rows containing its fields/elements.\nIts result schema isstruct<pos int, key string, value variant>.posis the position of\nthe field/element in its parent object/array, andvalueis the field/element value.keyis the field name when exploding a variant object, or is NULL when exploding a variant\narray. Unlike variant_explode, if the given variant is not a variant array/object, including\nSQL NULL, variant null, and any other variant values, then NULL is produced.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> spark.tvf.variant_explode_outer(sf.parse_json(sf.lit('[\"hello\", \"world\"]'))).show()\n+---+----+-------+\n|pos| key|  value|\n+---+----+-------+\n|  0|NULL|\"hello\"|\n|  1|NULL|\"world\"|\n+---+----+-------+\n>>> spark.tvf.variant_explode_outer(sf.parse_json(sf.lit('[]'))).show()\n+----+----+-----+\n| pos| key|value|\n+----+----+-----+\n|NULL|NULL| NULL|\n+----+----+-----+\n>>> spark.tvf.variant_explode_outer(sf.parse_json(sf.lit('{\"a\": true, \"b\": 3.14}'))).show()\n+---+---+-----+\n|pos|key|value|\n+---+---+-----+\n|  0|  a| true|\n|  1|  b| 3.14|\n+---+---+-----+\n>>> spark.tvf.variant_explode_outer(sf.parse_json(sf.lit('{}'))).show()\n+----+----+-----+\n| pos| key|value|\n+----+----+-----+\n|NULL|NULL| NULL|\n+----+----+-----+"], "Parameters": [["input Column", "input column of values to explode."]], "Returns": [["DataFrame", ""]], "Category": ["Functions"], "index": 577}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.agg.html#pyspark.sql.GroupedData.agg"], "Title": ["GroupedData.agg"], "Feature": ["GroupedData.agg"], "Description": "Compute aggregates and returns the result as aDataFrame.\nThe available aggregate functions can be:\nIfexprsis a singledictmapping from string to string, then the key\nis the column to perform aggregation on, and the value is the aggregate function.\nAlternatively,exprscan also be a list of aggregateColumnexpressions.\nNotes\nBuilt-in aggregation functions and group aggregate pandas UDFs cannot be mixed\nin a single call to this function.", "Examples": [">>> import pandas as pd  \n>>> from pyspark.sql import functions as sf\n>>> from pyspark.sql.functions import pandas_udf\n>>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (3, \"Alice\"), (5, \"Bob\"), (10, \"Bob\")], [\"age\", \"name\"])\n>>> df.show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  3|Alice|\n|  5|  Bob|\n| 10|  Bob|\n+---+-----+", ">>> df.groupBy(df.name)\nGroupedData[grouping...: [name...], value: [age: bigint, name: string], type: GroupBy]", ">>> df.groupBy(df.name).agg({\"*\": \"count\"}).sort(\"name\").show()\n+-----+--------+\n| name|count(1)|\n+-----+--------+\n|Alice|       2|\n|  Bob|       2|\n+-----+--------+", ">>> df.groupBy(df.name).agg(sf.min(df.age)).sort(\"name\").show()\n+-----+--------+\n| name|min(age)|\n+-----+--------+\n|Alice|       2|\n|  Bob|       5|\n+-----+--------+", ">>> @pandas_udf('int')  \n... def min_udf(v: pd.Series) -> int:\n...     return v.min()\n...\n>>> df.groupBy(df.name).agg(min_udf(df.age)).sort(\"name\").show()  \n+-----+------------+\n| name|min_udf(age)|\n+-----+------------+\n|Alice|           2|\n|  Bob|           5|\n+-----+------------+"], "Parameters": [["exprs dict", "a dict mapping from column name (string) to aggregate functions (string),\nor a list of Column ."]], "Returns": [], "Category": ["Grouping"], "index": 578}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.rollup.html#pyspark.sql.DataFrame.rollup"], "Title": ["DataFrame.rollup"], "Feature": ["DataFrame.rollup"], "Description": "Create a multi-dimensional rollup for the currentDataFrameusing\nthe specified columns, allowing for aggregation on them.\nNotes\nA column ordinal starts from 1, which is different from the\n0-based__getitem__().", "Examples": [">>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], schema=[\"name\", \"age\"])", ">>> df.rollup(\"name\").count().orderBy(\"name\").show()\n+-----+-----+\n| name|count|\n+-----+-----+\n| NULL|    2|\n|Alice|    1|\n|  Bob|    1|\n+-----+-----+", ">>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n+-----+----+-----+\n| name| age|count|\n+-----+----+-----+\n| NULL|NULL|    2|\n|Alice|NULL|    1|\n|Alice|   2|    1|\n|  Bob|NULL|    1|\n|  Bob|   5|    1|\n+-----+----+-----+", ">>> df.rollup(1, 2).count().orderBy(1, 2).show()\n+-----+----+-----+\n| name| age|count|\n+-----+----+-----+\n| NULL|NULL|    2|\n|Alice|NULL|    1|\n|Alice|   2|    1|\n|  Bob|NULL|    1|\n|  Bob|   5|    1|\n+-----+----+-----+"], "Parameters": [["cols list, str, int or Column", "The columns to roll-up by.\nEach element should be a column name (string) or an expression ( Column )\nor a column ordinal (int, 1-based) or list of them. Changed in version 4.0.0: Supports column ordinal."]], "Returns": [["GroupedData", "Rolled-up data based on the specified columns."]], "Category": ["DataFrame"], "index": 579}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.apply.html#pyspark.sql.GroupedData.apply"], "Title": ["GroupedData.apply"], "Feature": ["GroupedData.apply"], "Description": "It is an alias ofpyspark.sql.GroupedData.applyInPandas(); however, it takes apyspark.sql.functions.pandas_udf()whereaspyspark.sql.GroupedData.applyInPandas()takes a Python native function.\nSee alsopyspark.sql.functions.pandas_udf\nNotes\nIt is preferred to usepyspark.sql.GroupedData.applyInPandas()over this\nAPI. This API will be deprecated in the future releases.", "Examples": [">>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n>>> df = spark.createDataFrame(\n...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n...     (\"id\", \"v\"))\n>>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  \n... def normalize(pdf):\n...     v = pdf.v\n...     return pdf.assign(v=(v - v.mean()) / v.std())\n...\n>>> df.groupby(\"id\").apply(normalize).show()  \n+---+-------------------+\n| id|                  v|\n+---+-------------------+\n|  1|-0.7071067811865475|\n|  1| 0.7071067811865475|\n|  2|-0.8320502943378437|\n|  2|-0.2773500981126146|\n|  2| 1.1094003924504583|\n+---+-------------------+"], "Parameters": [["udf pyspark.sql.functions.pandas_udf()", "a grouped map user-defined function returned by pyspark.sql.functions.pandas_udf() ."]], "Returns": [], "Category": ["Grouping"], "index": 580}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.applyInArrow.html#pyspark.sql.GroupedData.applyInArrow"], "Title": ["GroupedData.applyInArrow"], "Feature": ["GroupedData.applyInArrow"], "Description": "Maps each group of the currentDataFrameusing an Arrow udf and returns the result\nas aDataFrame.\nThe function should take apyarrow.Tableand return anotherpyarrow.Table. Alternatively, the user can pass a function that takes\na tuple ofpyarrow.Scalargrouping key(s) and apyarrow.Table.\nFor each group, all columns are passed together as apyarrow.Tableto the user-function and the returnedpyarrow.Tableare combined as aDataFrame.\nTheschemashould be aStructTypedescribing the schema of the returnedpyarrow.Table. The column labels of the returnedpyarrow.Tablemust either match\nthe field names in the defined schema if specified as strings, or match the\nfield data types by position if not strings, e.g. integer indices.\nThe length of the returnedpyarrow.Tablecan be arbitrary.\nSee alsopyspark.sql.functions.pandas_udf\nNotes\nThis function requires a full shuffle. All the data of a group will be loaded\ninto memory, so the user should be aware of the potential OOM risk if data is skewed\nand certain groups are too large to fit in memory.\nThis API is unstable, and for developers.", "Examples": [">>> from pyspark.sql.functions import ceil\n>>> import pyarrow  \n>>> import pyarrow.compute as pc  \n>>> df = spark.createDataFrame(\n...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n...     (\"id\", \"v\"))  \n>>> def normalize(table):\n...     v = table.column(\"v\")\n...     norm = pc.divide(pc.subtract(v, pc.mean(v)), pc.stddev(v, ddof=1))\n...     return table.set_column(1, \"v\", norm)\n>>> df.groupby(\"id\").applyInArrow(\n...     normalize, schema=\"id long, v double\").show()  \n+---+-------------------+\n| id|                  v|\n+---+-------------------+\n|  1|-0.7071067811865475|\n|  1| 0.7071067811865475|\n|  2|-0.8320502943378437|\n|  2|-0.2773500981126146|\n|  2| 1.1094003924504583|\n+---+-------------------+", ">>> df = spark.createDataFrame(\n...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n...     (\"id\", \"v\"))  \n>>> def mean_func(key, table):\n...     # key is a tuple of one pyarrow.Int64Scalar, which is the value\n...     # of 'id' for the current group\n...     mean = pc.mean(table.column(\"v\"))\n...     return pyarrow.Table.from_pydict({\"id\": [key[0].as_py()], \"v\": [mean.as_py()]})\n>>> df.groupby('id').applyInArrow(\n...     mean_func, schema=\"id long, v double\")  \n+---+---+\n| id|  v|\n+---+---+\n|  1|1.5|\n|  2|6.0|\n+---+---+", ">>> def sum_func(key, table):\n...     # key is a tuple of two pyarrow.Int64Scalars, which is the values\n...     # of 'id' and 'ceil(df.v / 2)' for the current group\n...     sum = pc.sum(table.column(\"v\"))\n...     return pyarrow.Table.from_pydict({\n...         \"id\": [key[0].as_py()],\n...         \"ceil(v / 2)\": [key[1].as_py()],\n...         \"v\": [sum.as_py()]\n...     })\n>>> df.groupby(df.id, ceil(df.v / 2)).applyInArrow(\n...     sum_func, schema=\"id long, `ceil(v / 2)` long, v double\").show()  \n+---+-----------+----+\n| id|ceil(v / 2)|   v|\n+---+-----------+----+\n|  2|          5|10.0|\n|  1|          1| 3.0|\n|  2|          3| 5.0|\n|  2|          2| 3.0|\n+---+-----------+----+"], "Parameters": [["func function", "a Python native function that takes a pyarrow.Table and outputs a pyarrow.Table , or that takes one tuple (grouping keys) and a pyarrow.Table and outputs a pyarrow.Table ."], ["schema pyspark.sql.types.DataType or str", "the return type of the func in PySpark. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string."]], "Returns": [], "Category": ["Grouping"], "index": 581}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.applyInPandas.html#pyspark.sql.GroupedData.applyInPandas"], "Title": ["GroupedData.applyInPandas"], "Feature": ["GroupedData.applyInPandas"], "Description": "Maps each group of the currentDataFrameusing a pandas udf and returns the result\nas aDataFrame.\nThe function should take apandas.DataFrameand return anotherpandas.DataFrame. Alternatively, the user can pass a function that takes\na tuple of the grouping key(s) and apandas.DataFrame.\nFor each group, all columns are passed together as apandas.DataFrameto the user-function and the returnedpandas.DataFrameare combined as aDataFrame.\nTheschemashould be aStructTypedescribing the schema of the returnedpandas.DataFrame. The column labels of the returnedpandas.DataFramemust either match\nthe field names in the defined schema if specified as strings, or match the\nfield data types by position if not strings, e.g. integer indices.\nThe length of the returnedpandas.DataFramecan be arbitrary.\nSee alsopyspark.sql.functions.pandas_udf\nNotes\nThis function requires a full shuffle. All the data of a group will be loaded\ninto memory, so the user should be aware of the potential OOM risk if data is skewed\nand certain groups are too large to fit in memory.", "Examples": [">>> import pandas as pd  \n>>> from pyspark.sql.functions import ceil\n>>> df = spark.createDataFrame(\n...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n...     (\"id\", \"v\"))  \n>>> def normalize(pdf):\n...     v = pdf.v\n...     return pdf.assign(v=(v - v.mean()) / v.std())\n...\n>>> df.groupby(\"id\").applyInPandas(\n...     normalize, schema=\"id long, v double\").show()  \n+---+-------------------+\n| id|                  v|\n+---+-------------------+\n|  1|-0.7071067811865475|\n|  1| 0.7071067811865475|\n|  2|-0.8320502943378437|\n|  2|-0.2773500981126146|\n|  2| 1.1094003924504583|\n+---+-------------------+", ">>> df = spark.createDataFrame(\n...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n...     (\"id\", \"v\"))  \n>>> def mean_func(key, pdf):\n...     # key is a tuple of one numpy.int64, which is the value\n...     # of 'id' for the current group\n...     return pd.DataFrame([key + (pdf.v.mean(),)])\n...\n>>> df.groupby('id').applyInPandas(\n...     mean_func, schema=\"id long, v double\").show()  \n+---+---+\n| id|  v|\n+---+---+\n|  1|1.5|\n|  2|6.0|\n+---+---+", ">>> def sum_func(key, pdf):\n...     # key is a tuple of two numpy.int64s, which is the values\n...     # of 'id' and 'ceil(df.v / 2)' for the current group\n...     return pd.DataFrame([key + (pdf.v.sum(),)])\n...\n>>> df.groupby(df.id, ceil(df.v / 2)).applyInPandas(\n...     sum_func, schema=\"id long, `ceil(v / 2)` long, v double\").show()  \n+---+-----------+----+\n| id|ceil(v / 2)|   v|\n+---+-----------+----+\n|  2|          5|10.0|\n|  1|          1| 3.0|\n|  2|          3| 5.0|\n|  2|          2| 3.0|\n+---+-----------+----+"], "Parameters": [["func function", "a Python native function that takes a pandas.DataFrame and outputs a pandas.DataFrame , or that takes one tuple (grouping keys) and a pandas.DataFrame and outputs a pandas.DataFrame ."], ["schema pyspark.sql.types.DataType or str", "the return type of the func in PySpark. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string."]], "Returns": [], "Category": ["Grouping"], "index": 582}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.applyInPandasWithState.html#pyspark.sql.GroupedData.applyInPandasWithState"], "Title": ["GroupedData.applyInPandasWithState"], "Feature": ["GroupedData.applyInPandasWithState"], "Description": "Applies the given function to each group of data, while maintaining a user-defined\nper-group state. The result Dataset will represent the flattened record returned by the\nfunction.\nFor a streamingDataFrame, the function will be invoked first for all input groups\nand then for all timed out states where the input data is set to be empty. Updates to each\ngroup’s state will be saved across invocations.\nThe function should take parameters (key, Iterator[pandas.DataFrame], state) and\nreturn another Iterator[pandas.DataFrame]. The grouping key(s) will be passed as a tuple\nof numpy data types, e.g.,numpy.int32andnumpy.float64. The state will be passed aspyspark.sql.streaming.state.GroupState.\nFor each group, all columns are passed together aspandas.DataFrameto the user-function,\nand the returnedpandas.DataFrameacross all invocations are combined as aDataFrame. Note that the user function should not make a guess of the number of\nelements in the iterator. To process all data, the user function needs to iterate all\nelements and process them. On the other hand, the user function is not strictly required to\niterate through all elements in the iterator if it intends to read a part of data.\nTheoutputStructTypeshould be aStructTypedescribing the schema of all\nelements in the returned value,pandas.DataFrame. The column labels of all elements in\nreturnedpandas.DataFramemust either match the field names in the defined schema if\nspecified as strings, or match the field data types by position if not strings,\ne.g. integer indices.\nThestateStructTypeshould beStructTypedescribing the schema of the\nuser-defined state. The value of the state will be presented as a tuple, as well as the\nupdate should be performed with the tuple. The corresponding Python types for\n:class:DataType are supported. Please refer to the page\nhttps://spark.apache.org/docs/latest/sql-ref-datatypes.html (Python tab).\nThe size of eachpandas.DataFramein both the input and output can be arbitrary. The\nnumber ofpandas.DataFramein both the input and output can also be arbitrary.\nNotes\nThis function requires a full shuffle.", "Examples": [">>> import pandas as pd  \n>>> from pyspark.sql.streaming.state import GroupStateTimeout\n>>> def count_fn(key, pdf_iter, state):\n...     assert isinstance(state, GroupStateImpl)\n...     total_len = 0\n...     for pdf in pdf_iter:\n...         total_len += len(pdf)\n...     state.update((total_len,))\n...     yield pd.DataFrame({\"id\": [key[0]], \"countAsString\": [str(total_len)]})\n...\n>>> df.groupby(\"id\").applyInPandasWithState(\n...     count_fn, outputStructType=\"id long, countAsString string\",\n...     stateStructType=\"len long\", outputMode=\"Update\",\n...     timeoutConf=GroupStateTimeout.NoTimeout)"], "Parameters": [["func function", "a Python native function to be called on every group. It should take parameters\n(key, Iterator[ pandas.DataFrame ], state) and return Iterator[ pandas.DataFrame ].\nNote that the type of the key is tuple and the type of the state is pyspark.sql.streaming.state.GroupState ."], ["outputStructType pyspark.sql.types.DataType or str", "the type of the output records. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string."], ["stateStructType pyspark.sql.types.DataType or str", "the type of the user-defined state. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string."], ["outputMode str", "the output mode of the function."], ["timeoutConf str", "timeout configuration for groups that do not receive data for a while. valid values\nare defined in pyspark.sql.streaming.state.GroupStateTimeout ."]], "Returns": [], "Category": ["Grouping"], "index": 583}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.avg.html#pyspark.sql.GroupedData.avg"], "Title": ["GroupedData.avg"], "Feature": ["GroupedData.avg"], "Description": "Computes average values for each numeric columns for each group.\nmean()is an alias foravg().", "Examples": [">>> df = spark.createDataFrame([\n...     (2, \"Alice\", 80), (3, \"Alice\", 100),\n...     (5, \"Bob\", 120), (10, \"Bob\", 140)], [\"age\", \"name\", \"height\"])\n>>> df.show()\n+---+-----+------+\n|age| name|height|\n+---+-----+------+\n|  2|Alice|    80|\n|  3|Alice|   100|\n|  5|  Bob|   120|\n| 10|  Bob|   140|\n+---+-----+------+", ">>> df.groupBy(\"name\").avg('age').sort(\"name\").show()\n+-----+--------+\n| name|avg(age)|\n+-----+--------+\n|Alice|     2.5|\n|  Bob|     7.5|\n+-----+--------+", ">>> df.groupBy().avg('age', 'height').show()\n+--------+-----------+\n|avg(age)|avg(height)|\n+--------+-----------+\n|     5.0|      110.0|\n+--------+-----------+"], "Parameters": [["cols str", "column names. Non-numeric columns are ignored."]], "Returns": [], "Category": ["Grouping"], "index": 584}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.cogroup.html#pyspark.sql.GroupedData.cogroup"], "Title": ["GroupedData.cogroup"], "Feature": ["GroupedData.cogroup"], "Description": "Cogroups this group with another group so that we can run cogrouped operations.\nSeePandasCogroupedOpsfor the operations that can be run.", "Examples": [], "Parameters": [], "Returns": [], "Category": ["Grouping"], "index": 585}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.count.html#pyspark.sql.GroupedData.count"], "Title": ["GroupedData.count"], "Feature": ["GroupedData.count"], "Description": "Counts the number of records for each group.", "Examples": [">>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (3, \"Alice\"), (5, \"Bob\"), (10, \"Bob\")], [\"age\", \"name\"])\n>>> df.show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  3|Alice|\n|  5|  Bob|\n| 10|  Bob|\n+---+-----+", ">>> df.groupBy(df.name).count().sort(\"name\").show()\n+-----+-----+\n| name|count|\n+-----+-----+\n|Alice|    2|\n|  Bob|    2|\n+-----+-----+"], "Parameters": [], "Returns": [], "Category": ["Grouping"], "index": 586}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.max.html#pyspark.sql.GroupedData.max"], "Title": ["GroupedData.max"], "Feature": ["GroupedData.max"], "Description": "Computes the max value for each numeric columns for each group.", "Examples": [">>> df = spark.createDataFrame([\n...     (2, \"Alice\", 80), (3, \"Alice\", 100),\n...     (5, \"Bob\", 120), (10, \"Bob\", 140)], [\"age\", \"name\", \"height\"])\n>>> df.show()\n+---+-----+------+\n|age| name|height|\n+---+-----+------+\n|  2|Alice|    80|\n|  3|Alice|   100|\n|  5|  Bob|   120|\n| 10|  Bob|   140|\n+---+-----+------+", ">>> df.groupBy(\"name\").max(\"age\").sort(\"name\").show()\n+-----+--------+\n| name|max(age)|\n+-----+--------+\n|Alice|       3|\n|  Bob|      10|\n+-----+--------+", ">>> df.groupBy().max(\"age\", \"height\").show()\n+--------+-----------+\n|max(age)|max(height)|\n+--------+-----------+\n|      10|        140|\n+--------+-----------+"], "Parameters": [], "Returns": [], "Category": ["Grouping"], "index": 587}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.mean.html#pyspark.sql.GroupedData.mean"], "Title": ["GroupedData.mean"], "Feature": ["GroupedData.mean"], "Description": "Computes average values for each numeric columns for each group.\nmean()is an alias foravg().", "Examples": [], "Parameters": [["cols str", "column names. Non-numeric columns are ignored."]], "Returns": [], "Category": ["Grouping"], "index": 588}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.min.html#pyspark.sql.GroupedData.min"], "Title": ["GroupedData.min"], "Feature": ["GroupedData.min"], "Description": "Computes the min value for each numeric column for each group.", "Examples": [">>> df = spark.createDataFrame([\n...     (2, \"Alice\", 80), (3, \"Alice\", 100),\n...     (5, \"Bob\", 120), (10, \"Bob\", 140)], [\"age\", \"name\", \"height\"])\n>>> df.show()\n+---+-----+------+\n|age| name|height|\n+---+-----+------+\n|  2|Alice|    80|\n|  3|Alice|   100|\n|  5|  Bob|   120|\n| 10|  Bob|   140|\n+---+-----+------+", ">>> df.groupBy(\"name\").min(\"age\").sort(\"name\").show()\n+-----+--------+\n| name|min(age)|\n+-----+--------+\n|Alice|       2|\n|  Bob|       5|\n+-----+--------+", ">>> df.groupBy().min(\"age\", \"height\").show()\n+--------+-----------+\n|min(age)|min(height)|\n+--------+-----------+\n|       2|         80|\n+--------+-----------+"], "Parameters": [["cols str", "column names. Non-numeric columns are ignored."]], "Returns": [], "Category": ["Grouping"], "index": 589}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sameSemantics.html#pyspark.sql.DataFrame.sameSemantics"], "Title": ["DataFrame.sameSemantics"], "Feature": ["DataFrame.sameSemantics"], "Description": "ReturnsTruewhen the logical query plans inside bothDataFrames are equal and\ntherefore return the same results.\nNotes\nThe equality comparison here is simplified by tolerating the cosmetic differences\nsuch as attribute names.\nThis API can compare bothDataFrames very fast but can still returnFalseon theDataFramethat return the same results, for instance, from\ndifferent plans. Such false negative semantic can be useful when caching as an example.\nThis API is a developer API.", "Examples": [">>> df1 = spark.range(10)\n>>> df2 = spark.range(10)\n>>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\nTrue\n>>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\nFalse\n>>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\nTrue"], "Parameters": [["other DataFrame", "The other DataFrame to compare against."]], "Returns": [["bool", "Whether these two DataFrames are similar."]], "Category": ["DataFrame"], "index": 590}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.sum.html#pyspark.sql.GroupedData.sum"], "Title": ["GroupedData.sum"], "Feature": ["GroupedData.sum"], "Description": "Computes the sum for each numeric columns for each group.", "Examples": [">>> df = spark.createDataFrame([\n...     (2, \"Alice\", 80), (3, \"Alice\", 100),\n...     (5, \"Bob\", 120), (10, \"Bob\", 140)], [\"age\", \"name\", \"height\"])\n>>> df.show()\n+---+-----+------+\n|age| name|height|\n+---+-----+------+\n|  2|Alice|    80|\n|  3|Alice|   100|\n|  5|  Bob|   120|\n| 10|  Bob|   140|\n+---+-----+------+", ">>> df.groupBy(\"name\").sum(\"age\").sort(\"name\").show()\n+-----+--------+\n| name|sum(age)|\n+-----+--------+\n|Alice|       5|\n|  Bob|      15|\n+-----+--------+", ">>> df.groupBy().sum(\"age\", \"height\").show()\n+--------+-----------+\n|sum(age)|sum(height)|\n+--------+-----------+\n|      20|        440|\n+--------+-----------+"], "Parameters": [["cols str", "column names. Non-numeric columns are ignored."]], "Returns": [], "Category": ["Grouping"], "index": 591}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.transformWithStateInPandas.html#pyspark.sql.GroupedData.transformWithStateInPandas"], "Title": ["GroupedData.transformWithStateInPandas"], "Feature": ["GroupedData.transformWithStateInPandas"], "Description": "Invokes methods defined in the stateful processor used in arbitrary state API v2. It\nrequires protobuf, pandas and pyarrow as dependencies to process input/state data. We\nallow the user to act on per-group set of input rows along with keyed state and the user\ncan choose to output/return 0 or more rows.\nFor a streaming dataframe, we will repeatedly invoke the interface methods for new rows\nin each trigger and the user’s state/state variables will be stored persistently across\ninvocations.\nThestatefulProcessorshould be a Python class that implements the interface defined inStatefulProcessor.\nTheoutputStructTypeshould be aStructTypedescribing the schema of all\nelements in the returned value,pandas.DataFrame. The column labels of all elements in\nreturnedpandas.DataFramemust either match the field names in the defined schema if\nspecified as strings, or match the field data types by position if not strings,\ne.g. integer indices.\nThe size of eachpandas.DataFramein both the input and output can be arbitrary. The\nnumber ofpandas.DataFramein both the input and output can also be arbitrary.\nNotes\nThis function requires a full shuffle.", "Examples": [">>> from typing import Iterator\n...\n>>> import pandas as pd \n...\n>>> from pyspark.sql import Row\n>>> from pyspark.sql.functions import col, split\n>>> from pyspark.sql.streaming import StatefulProcessor, StatefulProcessorHandle\n>>> from pyspark.sql.types import IntegerType, LongType, StringType, StructField, StructType\n...\n>>> spark.conf.set(\"spark.sql.streaming.stateStore.providerClass\",\n...     \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\")\n... # Below is a simple example to find erroneous sensors from temperature sensor data. The\n... # processor returns a count of total readings, while keeping erroneous reading counts\n... # in streaming state. A violation is defined when the temperature is above 100.\n... # The input data is a DataFrame with the following schema:\n... #    `id: string, temperature: long`.\n... # The output schema and state schema are defined as below.\n>>> output_schema = StructType([\n...     StructField(\"id\", StringType(), True),\n...     StructField(\"count\", IntegerType(), True)\n... ])\n>>> state_schema = StructType([\n...     StructField(\"value\", IntegerType(), True)\n... ])\n>>> class SimpleStatefulProcessor(StatefulProcessor):\n...     def init(self, handle: StatefulProcessorHandle):\n...         self.num_violations_state = handle.getValueState(\"numViolations\", state_schema)\n...\n...     def handleInputRows(self, key, rows):\n...         new_violations = 0\n...         count = 0\n...         exists = self.num_violations_state.exists()\n...         if exists:\n...             existing_violations_row = self.num_violations_state.get()\n...             existing_violations = existing_violations_row[0]\n...         else:\n...             existing_violations = 0\n...         for pdf in rows:\n...             pdf_count = pdf.count()\n...             count += pdf_count.get('temperature')\n...             violations_pdf = pdf.loc[pdf['temperature'] > 100]\n...             new_violations += violations_pdf.count().get('temperature')\n...         updated_violations = new_violations + existing_violations\n...         self.num_violations_state.update((updated_violations,))\n...         yield pd.DataFrame({'id': key, 'count': count})\n...\n...     def close(self) -> None:\n...         pass", ">>> df.groupBy(\"value\").transformWithStateInPandas(statefulProcessor =\n...     SimpleStatefulProcessor(), outputStructType=output_schema, outputMode=\"Update\",\n...     timeMode=\"None\")"], "Parameters": [["statefulProcessor pyspark.sql.streaming.stateful_processor.StatefulProcessor", "Instance of StatefulProcessor whose functions will be invoked by the operator."], ["outputStructType pyspark.sql.types.DataType or str", "The type of the output records. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string."], ["outputMode str", "The output mode of the stateful processor."], ["timeMode str", "The time mode semantics of the stateful processor for timers and TTL."], ["initialState pyspark.sql.GroupedData", "Optional. The grouped dataframe as initial states used for initialization\nof state variables in the first batch."]], "Returns": [], "Category": ["Grouping"], "index": 592}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.PandasCogroupedOps.applyInArrow.html#pyspark.sql.PandasCogroupedOps.applyInArrow"], "Title": ["PandasCogroupedOps.applyInArrow"], "Feature": ["PandasCogroupedOps.applyInArrow"], "Description": "Applies a function to each cogroup using Arrow and returns the result\nas aDataFrame.\nThe function should take twopyarrow.Tables and return anotherpyarrow.Table. Alternatively, the user can pass a function that takes\na tuple ofpyarrow.Scalargrouping key(s) and the twopyarrow.Tables.\nFor each side of the cogroup, all columns are passed together as apyarrow.Tableto the user-function and the returnedpyarrow.Tableare combined as\naDataFrame.\nTheschemashould be aStructTypedescribing the schema of the returnedpyarrow.Table. The column labels of the returnedpyarrow.Tablemust either match\nthe field names in the defined schema if specified as strings, or match the\nfield data types by position if not strings, e.g. integer indices.\nThe length of the returnedpyarrow.Tablecan be arbitrary.\nSee alsopyspark.sql.functions.pandas_udf\nNotes\nThis function requires a full shuffle. All the data of a cogroup will be loaded\ninto memory, so the user should be aware of the potential OOM risk if data is skewed\nand certain groups are too large to fit in memory.\nThis API is unstable, and for developers.", "Examples": [">>> import pyarrow  \n>>> df1 = spark.createDataFrame([(1, 1.0), (2, 2.0), (1, 3.0), (2, 4.0)], (\"id\", \"v1\"))\n>>> df2 = spark.createDataFrame([(1, \"x\"), (2, \"y\")], (\"id\", \"v2\"))\n>>> def summarize(l, r):\n...     return pyarrow.Table.from_pydict({\n...         \"left\": [l.num_rows],\n...         \"right\": [r.num_rows]\n...     })\n>>> df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInArrow(\n...     summarize, schema=\"left long, right long\"\n... ).show()  \n+----+-----+\n|left|right|\n+----+-----+\n|   2|    1|\n|   2|    1|\n+----+-----+", ">>> def summarize(key, l, r):\n...     return pyarrow.Table.from_pydict({\n...         \"key\": [key[0].as_py()],\n...         \"left\": [l.num_rows],\n...         \"right\": [r.num_rows]\n...     })\n>>> df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInArrow(\n...     summarize, schema=\"key long, left long, right long\"\n... ).show()  \n+---+----+-----+\n|key|left|right|\n+---+----+-----+\n|  1|   2|    1|\n|  2|   2|    1|\n+---+----+-----+"], "Parameters": [["func function", "a Python native function that takes two pyarrow.Table s, and\noutputs a pyarrow.Table , or that takes one tuple (grouping keys) and two pyarrow.Table s, and outputs a pyarrow.Table ."], ["schema pyspark.sql.types.DataType or str", "the return type of the func in PySpark. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string."]], "Returns": [], "Category": ["Grouping"], "index": 593}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.PandasCogroupedOps.applyInPandas.html#pyspark.sql.PandasCogroupedOps.applyInPandas"], "Title": ["PandasCogroupedOps.applyInPandas"], "Feature": ["PandasCogroupedOps.applyInPandas"], "Description": "Applies a function to each cogroup using pandas and returns the result\nas aDataFrame.\nThe function should take twopandas.DataFrames and return anotherpandas.DataFrame. Alternatively, the user can pass a function that takes\na tuple of the grouping key(s) and the twopandas.DataFrames.\nFor each side of the cogroup, all columns are passed together as apandas.DataFrameto the user-function and the returnedpandas.DataFrameare combined as\naDataFrame.\nTheschemashould be aStructTypedescribing the schema of the returnedpandas.DataFrame. The column labels of the returnedpandas.DataFramemust either match\nthe field names in the defined schema if specified as strings, or match the\nfield data types by position if not strings, e.g. integer indices.\nThe length of the returnedpandas.DataFramecan be arbitrary.\nSee alsopyspark.sql.functions.pandas_udf\nNotes\nThis function requires a full shuffle. All the data of a cogroup will be loaded\ninto memory, so the user should be aware of the potential OOM risk if data is skewed\nand certain groups are too large to fit in memory.", "Examples": [">>> df1 = spark.createDataFrame(\n...     [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n...     (\"time\", \"id\", \"v1\"))\n>>> df2 = spark.createDataFrame(\n...     [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n...     (\"time\", \"id\", \"v2\"))\n>>> def asof_join(l, r):\n...     return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n...\n>>> df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n...     asof_join, schema=\"time int, id int, v1 double, v2 string\"\n... ).show()  \n+--------+---+---+---+\n|    time| id| v1| v2|\n+--------+---+---+---+\n|20000101|  1|1.0|  x|\n|20000102|  1|3.0|  x|\n|20000101|  2|2.0|  y|\n|20000102|  2|4.0|  y|\n+--------+---+---+---+", ">>> def asof_join(k, l, r):\n...     if k == (1,):\n...         return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n...     else:\n...         return pd.DataFrame(columns=['time', 'id', 'v1', 'v2'])\n...\n>>> df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n...     asof_join, \"time int, id int, v1 double, v2 string\").show()  \n+--------+---+---+---+\n|    time| id| v1| v2|\n+--------+---+---+---+\n|20000101|  1|1.0|  x|\n|20000102|  1|3.0|  x|\n+--------+---+---+---+"], "Parameters": [["func function", "a Python native function that takes two pandas.DataFrame s, and\noutputs a pandas.DataFrame , or that takes one tuple (grouping keys) and two pandas.DataFrame s, and outputs a pandas.DataFrame ."], ["schema pyspark.sql.types.DataType or str", "the return type of the func in PySpark. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string."]], "Returns": [], "Category": ["Grouping"], "index": 594}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sampleBy.html#pyspark.sql.DataFrame.sampleBy"], "Title": ["DataFrame.sampleBy"], "Feature": ["DataFrame.sampleBy"], "Description": "Returns a stratified sample without replacement based on the\nfraction given on each stratum.", "Examples": [">>> from pyspark.sql.functions import col\n>>> dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n>>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n>>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n+---+-----+\n|key|count|\n+---+-----+\n|  0|    3|\n|  1|    6|\n+---+-----+\n>>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n33"], "Parameters": [["col Column or str", "column that defines strata Changed in version 3.0.0: Added sampling by a column of Column"], ["fractions dict", "sampling fraction for each stratum. If a stratum is not\nspecified, we treat its fraction as zero."], ["seed int, optional", "random seed"]], "Returns": [["a new DataFrame that represents the stratified sample", ""]], "Category": ["DataFrame"], "index": 595}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.scalar.html#pyspark.sql.DataFrame.scalar"], "Title": ["DataFrame.scalar"], "Feature": ["DataFrame.scalar"], "Description": "Return aColumnobject for a SCALAR Subquery containing exactly one row and one column.\nThescalar()method is useful for extracting aColumnobject that represents a scalar\nvalue from a DataFrame, especially when the DataFrame results from an aggregation or\nsingle-value computation. This returnedColumncan then be used directly inselectclauses or as predicates in filters on the outer DataFrame, enabling dynamic data filtering\nand calculations based on scalar values.", "Examples": [">>> data = [\n...     (1, \"Alice\", 45000, 101), (2, \"Bob\", 54000, 101), (3, \"Charlie\", 29000, 102),\n...     (4, \"David\", 61000, 102), (5, \"Eve\", 48000, 101),\n... ]\n>>> employees = spark.createDataFrame(data, [\"id\", \"name\", \"salary\", \"department_id\"])", ">>> from pyspark.sql import functions as sf\n>>> employees.where(\n...     sf.col(\"salary\") > employees.select(sf.avg(\"salary\")).scalar()\n... ).select(\"name\", \"salary\", \"department_id\").orderBy(\"name\").show()\n+-----+------+-------------+\n| name|salary|department_id|\n+-----+------+-------------+\n|  Bob| 54000|          101|\n|David| 61000|          102|\n|  Eve| 48000|          101|\n+-----+------+-------------+", ">>> from pyspark.sql import functions as sf\n>>> employees.alias(\"e1\").where(\n...     sf.col(\"salary\")\n...     > employees.alias(\"e2\").where(\n...         sf.col(\"e2.department_id\") == sf.col(\"e1.department_id\").outer()\n...     ).select(sf.avg(\"salary\")).scalar()\n... ).select(\"name\", \"salary\", \"department_id\").orderBy(\"name\").show()\n+-----+------+-------------+\n| name|salary|department_id|\n+-----+------+-------------+\n|  Bob| 54000|          101|\n|David| 61000|          102|\n+-----+------+-------------+", ">>> from pyspark.sql import functions as sf\n>>> employees.alias(\"e1\").select(\n...     \"name\", \"salary\", \"department_id\",\n...     sf.format_number(\n...         sf.lit(100) * sf.col(\"salary\") /\n...             employees.alias(\"e2\").where(\n...                 sf.col(\"e2.department_id\") == sf.col(\"e1.department_id\").outer()\n...             ).select(sf.sum(\"salary\")).scalar().alias(\"avg_salary\"),\n...         1\n...     ).alias(\"salary_proportion_in_department\")\n... ).orderBy(\"name\").show()\n+-------+------+-------------+-------------------------------+\n|   name|salary|department_id|salary_proportion_in_department|\n+-------+------+-------------+-------------------------------+\n|  Alice| 45000|          101|                           30.6|\n|    Bob| 54000|          101|                           36.7|\n|Charlie| 29000|          102|                           32.2|\n|  David| 61000|          102|                           67.8|\n|    Eve| 48000|          101|                           32.7|\n+-------+------+-------------+-------------------------------+"], "Parameters": [], "Returns": [["Column", "A Column object representing a SCALAR subquery."]], "Category": ["DataFrame"], "index": 596}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.schema.html#pyspark.sql.DataFrame.schema"], "Title": ["DataFrame.schema"], "Feature": ["DataFrame.schema"], "Description": "Returns the schema of thisDataFrameas apyspark.sql.types.StructType.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.schema\nStructType([StructField('age', LongType(), True),\n            StructField('name', StringType(), True)])", ">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")],\n...     \"age INT, name STRING\")\n>>> df.schema\nStructType([StructField('age', IntegerType(), True),\n            StructField('name', StringType(), True)])", ">>> from pyspark.sql.types import StructType, StructField, StringType\n>>> df = spark.createDataFrame(\n...     [(\"a\",), (\"b\",), (\"c\",)],\n...     StructType([StructField(\"value\", StringType(), False)]))\n>>> df.schema\nStructType([StructField('value', StringType(), False)])"], "Parameters": [], "Returns": [["StructType", ""]], "Category": ["DataFrame"], "index": 597}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.selectExpr.html#pyspark.sql.DataFrame.selectExpr"], "Title": ["DataFrame.selectExpr"], "Feature": ["DataFrame.selectExpr"], "Description": "Projects a set of SQL expressions and returns a newDataFrame.\nThis is a variant ofselect()that accepts SQL expressions.", "Examples": [">>> df = spark.createDataFrame([\n...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.selectExpr(\"age * 2\", \"abs(age)\").show()\n+---------+--------+\n|(age * 2)|abs(age)|\n+---------+--------+\n|        4|       2|\n|       10|       5|\n+---------+--------+"], "Parameters": [], "Returns": [["DataFrame", "A DataFrame with new/old columns transformed by expressions."]], "Category": ["DataFrame"], "index": 598}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.semanticHash.html#pyspark.sql.DataFrame.semanticHash"], "Title": ["DataFrame.semanticHash"], "Feature": ["DataFrame.semanticHash"], "Description": "Returns a hash code of the logical query plan against thisDataFrame.\nNotes\nUnlike the standard hash code, the hash is calculated against the query plan\nsimplified by tolerating the cosmetic differences such as attribute names.\nThis API is a developer API.", "Examples": [">>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  \n1855039936\n>>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  \n1855039936"], "Parameters": [], "Returns": [["int", "Hash value."]], "Category": ["DataFrame"], "index": 599}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html#pyspark.sql.DataFrame.show"], "Title": ["DataFrame.show"], "Feature": ["DataFrame.show"], "Description": "Prints the firstnrows of the DataFrame to the console.", "Examples": [">>> df = spark.createDataFrame([\n...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\"), (19, \"This is a super long name\")],\n...     [\"age\", \"name\"])", ">>> df.show()\n+---+--------------------+\n|age|                name|\n+---+--------------------+\n| 14|                 Tom|\n| 23|               Alice|\n| 16|                 Bob|\n| 19|This is a super l...|\n+---+--------------------+", ">>> df.show(2)\n+---+-----+\n|age| name|\n+---+-----+\n| 14|  Tom|\n| 23|Alice|\n+---+-----+\nonly showing top 2 rows", ">>> df.show(truncate=False)\n+---+-------------------------+\n|age|name                     |\n+---+-------------------------+\n|14 |Tom                      |\n|23 |Alice                    |\n|16 |Bob                      |\n|19 |This is a super long name|\n+---+-------------------------+", ">>> df.show(truncate=3)\n+---+----+\n|age|name|\n+---+----+\n| 14| Tom|\n| 23| Ali|\n| 16| Bob|\n| 19| Thi|\n+---+----+", ">>> df.show(vertical=True)\n-RECORD 0--------------------\nage  | 14\nname | Tom\n-RECORD 1--------------------\nage  | 23\nname | Alice\n-RECORD 2--------------------\nage  | 16\nname | Bob\n-RECORD 3--------------------\nage  | 19\nname | This is a super l..."], "Parameters": [["n int, optional, default 20", "Number of rows to show."], ["truncate bool or int, optional, default True", "If set to True , truncate strings longer than 20 chars.\nIf set to a number greater than one, truncates long strings to length truncate and align cells right."], ["vertical bool, optional", "If set to True , print output rows vertically (one line per column value)."]], "Returns": [], "Category": ["DataFrame"], "index": 600}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.checkpoint.html#pyspark.sql.DataFrame.checkpoint"], "Title": ["DataFrame.checkpoint"], "Feature": ["DataFrame.checkpoint"], "Description": "Returns a checkpointed version of thisDataFrame. Checkpointing can be\nused to truncate the logical plan of thisDataFrame, which is especially\nuseful in iterative algorithms where the plan may grow exponentially. It will be\nsaved to files inside the checkpoint directory set withSparkContext.setCheckpointDir(), orspark.checkpoint.dirconfiguration.\nNotes\nThis API is experimental.", "Examples": [">>> df = spark.createDataFrame([\n...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.checkpoint(False)  \nDataFrame[age: bigint, name: string]"], "Parameters": [["eager bool, optional, default True", "Whether to checkpoint this DataFrame immediately."]], "Returns": [["DataFrame", "Checkpointed DataFrame."]], "Category": ["DataFrame"], "index": 601}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sort.html#pyspark.sql.DataFrame.sort"], "Title": ["DataFrame.sort"], "Feature": ["DataFrame.sort"], "Description": "Returns a newDataFramesorted by the specified column(s).\nNotes\nA column ordinal starts from 1, which is different from the\n0-based__getitem__().\nIf a column ordinal is negative, it means sort descending.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])", ">>> df.sort(sf.asc(\"age\")).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+", ">>> df.sort(1).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+", ">>> df.sort(df.age.desc()).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n+---+-----+", ">>> df.orderBy(df.age.desc()).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n+---+-----+", ">>> df.sort(\"age\", ascending=False).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n+---+-----+", ">>> df.sort(-1).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n+---+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.orderBy(sf.desc(\"age\"), \"name\").show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n|  2|  Bob|\n+---+-----+", ">>> df.orderBy(-1, \"name\").show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n|  2|  Bob|\n+---+-----+", ">>> df.orderBy(-1, 2).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n|  2|  Bob|\n+---+-----+", ">>> df.orderBy([\"age\", \"name\"], ascending=[False, False]).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|  Bob|\n|  2|Alice|\n+---+-----+", ">>> df.orderBy([1, \"name\"], ascending=[False, False]).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|  Bob|\n|  2|Alice|\n+---+-----+", ">>> df.orderBy([1, 2], ascending=[False, False]).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|  Bob|\n|  2|Alice|\n+---+-----+"], "Parameters": [["cols int, str, list, or Column , optional", "list of Column or column names or column ordinals to sort by. Changed in version 4.0.0: Supports column ordinal."]], "Returns": [["DataFrame", "Sorted DataFrame."]], "Category": ["DataFrame"], "index": 602}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sparkSession.html#pyspark.sql.DataFrame.sparkSession"], "Title": ["DataFrame.sparkSession"], "Feature": ["DataFrame.sparkSession"], "Description": "Returns Spark session that created thisDataFrame.", "Examples": [">>> df = spark.range(1)\n>>> type(df.sparkSession)\n<class '...session.SparkSession'>"], "Parameters": [], "Returns": [["SparkSession", ""]], "Category": ["DataFrame"], "index": 603}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.stat.html#pyspark.sql.DataFrame.stat"], "Title": ["DataFrame.stat"], "Feature": ["DataFrame.stat"], "Description": "Returns aDataFrameStatFunctionsfor statistic functions.", "Examples": [">>> import pyspark.sql.functions as f\n>>> df = spark.range(3).withColumn(\"c\", f.expr(\"id + 1\"))\n>>> type(df.stat)\n<class '...dataframe.DataFrameStatFunctions'>\n>>> df.stat.corr(\"id\", \"c\")\n1.0"], "Parameters": [], "Returns": [["DataFrameStatFunctions", ""]], "Category": ["DataFrame"], "index": 604}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.storageLevel.html#pyspark.sql.DataFrame.storageLevel"], "Title": ["DataFrame.storageLevel"], "Feature": ["DataFrame.storageLevel"], "Description": "Get theDataFrame’s current storage level.", "Examples": [">>> df1 = spark.range(10)\n>>> df1.storageLevel\nStorageLevel(False, False, False, False, 1)\n>>> df1.cache().storageLevel\nStorageLevel(True, True, False, True, 1)", ">>> df2 = spark.range(5)\n>>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\nStorageLevel(True, False, False, False, 2)"], "Parameters": [], "Returns": [["StorageLevel", "Currently defined storage level."]], "Category": ["DataFrame"], "index": 605}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.summary.html#pyspark.sql.DataFrame.summary"], "Title": ["DataFrame.summary"], "Feature": ["DataFrame.summary"], "Description": "Computes specified statistics for numeric and string columns. Available statistics are:\n- count\n- mean\n- stddev\n- min\n- max\n- arbitrary approximate percentiles specified as a percentage (e.g., 75%)\nIf no statistics are given, this function computes count, mean, stddev, min,\napproximate quartiles (percentiles at 25%, 50%, and 75%), and max.\nSee alsoDataFrame.describeComputes basic statistics for numeric and string columns.\nNotes\nThis function is meant for exploratory data analysis, as we make no\nguarantee about the backward compatibility of the schema of the resultingDataFrame.", "Examples": [">>> df = spark.createDataFrame(\n...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n...     [\"name\", \"age\", \"weight\", \"height\"],\n... )\n>>> df.select(\"age\", \"weight\", \"height\").summary().show()\n+-------+----+------------------+-----------------+\n|summary| age|            weight|           height|\n+-------+----+------------------+-----------------+\n|  count|   3|                 3|                3|\n|   mean|12.0| 40.73333333333333|            145.0|\n| stddev| 1.0|3.1722757341273704|4.763402145525822|\n|    min|  11|              37.8|            142.2|\n|    25%|  11|              37.8|            142.2|\n|    50%|  12|              40.3|            142.3|\n|    75%|  13|              44.1|            150.5|\n|    max|  13|              44.1|            150.5|\n+-------+----+------------------+-----------------+", ">>> df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n+-------+---+------+------+\n|summary|age|weight|height|\n+-------+---+------+------+\n|  count|  3|     3|     3|\n|    min| 11|  37.8| 142.2|\n|    25%| 11|  37.8| 142.2|\n|    75%| 13|  44.1| 150.5|\n|    max| 13|  44.1| 150.5|\n+-------+---+------+------+"], "Parameters": [["statistics str, optional", "Column names to calculate statistics by (default All columns)."]], "Returns": [["DataFrame", "A new DataFrame that provides statistics for the given DataFrame."]], "Category": ["DataFrame"], "index": 606}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.tail.html#pyspark.sql.DataFrame.tail"], "Title": ["DataFrame.tail"], "Feature": ["DataFrame.tail"], "Description": "Returns the lastnumrows as alistofRow.\nRunning tail requires moving data into the application’s driver process, and doing so with\na very largenumcan crash the driver process with OutOfMemoryError.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])", ">>> df.tail(2)\n[Row(age=23, name='Alice'), Row(age=16, name='Bob')]"], "Parameters": [["num int", "Number of records to return. Will return this number of records\nor all records if the DataFrame contains less than this number of records."]], "Returns": [["list", "List of rows"]], "Category": ["DataFrame"], "index": 607}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.take.html#pyspark.sql.DataFrame.take"], "Title": ["DataFrame.take"], "Feature": ["DataFrame.take"], "Description": "Returns the firstnumrows as alistofRow.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])", ">>> df.take(2)\n[Row(age=14, name='Tom'), Row(age=23, name='Alice')]"], "Parameters": [["num int", "Number of records to return. Will return this number of records\nor all records if the DataFrame contains less than this number of records.."]], "Returns": [["list", "List of rows"]], "Category": ["DataFrame"], "index": 608}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.to.html#pyspark.sql.DataFrame.to"], "Title": ["DataFrame.to"], "Feature": ["DataFrame.to"], "Description": "Returns a newDataFramewhere each row is reconciled to match the specified\nschema.\nNotes\nReorder columns and/or inner fields by name to match the specified schema.Project away columns and/or inner fields that are not needed by the specified schema.Missing columns and/or inner fields (present in the specified schema but not input\nDataFrame) lead to failures.Cast the columns and/or inner fields to match the data types in the specified schema,if the types are compatible, e.g., numeric to numeric (error if overflows), but\nnot string to int.Carry over the metadata from the specified schema, while the columns and/or inner fieldsstill keep their own metadata if not overwritten by the specified schema.Fail if the nullability is not compatible. For example, the column and/or inner fieldis nullable but the specified schema requires them to be not nullable.\nSupports Spark Connect.", "Examples": [">>> from pyspark.sql.types import StructField, StringType\n>>> df = spark.createDataFrame([(\"a\", 1)], [\"i\", \"j\"])\n>>> df.schema\nStructType([StructField('i', StringType(), True), StructField('j', LongType(), True)])", ">>> schema = StructType([StructField(\"j\", StringType()), StructField(\"i\", StringType())])\n>>> df2 = df.to(schema)\n>>> df2.schema\nStructType([StructField('j', StringType(), True), StructField('i', StringType(), True)])\n>>> df2.show()\n+---+---+\n|  j|  i|\n+---+---+\n|  1|  a|\n+---+---+"], "Parameters": [["schema StructType", "Specified schema."]], "Returns": [["DataFrame", "Reconciled DataFrame."]], "Category": ["DataFrame"], "index": 609}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toArrow.html#pyspark.sql.DataFrame.toArrow"], "Title": ["DataFrame.toArrow"], "Feature": ["DataFrame.toArrow"], "Description": "Returns the contents of thisDataFrameas PyArrowpyarrow.Table.\nThis is only available if PyArrow is installed and available.\nNotes\nThis method should only be used if the resulting PyArrowpyarrow.Tableis\nexpected to be small, as all the data is loaded into the driver’s memory.\nThis API is a developer API.", "Examples": [">>> df.toArrow()  \npyarrow.Table\nage: int64\nname: string\n----\nage: [[2,5]]\nname: [[\"Alice\",\"Bob\"]]"], "Parameters": [], "Returns": [], "Category": ["DataFrame"], "index": 610}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toDF.html#pyspark.sql.DataFrame.toDF"], "Title": ["DataFrame.toDF"], "Feature": ["DataFrame.toDF"], "Description": "Returns a newDataFramethat with new specified column names", "Examples": [">>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"),\n...     (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.toDF('f1', 'f2').show()\n+---+-----+\n| f1|   f2|\n+---+-----+\n| 14|  Tom|\n| 23|Alice|\n| 16|  Bob|\n+---+-----+"], "Parameters": [["*cols tuple", "a tuple of string new column name. The length of the\nlist needs to be the same as the number of columns in the initial DataFrame"]], "Returns": [["DataFrame", "DataFrame with new column names."]], "Category": ["DataFrame"], "index": 611}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html#pyspark.sql.DataFrame.coalesce"], "Title": ["DataFrame.coalesce"], "Feature": ["DataFrame.coalesce"], "Description": "Returns a newDataFramethat has exactlynumPartitionspartitions.\nSimilar to coalesce defined on anRDD, this operation results in a\nnarrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\nthere will not be a shuffle, instead each of the 100 new partitions will\nclaim 10 of the current partitions. If a larger number of partitions is requested,\nit will stay at the current number of partitions.\nHowever, if you’re doing a drastic coalesce, e.g. to numPartitions = 1,\nthis may result in your computation taking place on fewer nodes than\nyou like (e.g. one node in the case of numPartitions = 1). To avoid this,\nyou can call repartition(). This will add a shuffle step, but means the\ncurrent upstream partitions will be executed in parallel (per whatever\nthe current partitioning is).", "Examples": [">>> from pyspark.sql import functions as sf\n>>> spark.range(0, 10, 1, 3).select(\n...     sf.spark_partition_id().alias(\"partition\")\n... ).distinct().sort(\"partition\").show()\n+---------+\n|partition|\n+---------+\n|        0|\n|        1|\n|        2|\n+---------+", ">>> from pyspark.sql import functions as sf\n>>> spark.range(0, 10, 1, 3).coalesce(1).select(\n...     sf.spark_partition_id().alias(\"partition\")\n... ).distinct().sort(\"partition\").show()\n+---------+\n|partition|\n+---------+\n|        0|\n+---------+"], "Parameters": [["numPartitions int", "specify the target number of partitions"]], "Returns": [["DataFrame", ""]], "Category": ["DataFrame"], "index": 612}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toJSON.html#pyspark.sql.DataFrame.toJSON"], "Title": ["DataFrame.toJSON"], "Feature": ["DataFrame.toJSON"], "Description": "Converts aDataFrameinto aRDDof string.\nEach row is turned into a JSON document as one element in the returned RDD.", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.toJSON().first()\n'{\"age\":2,\"name\":\"Alice\"}'"], "Parameters": [["use_unicode bool, optional, default True", "Whether to convert to unicode or not."]], "Returns": [["RDD", ""]], "Category": ["DataFrame"], "index": 613}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toLocalIterator.html#pyspark.sql.DataFrame.toLocalIterator"], "Title": ["DataFrame.toLocalIterator"], "Feature": ["DataFrame.toLocalIterator"], "Description": "Returns an iterator that contains all of the rows in thisDataFrame.\nThe iterator will consume as much memory as the largest partition in thisDataFrame. With prefetch it may consume up to the memory of the 2 largest\npartitions.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> list(df.toLocalIterator())\n[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]"], "Parameters": [["prefetchPartitions bool, optional", "If Spark should pre-fetch the next partition before it is needed. Changed in version 3.4.0: This argument does not take effect for Spark Connect."]], "Returns": [["Iterator", "Iterator of rows."]], "Category": ["DataFrame"], "index": 614}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html#pyspark.sql.DataFrame.toPandas"], "Title": ["DataFrame.toPandas"], "Feature": ["DataFrame.toPandas"], "Description": "Returns the contents of thisDataFrameas Pandaspandas.DataFrame.\nThis is only available if Pandas is installed and available.\nNotes\nThis method should only be used if the resulting Pandaspandas.DataFrameis\nexpected to be small, as all the data is loaded into the driver’s memory.\nUsage withspark.sql.execution.arrow.pyspark.enabled=Trueis experimental.", "Examples": [">>> df.toPandas()  \n   age   name\n0    2  Alice\n1    5    Bob"], "Parameters": [], "Returns": [], "Category": ["DataFrame"], "index": 615}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.transpose.html#pyspark.sql.DataFrame.transpose"], "Title": ["DataFrame.transpose"], "Feature": ["DataFrame.transpose"], "Description": "Transposes a DataFrame such that the values in the specified index column become the new\ncolumns of the DataFrame. If no index column is provided, the first column is used as\nthe default.\nPlease note:\n- All columns except the index column must share a least common data type. Unless they\nare the same data type, all columns are cast to the nearest common data type.\n- The name of the column into which the original column names are transposed defaults\nto “key”.\n- null values in the index column are excluded from the column names for the\ntransposed table, which are ordered in ascending order.\nNotes\nSupports Spark Connect.", "Examples": [">>> df = spark.createDataFrame(\n...     [(\"A\", 1, 2), (\"B\", 3, 4)],\n...     [\"id\", \"val1\", \"val2\"],\n... )\n>>> df.show()\n+---+----+----+\n| id|val1|val2|\n+---+----+----+\n|  A|   1|   2|\n|  B|   3|   4|\n+---+----+----+", ">>> df.transpose().show()\n+----+---+---+\n| key|  A|  B|\n+----+---+---+\n|val1|  1|  3|\n|val2|  2|  4|\n+----+---+---+", ">>> df.transpose(df.id).show()\n+----+---+---+\n| key|  A|  B|\n+----+---+---+\n|val1|  1|  3|\n|val2|  2|  4|\n+----+---+---+"], "Parameters": [["indexColumn str or Column , optional", "The single column that will be treated as the index for the transpose operation. This\ncolumn will be used to transform the DataFrame such that the values of the indexColumn\nbecome the new columns in the transposed DataFrame. If not provided, the first column of\nthe DataFrame will be used as the default."]], "Returns": [["DataFrame", "Transposed DataFrame."]], "Category": ["DataFrame"], "index": 616}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.unionAll.html#pyspark.sql.DataFrame.unionAll"], "Title": ["DataFrame.unionAll"], "Feature": ["DataFrame.unionAll"], "Description": "Return a newDataFramecontaining the union of rows in this and anotherDataFrame.\nSee alsoDataFrame.union\nNotes\nThis method combines all rows from bothDataFrameobjects with no automatic\ndeduplication of elements.\nUse thedistinct()method to perform deduplication of rows.\nunionAll()is an alias tounion()", "Examples": [], "Parameters": [["other DataFrame", "Another DataFrame that needs to be combined"]], "Returns": [["DataFrame", "A new DataFrame containing combined rows from both dataframes."]], "Category": ["DataFrame"], "index": 617}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.unionByName.html#pyspark.sql.DataFrame.unionByName"], "Title": ["DataFrame.unionByName"], "Feature": ["DataFrame.unionByName"], "Description": "Returns a newDataFramecontaining union of rows in this and anotherDataFrame.\nThis method performs a union operation on both input DataFrames, resolving columns by\nname (rather than position). WhenallowMissingColumnsis True, missing columns will\nbe filled with null.", "Examples": [">>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n>>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n>>> df1.unionByName(df2).show()\n+----+----+----+\n|col0|col1|col2|\n+----+----+----+\n|   1|   2|   3|\n|   6|   4|   5|\n+----+----+----+", ">>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n>>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n>>> df1.unionByName(df2, allowMissingColumns=True).show()\n+----+----+----+----+\n|col0|col1|col2|col3|\n+----+----+----+----+\n|   1|   2|   3|NULL|\n|NULL|   4|   5|   6|\n+----+----+----+----+", ">>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n>>> df2 = spark.createDataFrame([[4, 5, 6, 7]], [\"col1\", \"col2\", \"col3\", \"col4\"])\n>>> df1.unionByName(df2, allowMissingColumns=True).show()\n+----+----+----+----+----+\n|col0|col1|col2|col3|col4|\n+----+----+----+----+----+\n|   1|   2|   3|NULL|NULL|\n|NULL|   4|   5|   6|   7|\n+----+----+----+----+----+", ">>> df1 = spark.createDataFrame([[0, 1, 2]], [\"col0\", \"col1\", \"col2\"])\n>>> df2 = spark.createDataFrame([[3, 4, 5]], [\"col3\", \"col4\", \"col5\"])\n>>> df1.unionByName(df2, allowMissingColumns=True).show()\n+----+----+----+----+----+----+\n|col0|col1|col2|col3|col4|col5|\n+----+----+----+----+----+----+\n|   0|   1|   2|NULL|NULL|NULL|\n|NULL|NULL|NULL|   3|   4|   5|\n+----+----+----+----+----+----+"], "Parameters": [["other DataFrame", "Another DataFrame that needs to be combined."], ["allowMissingColumns bool, optional, default False", "Specify whether to allow missing columns. New in version 3.1.0."]], "Returns": [["DataFrame", "A new DataFrame containing the combined rows with corresponding\ncolumns of the two given DataFrames."]], "Category": ["DataFrame"], "index": 618}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.unpersist.html#pyspark.sql.DataFrame.unpersist"], "Title": ["DataFrame.unpersist"], "Feature": ["DataFrame.unpersist"], "Description": "Marks theDataFrameas non-persistent, and remove all blocks for it from\nmemory and disk.\nNotes\nblockingdefault has changed toFalseto match Scala in 2.0.", "Examples": [">>> df = spark.range(1)\n>>> df.persist()\nDataFrame[id: bigint]\n>>> df.unpersist()\nDataFrame[id: bigint]\n>>> df = spark.range(1)\n>>> df.unpersist(True)\nDataFrame[id: bigint]"], "Parameters": [["blocking bool", "Whether to block until all blocks are deleted."]], "Returns": [["DataFrame", "Unpersisted DataFrame."]], "Category": ["DataFrame"], "index": 619}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html#pyspark.sql.DataFrame.withColumn"], "Title": ["DataFrame.withColumn"], "Feature": ["DataFrame.withColumn"], "Description": "Returns a newDataFrameby adding a column or replacing the\nexisting column that has the same name.\nThe column expression must be an expression over thisDataFrame; attempting to add\na column from some otherDataFramewill raise an error.\nNotes\nThis method introduces a projection internally. Therefore, calling it multiple\ntimes, for instance, via loops in order to add multiple columns can generate big\nplans which can cause performance issues and evenStackOverflowException.\nTo avoid this, useselect()with multiple columns at once.", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.withColumn('age2', df.age + 2).show()\n+---+-----+----+\n|age| name|age2|\n+---+-----+----+\n|  2|Alice|   4|\n|  5|  Bob|   7|\n+---+-----+----+"], "Parameters": [["colName str", "string, name of the new column."], ["col Column", "a Column expression for the new column."]], "Returns": [["DataFrame", "DataFrame with new or replaced column."]], "Category": ["DataFrame"], "index": 620}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumns.html#pyspark.sql.DataFrame.withColumns"], "Title": ["DataFrame.withColumns"], "Feature": ["DataFrame.withColumns"], "Description": "Returns a newDataFrameby adding multiple columns or replacing the\nexisting columns that have the same names.\nThe colsMap is a map of column name and column, the column must only refer to attributes\nsupplied by this Dataset. It is an error to add columns that refer to some other Dataset.", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).show()\n+---+-----+----+----+\n|age| name|age2|age3|\n+---+-----+----+----+\n|  2|Alice|   4|   5|\n|  5|  Bob|   7|   8|\n+---+-----+----+----+"], "Parameters": [["colsMap dict", "a dict of column name and Column . Currently, only a single map is supported."]], "Returns": [["DataFrame", "DataFrame with new or replaced columns."]], "Category": ["DataFrame"], "index": 621}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumnRenamed.html#pyspark.sql.DataFrame.withColumnRenamed"], "Title": ["DataFrame.withColumnRenamed"], "Feature": ["DataFrame.withColumnRenamed"], "Description": "Returns a newDataFrameby renaming an existing column.\nThis is a no-op if the schema doesn’t contain the given column name.\nSee alsoDataFrame.withColumnsRenamed", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])", ">>> df.withColumnRenamed(\"age\", \"age2\").show()\n+----+-----+\n|age2| name|\n+----+-----+\n|   2|Alice|\n|   5|  Bob|\n+----+-----+", ">>> df.withColumnRenamed(\"non_existing\", \"new_name\").show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+", ">>> df.withColumnRenamed(\"age\", \"age2\").withColumnRenamed(\"name\", \"name2\").show()\n+----+-----+\n|age2|name2|\n+----+-----+\n|   2|Alice|\n|   5|  Bob|\n+----+-----+"], "Parameters": [["existing str", "The name of the existing column to be renamed."], ["new str", "The new name to be assigned to the column."]], "Returns": [["DataFrame", "A new DataFrame with renamed column."]], "Category": ["DataFrame"], "index": 622}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.colRegex.html#pyspark.sql.DataFrame.colRegex"], "Title": ["DataFrame.colRegex"], "Feature": ["DataFrame.colRegex"], "Description": "Selects column based on the column name specified as a regex and returns it\nasColumn.", "Examples": [">>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n>>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n+----+\n|Col2|\n+----+\n|   1|\n|   2|\n|   3|\n+----+"], "Parameters": [["colName str", "string, column name specified as a regex."]], "Returns": [["Column", ""]], "Category": ["DataFrame"], "index": 623}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumnsRenamed.html#pyspark.sql.DataFrame.withColumnsRenamed"], "Title": ["DataFrame.withColumnsRenamed"], "Feature": ["DataFrame.withColumnsRenamed"], "Description": "Returns a newDataFrameby renaming multiple columns.\nThis is a no-op if the schema doesn’t contain the given column names.\nSee alsoDataFrame.withColumnRenamed", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])", ">>> df.withColumnsRenamed({\"age\": \"age2\"}).show()\n+----+-----+\n|age2| name|\n+----+-----+\n|   2|Alice|\n|   5|  Bob|\n+----+-----+", ">>> df.withColumnsRenamed({\"age\": \"age2\", \"name\": \"name2\"}).show()\n+----+-----+\n|age2|name2|\n+----+-----+\n|   2|Alice|\n|   5|  Bob|\n+----+-----+", ">>> df.withColumnsRenamed({\"non_existing\": \"new_name\"}).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+", ">>> df.withColumnsRenamed({}).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+"], "Parameters": [["colsMap dict", "A dict of existing column names and corresponding desired column names.\nCurrently, only a single map is supported."]], "Returns": [["DataFrame", "DataFrame with renamed columns."]], "Category": ["DataFrame"], "index": 624}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withMetadata.html#pyspark.sql.DataFrame.withMetadata"], "Title": ["DataFrame.withMetadata"], "Feature": ["DataFrame.withMetadata"], "Description": "Returns a newDataFrameby updating an existing column with metadata.", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n>>> df_meta.schema['age'].metadata\n{'foo': 'bar'}"], "Parameters": [["columnName str", "string, name of the existing column to update the metadata."], ["metadata dict", "dict, new metadata to be assigned to df.schema[columnName].metadata"]], "Returns": [["DataFrame", "DataFrame with updated metadata column."]], "Category": ["DataFrame"], "index": 625}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withWatermark.html#pyspark.sql.DataFrame.withWatermark"], "Title": ["DataFrame.withWatermark"], "Feature": ["DataFrame.withWatermark"], "Description": "Defines an event time watermark for thisDataFrame. A watermark tracks a point\nin time before which we assume no more late data is going to arrive.\nThe current watermark is computed by looking at theMAX(eventTime)seen across\nall of the partitions in the query minus a user specifieddelayThreshold.  Due to the cost\nof coordinating this value across partitions, the actual watermark used is only guaranteed\nto be at leastdelayThresholdbehind the actual event time.  In some cases we may still\nprocess records that arrive more thandelayThresholdlate.\nNotes\nThis is a feature only for Structured Streaming.\nThis API is evolving.", "Examples": [">>> from pyspark.sql import Row\n>>> from pyspark.sql.functions import timestamp_seconds\n>>> df = spark.readStream.format(\"rate\").load().selectExpr(\n...     \"value % 5 AS value\", \"timestamp\")\n>>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\nDataFrame[value: bigint, time: timestamp]", ">>> import time\n>>> from pyspark.sql.functions import window\n>>> query = (df\n...     .withWatermark(\"timestamp\", \"10 minutes\")\n...     .groupBy(\n...         window(df.timestamp, \"10 minutes\", \"5 minutes\"),\n...         df.value)\n...     ).count().writeStream.outputMode(\"complete\").format(\"console\").start()\n>>> time.sleep(3)\n>>> query.stop()"], "Parameters": [["eventTime str", "the name of the column that contains the event time of the row."], ["delayThreshold str", "the minimum delay to wait to data to arrive late, relative to the\nlatest record that has been processed in the form of an interval\n(e.g. “1 minute” or “5 hours”)."]], "Returns": [["DataFrame", "Watermarked DataFrame"]], "Category": ["DataFrame"], "index": 626}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.write.html#pyspark.sql.DataFrame.write"], "Title": ["DataFrame.write"], "Feature": ["DataFrame.write"], "Description": "Interface for saving the content of the non-streamingDataFrameout into external\nstorage.", "Examples": [">>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> type(df.write)\n<class '...readwriter.DataFrameWriter'>", ">>> _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n>>> df.write.saveAsTable(\"tab2\")\n>>> _ = spark.sql(\"DROP TABLE tab2\")"], "Parameters": [], "Returns": [["DataFrameWriter", ""]], "Category": ["DataFrame"], "index": 627}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.writeStream.html#pyspark.sql.DataFrame.writeStream"], "Title": ["DataFrame.writeStream"], "Feature": ["DataFrame.writeStream"], "Description": "Interface for saving the content of the streamingDataFrameout into external\nstorage.\nNotes\nThis API is evolving.", "Examples": [">>> import time\n>>> import tempfile\n>>> df = spark.readStream.format(\"rate\").load()\n>>> type(df.writeStream)\n<class '...streaming.readwriter.DataStreamWriter'>", ">>> with tempfile.TemporaryDirectory(prefix=\"writeStream\") as d:\n...     # Create a table with Rate source.\n...     query = df.writeStream.toTable(\n...         \"my_table\", checkpointLocation=d)\n...     time.sleep(3)\n...     query.stop()"], "Parameters": [], "Returns": [["DataStreamWriter", ""]], "Category": ["DataFrame"], "index": 628}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.writeTo.html#pyspark.sql.DataFrame.writeTo"], "Title": ["DataFrame.writeTo"], "Feature": ["DataFrame.writeTo"], "Description": "Create a write configuration builder for v2 sources.\nThis builder is used to configure and execute write operations.\nFor example, to append or create or replace existing tables.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.writeTo(\"catalog.db.table\").append()  \n>>> df.writeTo(                              \n...     \"catalog.db.table\"\n... ).partitionedBy(\"col\").createOrReplace()"], "Parameters": [["table str", "Target table name to write to."]], "Returns": [["DataFrameWriterV2", "DataFrameWriterV2 to use further to specify how to save the data"]], "Category": ["DataFrame"], "index": 629}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.mergeInto.html#pyspark.sql.DataFrame.mergeInto"], "Title": ["DataFrame.mergeInto"], "Feature": ["DataFrame.mergeInto"], "Description": "Merges a set of updates, insertions, and deletions based on a source table into\na target table.\nNotes\nThis method does not support streaming queries.", "Examples": [">>> from pyspark.sql.functions import expr\n>>> source = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"id\", \"name\"])\n>>> (source.mergeInto(\"target\", \"id\")  \n...     .whenMatched().update({ \"name\": source.name })\n...     .whenNotMatched().insertAll()\n...     .whenNotMatchedBySource().delete()\n...     .merge())"], "Parameters": [["table str", "Target table name to merge into."], ["condition Column", "The condition that determines whether a row in the target table matches one in the\nsource DataFrame."]], "Returns": [["MergeIntoWriter", "MergeIntoWriter to use further to specify how to merge the source DataFrame\ninto the target table."]], "Category": ["DataFrame"], "index": 630}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.pandas_api.html#pyspark.sql.DataFrame.pandas_api"], "Title": ["DataFrame.pandas_api"], "Feature": ["DataFrame.pandas_api"], "Description": "Converts the existing DataFrame into a pandas-on-Spark DataFrame.\nIf a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\nto pandas-on-Spark, it will lose the index information and the original index\nwill be turned into a normal column.\nThis is only available if Pandas is installed and available.\nSee alsopyspark.pandas.frame.DataFrame.to_spark", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])", ">>> df.pandas_api()  \n   age   name\n0   14    Tom\n1   23  Alice\n2   16    Bob", ">>> df.pandas_api(index_col=\"age\")  \n      name\nage\n14     Tom\n23   Alice\n16     Bob"], "Parameters": [["index_col: str or list of str, optional", "Index column of table in Spark."]], "Returns": [["PandasOnSparkDataFrame", ""]], "Category": ["DataFrame"], "index": 631}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameNaFunctions.drop.html#pyspark.sql.DataFrameNaFunctions.drop"], "Title": ["DataFrameNaFunctions.drop"], "Feature": ["DataFrameNaFunctions.drop"], "Description": "Returns a newDataFrameomitting rows with null or NaN values.DataFrame.dropna()andDataFrameNaFunctions.drop()are\naliases of each other.", "Examples": [">>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([\n...     Row(age=10, height=80.0, name=\"Alice\"),\n...     Row(age=5, height=float(\"nan\"), name=\"Bob\"),\n...     Row(age=None, height=None, name=\"Tom\"),\n...     Row(age=None, height=float(\"nan\"), name=None),\n... ])", ">>> df.na.drop().show()\n+---+------+-----+\n|age|height| name|\n+---+------+-----+\n| 10|  80.0|Alice|\n+---+------+-----+", ">>> df.na.drop(how='all').show()\n+----+------+-----+\n| age|height| name|\n+----+------+-----+\n|  10|  80.0|Alice|\n|   5|   NaN|  Bob|\n|NULL|  NULL|  Tom|\n+----+------+-----+", ">>> df.na.drop(thresh=2).show()\n+---+------+-----+\n|age|height| name|\n+---+------+-----+\n| 10|  80.0|Alice|\n|  5|   NaN|  Bob|\n+---+------+-----+", ">>> df.na.drop(subset=['age', 'name']).show()\n+---+------+-----+\n|age|height| name|\n+---+------+-----+\n| 10|  80.0|Alice|\n|  5|   NaN|  Bob|\n+---+------+-----+"], "Parameters": [["how str, optional, the values that can be ‘any’ or ‘all’, default ‘any’.", "If ‘any’, drop a row if it contains any nulls.\nIf ‘all’, drop a row only if all its values are null."], ["thresh: int, optional, default None.", "If specified, drop rows that have less than thresh non-null values.\nThis overwrites the how parameter."], ["subset str, tuple or list, optional", "optional list of column names to consider."]], "Returns": [["DataFrame", "DataFrame with null only rows excluded."]], "Category": ["DataFrame"], "index": 632}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameNaFunctions.fill.html#pyspark.sql.DataFrameNaFunctions.fill"], "Title": ["DataFrameNaFunctions.fill"], "Feature": ["DataFrameNaFunctions.fill"], "Description": "Returns a newDataFramewhich null values are filled with new value.DataFrame.fillna()andDataFrameNaFunctions.fill()are\naliases of each other.", "Examples": [">>> df = spark.createDataFrame([\n...     (10, 80.5, \"Alice\", None),\n...     (5, None, \"Bob\", None),\n...     (None, None, \"Tom\", None),\n...     (None, None, None, True)],\n...     schema=[\"age\", \"height\", \"name\", \"bool\"])", ">>> df.na.fill(50).show()\n+---+------+-----+----+\n|age|height| name|bool|\n+---+------+-----+----+\n| 10|  80.5|Alice|NULL|\n|  5|  50.0|  Bob|NULL|\n| 50|  50.0|  Tom|NULL|\n| 50|  50.0| NULL|true|\n+---+------+-----+----+", ">>> df.na.fill(False).show()\n+----+------+-----+-----+\n| age|height| name| bool|\n+----+------+-----+-----+\n|  10|  80.5|Alice|false|\n|   5|  NULL|  Bob|false|\n|NULL|  NULL|  Tom|false|\n|NULL|  NULL| NULL| true|\n+----+------+-----+-----+", ">>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n+---+------+-------+----+\n|age|height|   name|bool|\n+---+------+-------+----+\n| 10|  80.5|  Alice|NULL|\n|  5|  NULL|    Bob|NULL|\n| 50|  NULL|    Tom|NULL|\n| 50|  NULL|unknown|true|\n+---+------+-------+----+", ">>> df.na.fill(value = 'Spark', subset = 'name').show()\n+----+------+-----+----+\n| age|height| name|bool|\n+----+------+-----+----+\n|  10|  80.5|Alice|NULL|\n|   5|  NULL|  Bob|NULL|\n|NULL|  NULL|  Tom|NULL|\n|NULL|  NULL|Spark|true|\n+----+------+-----+----+"], "Parameters": [["value int, float, string, bool or dict, the value to replace null values with.", "If the value is a dict, then subset is ignored and value must be a mapping\nfrom column name (string) to replacement value. The replacement value must be\nan int, float, boolean, or string."], ["subset str, tuple or list, optional", "optional list of column names to consider.\nColumns specified in subset that do not have matching data types are ignored.\nFor example, if value is a string, and subset contains a non-string column,\nthen the non-string column is simply ignored."]], "Returns": [["DataFrame", "DataFrame with replaced null values."]], "Category": ["DataFrame"], "index": 633}
