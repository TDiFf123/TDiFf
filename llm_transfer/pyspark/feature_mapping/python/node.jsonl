{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html#pyspark.sql.DataFrame.select"], "Title": ["DataFrame.select"], "Feature": ["DataFrame.select"], "Description": "Projects a set of expressions and returns a newDataFrame.", "Examples": [">>> df = spark.createDataFrame([\n...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])", ">>> df.select('*').show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+", ">>> df.select(df.name, (df.age + 10).alias('age')).show()\n+-----+---+\n| name|age|\n+-----+---+\n|Alice| 12|\n|  Bob| 15|\n+-----+---+"], "Parameters": [["cols str, Column , or list", "column names (string) or expressions ( Column ).\nIf one of the column names is ‘*’, that column is expanded to include all columns\nin the current DataFrame ."]], "Returns": [["DataFrame", "A DataFrame with subset (or all) of columns."]], "Category": ["DataFrame"], "index": 0}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html#pyspark.sql.DataFrame.orderBy"], "Title": ["DataFrame.orderBy"], "Feature": ["DataFrame.orderBy"], "Description": "Returns a newDataFramesorted by the specified column(s).\nNotes\nA column ordinal starts from 1, which is different from the\n0-based__getitem__().\nIf a column ordinal is negative, it means sort descending.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])", ">>> df.sort(sf.asc(\"age\")).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+", ">>> df.sort(1).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+", ">>> df.sort(df.age.desc()).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n+---+-----+", ">>> df.orderBy(df.age.desc()).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n+---+-----+", ">>> df.sort(\"age\", ascending=False).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n+---+-----+", ">>> df.sort(-1).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n+---+-----+", ">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([\n...     (2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.orderBy(sf.desc(\"age\"), \"name\").show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n|  2|  Bob|\n+---+-----+", ">>> df.orderBy(-1, \"name\").show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n|  2|  Bob|\n+---+-----+", ">>> df.orderBy(-1, 2).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n|  2|  Bob|\n+---+-----+", ">>> df.orderBy([\"age\", \"name\"], ascending=[False, False]).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|  Bob|\n|  2|Alice|\n+---+-----+", ">>> df.orderBy([1, \"name\"], ascending=[False, False]).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|  Bob|\n|  2|Alice|\n+---+-----+", ">>> df.orderBy([1, 2], ascending=[False, False]).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|  Bob|\n|  2|Alice|\n+---+-----+"], "Parameters": [["cols int, str, list, or Column , optional", "list of Column or column names or column ordinals to sort by. Changed in version 4.0.0: Supports column ordinal."]], "Returns": [["DataFrame", "Sorted DataFrame."]], "Category": ["DataFrame"], "index": 1}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.subtract.html#pyspark.sql.DataFrame.subtract"], "Title": ["DataFrame.subtract"], "Feature": ["DataFrame.subtract"], "Description": "Return a newDataFramecontaining rows in thisDataFramebut not in anotherDataFrame.\nSee alsoDataFrame.exceptAllSimilar tosubtract, but preserves duplicates.\nNotes\nThis is equivalent toEXCEPT DISTINCTin SQL.", "Examples": [">>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n>>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n>>> result_df = df1.subtract(df2)\n>>> result_df.show()\n+---+---+\n| C1| C2|\n+---+---+\n|  c|  4|\n+---+---+", ">>> df1 = spark.createDataFrame([(1, \"A\"), (2, \"B\")], [\"id\", \"value\"])\n>>> df2 = spark.createDataFrame([(2, \"B\"), (3, \"C\")], [\"id\", \"value\"])\n>>> result_df = df1.subtract(df2)\n>>> result_df.show()\n+---+-----+\n| id|value|\n+---+-----+\n|  1|    A|\n+---+-----+", ">>> df1 = spark.createDataFrame([(1, 2)], [\"A\", \"B\"])\n>>> df2 = spark.createDataFrame([(1, 2)], [\"C\", \"D\"])\n>>> result_df = df1.subtract(df2)\n>>> result_df.show()\n+---+---+\n|  A|  B|\n+---+---+\n+---+---+"], "Parameters": [["other DataFrame", "Another DataFrame that needs to be subtracted."]], "Returns": [["DataFrame", "Subtracted DataFrame."]], "Category": ["DataFrame"], "index": 2}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.intersect.html#pyspark.sql.DataFrame.intersect"], "Title": ["DataFrame.intersect"], "Feature": ["DataFrame.intersect"], "Description": "Return a newDataFramecontaining rows only in\nboth thisDataFrameand anotherDataFrame.\nNote that any duplicates are removed. To preserve duplicates\nuseintersectAll().\nNotes\nThis is equivalent toINTERSECTin SQL.", "Examples": [">>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n>>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n>>> result_df = df1.intersect(df2).sort(\"C1\", \"C2\")\n>>> result_df.show()\n+---+---+\n| C1| C2|\n+---+---+\n|  a|  1|\n|  b|  3|\n+---+---+", ">>> df1 = spark.createDataFrame([(1, \"A\"), (2, \"B\")], [\"id\", \"value\"])\n>>> df2 = spark.createDataFrame([(2, \"B\"), (3, \"C\")], [\"id\", \"value\"])\n>>> result_df = df1.intersect(df2).sort(\"id\", \"value\")\n>>> result_df.show()\n+---+-----+\n| id|value|\n+---+-----+\n|  2|    B|\n+---+-----+", ">>> df1 = spark.createDataFrame([(1, 2), (1, 2), (3, 4)], [\"A\", \"B\"])\n>>> df2 = spark.createDataFrame([(1, 2), (1, 2)], [\"C\", \"D\"])\n>>> result_df = df1.intersect(df2).sort(\"A\", \"B\")\n>>> result_df.show()\n+---+---+\n|  A|  B|\n+---+---+\n|  1|  2|\n+---+---+"], "Parameters": [["other DataFrame", "Another DataFrame that needs to be combined."]], "Returns": [["DataFrame", "Combined DataFrame."]], "Category": ["DataFrame"], "index": 3}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.union.html#pyspark.sql.DataFrame.union"], "Title": ["DataFrame.union"], "Feature": ["DataFrame.union"], "Description": "Return a newDataFramecontaining the union of rows in this and anotherDataFrame.\nSee alsoDataFrame.unionAll\nNotes\nThis method performs a SQL-style set union of the rows from bothDataFrameobjects,\nwith no automatic deduplication of elements.\nUse thedistinct()method to perform deduplication of rows.\nThe method resolves columns by position (not by name), following the standard behavior\nin SQL.", "Examples": [">>> df1 = spark.createDataFrame([(1, 'A'), (2, 'B')], ['id', 'value'])\n>>> df2 = spark.createDataFrame([(3, 'C'), (4, 'D')], ['id', 'value'])\n>>> df3 = df1.union(df2)\n>>> df3.show()\n+---+-----+\n| id|value|\n+---+-----+\n|  1|    A|\n|  2|    B|\n|  3|    C|\n|  4|    D|\n+---+-----+", ">>> from pyspark.sql.functions import lit\n>>> df1 = spark.createDataFrame([(100001, 1), (100002, 2)], schema=\"id LONG, money INT\")\n>>> df2 = spark.createDataFrame([(3, 100003), (4, 100003)], schema=\"money INT, id LONG\")\n>>> df1 = df1.withColumn(\"age\", lit(30))\n>>> df2 = df2.withColumn(\"age\", lit(40))\n>>> df3 = df1.union(df2)\n>>> df3.show()\n+------+------+---+\n|    id| money|age|\n+------+------+---+\n|100001|     1| 30|\n|100002|     2| 30|\n|     3|100003| 40|\n|     4|100003| 40|\n+------+------+---+", ">>> df1 = spark.createDataFrame([(1, 2)], [\"A\", \"B\"])\n>>> df2 = spark.createDataFrame([(3, 4)], [\"C\", \"D\"])\n>>> df3 = df1.union(df2)\n>>> df3.show()\n+---+---+\n|  A|  B|\n+---+---+\n|  1|  2|\n|  3|  4|\n+---+---+", ">>> df1 = spark.createDataFrame([(1, 'A'), (2, 'B'), (3, 'C')], ['id', 'value'])\n>>> df2 = spark.createDataFrame([(3, 'C'), (4, 'D')], ['id', 'value'])\n>>> df3 = df1.union(df2).distinct().sort(\"id\")\n>>> df3.show()\n+---+-----+\n| id|value|\n+---+-----+\n|  1|    A|\n|  2|    B|\n|  3|    C|\n|  4|    D|\n+---+-----+"], "Parameters": [["other DataFrame", "Another DataFrame that needs to be unioned."]], "Returns": [["DataFrame", "A new DataFrame containing the combined rows with corresponding columns."]], "Category": ["DataFrame"], "index": 4}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sortWithinPartitions.html#pyspark.sql.DataFrame.sortWithinPartitions"], "Title": ["DataFrame.sortWithinPartitions"], "Feature": ["DataFrame.sortWithinPartitions"], "Description": "Returns a newDataFramewith each partition sorted by the specified column(s).\nNotes\nA column ordinal starts from 1, which is different from the\n0-based__getitem__().\nIf a column ordinal is negative, it means sort descending.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n>>> df.sortWithinPartitions(\"age\", ascending=False)\nDataFrame[age: bigint, name: string]", ">>> df.coalesce(1).sortWithinPartitions(1).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+", ">>> df.coalesce(1).sortWithinPartitions(-1).show()\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n+---+-----+"], "Parameters": [["cols int, str, list or Column , optional", "list of Column or column names or column ordinals to sort by. Changed in version 4.0.0: Supports column ordinal."]], "Returns": [["DataFrame", "DataFrame sorted by partitions."]], "Category": ["DataFrame"], "index": 5}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sample.html#pyspark.sql.DataFrame.sample"], "Title": ["DataFrame.sample"], "Feature": ["DataFrame.sample"], "Description": "Returns a sampled subset of thisDataFrame.\nNotes\nThis is not guaranteed to provide exactly the fraction specified of the total\ncount of the givenDataFrame.\nfractionis required and,withReplacementandseedare optional.", "Examples": [">>> df = spark.range(10)\n>>> df.sample(0.5, 3).count() \n7\n>>> df.sample(fraction=0.5, seed=3).count() \n7\n>>> df.sample(withReplacement=True, fraction=0.5, seed=3).count() \n1\n>>> df.sample(1.0).count()\n10\n>>> df.sample(fraction=1.0).count()\n10\n>>> df.sample(False, fraction=1.0).count()\n10"], "Parameters": [["withReplacement bool, optional", "Sample with replacement or not (default False )."], ["fraction float, optional", "Fraction of rows to generate, range [0.0, 1.0]."], ["seed int, optional", "Seed for sampling (default a random seed)."]], "Returns": [["DataFrame", "Sampled rows from given DataFrame."]], "Category": ["DataFrame"], "index": 6}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.where.html#pyspark.sql.DataFrame.where"], "Title": ["DataFrame.where"], "Feature": ["DataFrame.where"], "Description": "where()is an alias forfilter().", "Examples": [], "Parameters": [], "Returns": [], "Category": ["DataFrame"], "index": 7}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.when.html#pyspark.sql.functions.when"], "Title": ["when"], "Feature": ["when"], "Description": "Evaluates a list of conditions and returns one of multiple possible result expressions.\nIfpyspark.sql.Column.otherwise()is not invoked, None is returned for unmatched\nconditions.\nSee alsopyspark.sql.Column.when()pyspark.sql.Column.otherwise()", "Examples": [">>> import pyspark.sql.functions as sf\n>>> df = spark.range(3)\n>>> df.select(\"*\", sf.when(df['id'] == 2, 3).otherwise(4)).show()\n+---+------------------------------------+\n| id|CASE WHEN (id = 2) THEN 3 ELSE 4 END|\n+---+------------------------------------+\n|  0|                                   4|\n|  1|                                   4|\n|  2|                                   3|\n+---+------------------------------------+", ">>> df.select(\"*\", sf.when(df.id == 2, df.id + 1)).show()\n+---+------------------------------------+\n| id|CASE WHEN (id = 2) THEN (id + 1) END|\n+---+------------------------------------+\n|  0|                                NULL|\n|  1|                                NULL|\n|  2|                                   3|\n+---+------------------------------------+"], "Parameters": [["condition Column", "a boolean Column expression."], ["value", "a literal value, or a Column expression."]], "Returns": [["Column", "column representing when expression."]], "Category": ["Functions"], "index": 8}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.pivot.html#pyspark.sql.GroupedData.pivot"], "Title": ["GroupedData.pivot"], "Feature": ["GroupedData.pivot"], "Description": "Pivots a column of the currentDataFrameand performs the specified aggregation.", "Examples": [">>> from pyspark.sql import Row\n>>> df1 = spark.createDataFrame([\n...     Row(course=\"dotNET\", year=2012, earnings=10000),\n...     Row(course=\"Java\", year=2012, earnings=20000),\n...     Row(course=\"dotNET\", year=2012, earnings=5000),\n...     Row(course=\"dotNET\", year=2013, earnings=48000),\n...     Row(course=\"Java\", year=2013, earnings=30000),\n... ])\n>>> df1.show()\n+------+----+--------+\n|course|year|earnings|\n+------+----+--------+\n|dotNET|2012|   10000|\n|  Java|2012|   20000|\n|dotNET|2012|    5000|\n|dotNET|2013|   48000|\n|  Java|2013|   30000|\n+------+----+--------+\n>>> df2 = spark.createDataFrame([\n...     Row(training=\"expert\", sales=Row(course=\"dotNET\", year=2012, earnings=10000)),\n...     Row(training=\"junior\", sales=Row(course=\"Java\", year=2012, earnings=20000)),\n...     Row(training=\"expert\", sales=Row(course=\"dotNET\", year=2012, earnings=5000)),\n...     Row(training=\"junior\", sales=Row(course=\"dotNET\", year=2013, earnings=48000)),\n...     Row(training=\"expert\", sales=Row(course=\"Java\", year=2013, earnings=30000)),\n... ])  \n>>> df2.show()  \n+--------+--------------------+\n|training|               sales|\n+--------+--------------------+\n|  expert|{dotNET, 2012, 10...|\n|  junior| {Java, 2012, 20000}|\n|  expert|{dotNET, 2012, 5000}|\n|  junior|{dotNET, 2013, 48...|\n|  expert| {Java, 2013, 30000}|\n+--------+--------------------+", ">>> df1.groupBy(\"year\").pivot(\n...     \"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").sort(\"year\").show()\n+----+------+-----+\n|year|dotNET| Java|\n+----+------+-----+\n|2012| 15000|20000|\n|2013| 48000|30000|\n+----+------+-----+", ">>> df1.groupBy(\"year\").pivot(\"course\").sum(\"earnings\").sort(\"year\").show()\n+----+-----+------+\n|year| Java|dotNET|\n+----+-----+------+\n|2012|20000| 15000|\n|2013|30000| 48000|\n+----+-----+------+\n>>> df2.groupBy(\n...     \"sales.year\").pivot(\"sales.course\").sum(\"sales.earnings\").sort(\"year\").show()\n... \n+----+-----+------+\n|year| Java|dotNET|\n+----+-----+------+\n|2012|20000| 15000|\n|2013|30000| 48000|\n+----+-----+------+"], "Parameters": [["pivot_col str", "Name of the column to pivot."], ["values list, optional", "List of values that will be translated to columns in the output DataFrame. If values is not provided, Spark will eagerly compute the distinct values in pivot_col so it can determine the resulting schema of the transformation. To avoid\nany eager computations, provide an explicit list of values."]], "Returns": [], "Category": ["Grouping"], "index": 9}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.unpivot.html#pyspark.sql.DataFrame.unpivot"], "Title": ["DataFrame.unpivot"], "Feature": ["DataFrame.unpivot"], "Description": "Unpivot a DataFrame from wide format to long format, optionally leaving\nidentifier columns set. This is the reverse togroupBy(…).pivot(…).agg(…),\nexcept for the aggregation, which cannot be reversed.\nThis function is useful to massage a DataFrame into a format where some\ncolumns are identifier columns (“ids”), while all other columns (“values”)\nare “unpivoted” to the rows, leaving just two non-id columns, named as given\nbyvariableColumnNameandvalueColumnName.\nWhen no “id” columns are given, the unpivoted DataFrame consists of only the\n“variable” and “value” columns.\nThevaluescolumns must not be empty so at least one value must be given to be unpivoted.\nWhenvaluesisNone, all non-id columns will be unpivoted.\nAll “value” columns must share a least common data type. Unless they are the same data type,\nall “value” columns are cast to the nearest common data type. For instance, typesIntegerTypeandLongTypeare cast toLongType, whileIntegerTypeandStringTypedo not have a common data type andunpivotfails.", "Examples": [">>> df = spark.createDataFrame(\n...     [(1, 11, 1.1), (2, 12, 1.2)],\n...     [\"id\", \"int\", \"double\"],\n... )\n>>> df.show()\n+---+---+------+\n| id|int|double|\n+---+---+------+\n|  1| 11|   1.1|\n|  2| 12|   1.2|\n+---+---+------+", ">>> df.unpivot(\"id\", [\"int\", \"double\"], \"var\", \"val\").show()\n+---+------+----+\n| id|   var| val|\n+---+------+----+\n|  1|   int|11.0|\n|  1|double| 1.1|\n|  2|   int|12.0|\n|  2|double| 1.2|\n+---+------+----+"], "Parameters": [["ids str, Column, tuple, list", "Column(s) to use as identifiers. Can be a single column or column name,\nor a list or tuple for multiple columns."], ["values str, Column, tuple, list, optional", "Column(s) to unpivot. Can be a single column or column name, or a list or tuple\nfor multiple columns. If specified, must not be empty. If not specified, uses all\ncolumns that are not set as ids ."], ["variableColumnName str", "Name of the variable column."], ["valueColumnName str", "Name of the value column."]], "Returns": [["DataFrame", "Unpivoted DataFrame."]], "Category": ["DataFrame"], "index": 10}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.lateralJoin.html#pyspark.sql.DataFrame.lateralJoin"], "Title": ["DataFrame.lateralJoin"], "Feature": ["DataFrame.lateralJoin"], "Description": "Lateral joins with anotherDataFrame, using the given join expression.\nA lateral join (also known as a correlated join) is a type of join where each row from\none DataFrame is used as input to a subquery or a derived table that computes a result\nspecific to that row. The right sideDataFramecan reference columns from the current\nrow of the left sideDataFrame, allowing for more complex and context-dependent results\nthan a standard join.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> from pyspark.sql import Row\n>>> customers_data = [\n...     Row(customer_id=1, name=\"Alice\"), Row(customer_id=2, name=\"Bob\"),\n...     Row(customer_id=3, name=\"Charlie\"), Row(customer_id=4, name=\"Diana\")\n... ]\n>>> customers = spark.createDataFrame(customers_data)\n>>> orders_data = [\n...     Row(order_id=101, customer_id=1, order_date=\"2024-01-10\",\n...         items=[Row(product=\"laptop\", quantity=5), Row(product=\"mouse\", quantity=12)]),\n...     Row(order_id=102, customer_id=1, order_date=\"2024-02-15\",\n...         items=[Row(product=\"phone\", quantity=2), Row(product=\"charger\", quantity=15)]),\n...     Row(order_id=105, customer_id=1, order_date=\"2024-03-20\",\n...         items=[Row(product=\"tablet\", quantity=4)]),\n...     Row(order_id=103, customer_id=2, order_date=\"2024-01-12\",\n...         items=[Row(product=\"tablet\", quantity=8)]),\n...     Row(order_id=104, customer_id=2, order_date=\"2024-03-05\",\n...         items=[Row(product=\"laptop\", quantity=7)]),\n...     Row(order_id=106, customer_id=3, order_date=\"2024-04-05\",\n...         items=[Row(product=\"monitor\", quantity=1)]),\n... ]\n>>> orders = spark.createDataFrame(orders_data)", ">>> customers.join(orders, \"customer_id\").lateralJoin(\n...     spark.tvf.explode(sf.col(\"items\").outer()).select(\"col.*\")\n... ).select(\n...     \"customer_id\", \"name\", \"order_id\", \"order_date\", \"product\", \"quantity\"\n... ).orderBy(\"customer_id\", \"order_id\", \"product\").show()\n+-----------+-------+--------+----------+-------+--------+\n|customer_id|   name|order_id|order_date|product|quantity|\n+-----------+-------+--------+----------+-------+--------+\n|          1|  Alice|     101|2024-01-10| laptop|       5|\n|          1|  Alice|     101|2024-01-10|  mouse|      12|\n|          1|  Alice|     102|2024-02-15|charger|      15|\n|          1|  Alice|     102|2024-02-15|  phone|       2|\n|          1|  Alice|     105|2024-03-20| tablet|       4|\n|          2|    Bob|     103|2024-01-12| tablet|       8|\n|          2|    Bob|     104|2024-03-05| laptop|       7|\n|          3|Charlie|     106|2024-04-05|monitor|       1|\n+-----------+-------+--------+----------+-------+--------+", ">>> customers.alias(\"c\").lateralJoin(\n...     orders.alias(\"o\")\n...     .where(sf.col(\"o.customer_id\") == sf.col(\"c.customer_id\").outer())\n...     .select(\"order_id\", \"order_date\")\n...     .orderBy(sf.col(\"order_date\").desc())\n...     .limit(2),\n...     how=\"left\"\n... ).orderBy(\"customer_id\", \"order_id\").show()\n+-----------+-------+--------+----------+\n|customer_id|   name|order_id|order_date|\n+-----------+-------+--------+----------+\n|          1|  Alice|     102|2024-02-15|\n|          1|  Alice|     105|2024-03-20|\n|          2|    Bob|     103|2024-01-12|\n|          2|    Bob|     104|2024-03-05|\n|          3|Charlie|     106|2024-04-05|\n|          4|  Diana|    NULL|      NULL|\n+-----------+-------+--------+----------+"], "Parameters": [["other DataFrame", "Right side of the join"], ["on Column , optional", "a join expression (Column)."], ["how str, optional", "default inner . Must be one of: inner , cross , left , leftouter ,\nand left_outer ."]], "Returns": [["DataFrame", "Joined DataFrame."]], "Category": ["DataFrame"], "index": 11}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.transform.html#pyspark.sql.DataFrame.transform"], "Title": ["DataFrame.transform"], "Feature": ["DataFrame.transform"], "Description": "Returns a newDataFrame. Concise syntax for chaining custom transformations.", "Examples": [">>> from pyspark.sql.functions import col\n>>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n>>> def cast_all_to_int(input_df):\n...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n...\n>>> def sort_columns_asc(input_df):\n...     return input_df.select(*sorted(input_df.columns))\n...\n>>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n+-----+---+\n|float|int|\n+-----+---+\n|    1|  1|\n|    2|  2|\n+-----+---+", ">>> def add_n(input_df, n):\n...     return input_df.select([(col(col_name) + n).alias(col_name)\n...                             for col_name in input_df.columns])\n>>> df.transform(add_n, 1).transform(add_n, n=10).show()\n+---+-----+\n|int|float|\n+---+-----+\n| 12| 12.0|\n| 13| 13.0|\n+---+-----+"], "Parameters": [["func function", "a function that takes and returns a DataFrame ."], ["*args", "Positional arguments to pass to func. New in version 3.3.0."], ["**kwargs", "Keyword arguments to pass to func. New in version 3.3.0."]], "Returns": [["DataFrame", "Transformed DataFrame."]], "Category": ["DataFrame"], "index": 12}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html#pyspark.sql.DataFrame.repartition"], "Title": ["DataFrame.repartition"], "Feature": ["DataFrame.repartition"], "Description": "Returns a newDataFramepartitioned by the given partitioning expressions. The\nresultingDataFrameis hash partitioned.", "Examples": [">>> from pyspark.sql import functions as sf\n>>> df = spark.range(0, 64, 1, 9).withColumn(\n...     \"name\", sf.concat(sf.lit(\"name_\"), sf.col(\"id\").cast(\"string\"))\n... ).withColumn(\n...     \"age\", sf.col(\"id\") - 32\n... )\n>>> df.select(\n...     sf.spark_partition_id().alias(\"partition\")\n... ).distinct().sort(\"partition\").show()\n+---------+\n|partition|\n+---------+\n|        0|\n|        1|\n|        2|\n|        3|\n|        4|\n|        5|\n|        6|\n|        7|\n|        8|\n+---------+", ">>> df.repartition(10).select(\n...     sf.spark_partition_id().alias(\"partition\")\n... ).distinct().sort(\"partition\").show()\n+---------+\n|partition|\n+---------+\n|        0|\n|        1|\n|        2|\n|        3|\n|        4|\n|        5|\n|        6|\n|        7|\n|        8|\n|        9|\n+---------+", ">>> df.repartition(7, \"age\").select(\n...     sf.spark_partition_id().alias(\"partition\")\n... ).distinct().sort(\"partition\").show()\n+---------+\n|partition|\n+---------+\n|        0|\n|        1|\n|        2|\n|        3|\n|        4|\n|        5|\n|        6|\n+---------+", ">>> df.repartition(3, \"name\", \"age\").select(\n...     sf.spark_partition_id().alias(\"partition\")\n... ).distinct().sort(\"partition\").show()\n+---------+\n|partition|\n+---------+\n|        0|\n|        1|\n|        2|\n+---------+"], "Parameters": [["numPartitions int", "can be an int to specify the target number of partitions or a Column.\nIf it is a Column, it will be used as the first partitioning column. If not specified,\nthe default number of partitions is used."], ["cols str or Column", "partitioning columns. Changed in version 1.6.0: Added optional arguments to specify the partitioning columns. Also made numPartitions\noptional if partitioning columns are specified."]], "Returns": [["DataFrame", "Repartitioned DataFrame."]], "Category": ["DataFrame"], "index": 13}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html#pyspark.sql.DataFrame.groupBy"], "Title": ["DataFrame.groupBy"], "Feature": ["DataFrame.groupBy"], "Description": "Groups theDataFrameby the specified columns so that aggregation\ncan be performed on them.\nSeeGroupedDatafor all the available aggregate functions.\ngroupby()is an alias forgroupBy().\nNotes\nA column ordinal starts from 1, which is different from the\n0-based__getitem__().", "Examples": [">>> df = spark.createDataFrame([\n...     (\"Alice\", 2), (\"Bob\", 2), (\"Bob\", 2), (\"Bob\", 5)], schema=[\"name\", \"age\"])", ">>> df.groupBy().avg().show()\n+--------+\n|avg(age)|\n+--------+\n|    2.75|\n+--------+", ">>> df.groupBy(\"name\").agg({\"age\": \"sum\"}).sort(\"name\").show()\n+-----+--------+\n| name|sum(age)|\n+-----+--------+\n|Alice|       2|\n|  Bob|       9|\n+-----+--------+", ">>> df.groupBy(df.name).max().sort(\"name\").show()\n+-----+--------+\n| name|max(age)|\n+-----+--------+\n|Alice|       2|\n|  Bob|       5|\n+-----+--------+", ">>> df.groupBy(1).max().sort(\"name\").show()\n+-----+--------+\n| name|max(age)|\n+-----+--------+\n|Alice|       2|\n|  Bob|       5|\n+-----+--------+", ">>> df.groupBy([\"name\", df.age]).count().sort(\"name\", \"age\").show()\n+-----+---+-----+\n| name|age|count|\n+-----+---+-----+\n|Alice|  2|    1|\n|  Bob|  2|    2|\n|  Bob|  5|    1|\n+-----+---+-----+", ">>> df.groupBy([df.name, 2]).count().sort(\"name\", \"age\").show()\n+-----+---+-----+\n| name|age|count|\n+-----+---+-----+\n|Alice|  2|    1|\n|  Bob|  2|    2|\n|  Bob|  5|    1|\n+-----+---+-----+"], "Parameters": [["cols list, str, int or Column", "The columns to group by.\nEach element can be a column name (string) or an expression ( Column )\nor a column ordinal (int, 1-based) or list of them. Changed in version 4.0.0: Supports column ordinal."]], "Returns": [["GroupedData", "A GroupedData object representing the grouped data by the specified columns."]], "Category": ["DataFrame"], "index": 14}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html#pyspark.sql.DataFrame.filter"], "Title": ["DataFrame.filter"], "Feature": ["DataFrame.filter"], "Description": "Filters rows using the given condition.\nwhere()is an alias forfilter().", "Examples": [">>> df = spark.createDataFrame([\n...     (2, \"Alice\", \"Math\"), (5, \"Bob\", \"Physics\"), (7, \"Charlie\", \"Chemistry\")],\n...     schema=[\"age\", \"name\", \"subject\"])", ">>> df.filter(df.age > 3).show()\n+---+-------+---------+\n|age|   name|  subject|\n+---+-------+---------+\n|  5|    Bob|  Physics|\n|  7|Charlie|Chemistry|\n+---+-------+---------+\n>>> df.where(df.age == 2).show()\n+---+-----+-------+\n|age| name|subject|\n+---+-----+-------+\n|  2|Alice|   Math|\n+---+-----+-------+", ">>> df.filter(\"age > 3\").show()\n+---+-------+---------+\n|age|   name|  subject|\n+---+-------+---------+\n|  5|    Bob|  Physics|\n|  7|Charlie|Chemistry|\n+---+-------+---------+\n>>> df.where(\"age = 2\").show()\n+---+-----+-------+\n|age| name|subject|\n+---+-----+-------+\n|  2|Alice|   Math|\n+---+-----+-------+", ">>> df.filter((df.age > 3) & (df.subject == \"Physics\")).show()\n+---+----+-------+\n|age|name|subject|\n+---+----+-------+\n|  5| Bob|Physics|\n+---+----+-------+\n>>> df.filter((df.age == 2) | (df.subject == \"Chemistry\")).show()\n+---+-------+---------+\n|age|   name|  subject|\n+---+-------+---------+\n|  2|  Alice|     Math|\n|  7|Charlie|Chemistry|\n+---+-------+---------+", ">>> df.filter(\"age > 3 AND name = 'Bob'\").show()\n+---+----+-------+\n|age|name|subject|\n+---+----+-------+\n|  5| Bob|Physics|\n+---+----+-------+", ">>> df.filter(df.name.isin(\"Alice\", \"Bob\")).show()\n+---+-----+-------+\n|age| name|subject|\n+---+-----+-------+\n|  2|Alice|   Math|\n|  5|  Bob|Physics|\n+---+-----+-------+", ">>> df.filter(df.subject.isin([\"Math\", \"Physics\"])).show()\n+---+-----+-------+\n|age| name|subject|\n+---+-----+-------+\n|  2|Alice|   Math|\n|  5|  Bob|Physics|\n+---+-----+-------+", ">>> df.filter(~df.name.isin([\"Alice\", \"Charlie\"])).show()\n+---+----+-------+\n|age|name|subject|\n+---+----+-------+\n|  5| Bob|Physics|\n+---+----+-------+", ">>> df.filter(df.name.isNotNull()).show()\n+---+-------+---------+\n|age|   name|  subject|\n+---+-------+---------+\n|  2|  Alice|     Math|\n|  5|    Bob|  Physics|\n|  7|Charlie|Chemistry|\n+---+-------+---------+", ">>> df.filter(df.name.like(\"Al%\")).show()\n+---+-----+-------+\n|age| name|subject|\n+---+-----+-------+\n|  2|Alice|   Math|\n+---+-----+-------+", ">>> df.filter(df.name.contains(\"i\")).show()\n+---+-------+---------+\n|age|   name|  subject|\n+---+-------+---------+\n|  2|  Alice|     Math|\n|  7|Charlie|Chemistry|\n+---+-------+---------+", ">>> df.filter(df.age.between(2, 5)).show()\n+---+-----+-------+\n|age| name|subject|\n+---+-----+-------+\n|  2|Alice|   Math|\n|  5|  Bob|Physics|\n+---+-----+-------+"], "Parameters": [["condition Column or str", "A Column of types.BooleanType or a string of SQL expressions."]], "Returns": [["DataFrame", "A new DataFrame with rows that satisfy the condition."]], "Category": ["DataFrame"], "index": 15}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html#pyspark.sql.DataFrame.join"], "Title": ["DataFrame.join"], "Feature": ["DataFrame.join"], "Description": "Joins with anotherDataFrame, using the given join expression.", "Examples": [">>> import pyspark.sql.functions as sf\n>>> from pyspark.sql import Row\n>>> df = spark.createDataFrame([Row(name=\"Alice\", age=2), Row(name=\"Bob\", age=5)])\n>>> df2 = spark.createDataFrame([Row(name=\"Tom\", height=80), Row(name=\"Bob\", height=85)])\n>>> df3 = spark.createDataFrame([\n...     Row(name=\"Alice\", age=10, height=80),\n...     Row(name=\"Bob\", age=5, height=None),\n...     Row(name=\"Tom\", age=None, height=None),\n...     Row(name=None, age=None, height=None),\n... ])", ">>> df.join(df2, \"name\").show()\n+----+---+------+\n|name|age|height|\n+----+---+------+\n| Bob|  5|    85|\n+----+---+------+", ">>> df.join(df3, [\"name\", \"age\"]).show()\n+----+---+------+\n|name|age|height|\n+----+---+------+\n| Bob|  5|  NULL|\n+----+---+------+", ">>> joined = df.join(df2, df.name == df2.name, \"outer\").sort(sf.desc(df.name))\n>>> joined.show() \n+-----+----+----+------+\n| name| age|name|height|\n+-----+----+----+------+\n|  Bob|   5| Bob|    85|\n|Alice|   2|NULL|  NULL|\n| NULL|NULL| Tom|    80|\n+-----+----+----+------+", ">>> joined.select(df.name, df2.height).show() \n+-----+------+\n| name|height|\n+-----+------+\n|  Bob|    85|\n|Alice|  NULL|\n| NULL|    80|\n+-----+------+", ">>> df.join(df, df.name == df.name, \"outer\").select(df.name).show() \nTraceback (most recent call last):\n...\npyspark.errors.exceptions.captured.AnalysisException: Column name#0 are ambiguous...", ">>> df.alias(\"a\").join(\n...     df.alias(\"b\"), sf.col(\"a.name\") == sf.col(\"b.name\"), \"outer\"\n... ).sort(sf.desc(\"a.name\")).select(\"a.name\", \"b.age\").show()\n+-----+---+\n| name|age|\n+-----+---+\n|  Bob|  5|\n|Alice|  2|\n+-----+---+", ">>> df.join(df2, \"name\", \"outer\").sort(sf.desc(\"name\")).show()\n+-----+----+------+\n| name| age|height|\n+-----+----+------+\n|  Tom|NULL|    80|\n|  Bob|   5|    85|\n|Alice|   2|  NULL|\n+-----+----+------+", ">>> df.join(df3, [\"name\", \"age\"], \"outer\").sort(\"name\", \"age\").show()\n+-----+----+------+\n| name| age|height|\n+-----+----+------+\n| NULL|NULL|  NULL|\n|Alice|   2|  NULL|\n|Alice|  10|    80|\n|  Bob|   5|  NULL|\n|  Tom|NULL|  NULL|\n+-----+----+------+", ">>> df.join(df2, \"name\", \"left_outer\").show()\n+-----+---+------+\n| name|age|height|\n+-----+---+------+\n|Alice|  2|  NULL|\n|  Bob|  5|    85|\n+-----+---+------+", ">>> df.join(df2, \"name\", \"right_outer\").show()\n+----+----+------+\n|name| age|height|\n+----+----+------+\n| Tom|NULL|    80|\n| Bob|   5|    85|\n+----+----+------+", ">>> df.join(df2, \"name\", \"left_semi\").show()\n+----+---+\n|name|age|\n+----+---+\n| Bob|  5|\n+----+---+", ">>> df.join(df2, \"name\", \"left_anti\").show()\n+-----+---+\n| name|age|\n+-----+---+\n|Alice|  2|\n+-----+---+"], "Parameters": [["other DataFrame", "Right side of the join"], ["on str, list or Column , optional", "a string for the join column name, a list of column names,\na join expression (Column), or a list of Columns.\nIf on is a string or a list of strings indicating the name of the join column(s),\nthe column(s) must exist on both sides, and this performs an equi-join."], ["how str, optional", "default inner . Must be one of: inner , cross , outer , full , fullouter , full_outer , left , leftouter , left_outer , right , rightouter , right_outer , semi , leftsemi , left_semi , anti , leftanti and left_anti ."]], "Returns": [["DataFrame", "Joined DataFrame."]], "Category": ["DataFrame"], "index": 16}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.like.html#pyspark.sql.Column.like"], "Title": ["Column.like"], "Feature": ["Column.like"], "Description": "SQL like expression. Returns a booleanColumnbased on a SQL LIKE match.", "Examples": [">>> df = spark.createDataFrame(\n...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n>>> df.filter(df.name.like('Al%')).collect()\n[Row(age=2, name='Alice')]"], "Parameters": [["other str", "a SQL LIKE pattern"]], "Returns": [["Column", "Column of booleans showing whether each element\nin the Column is matched by SQL LIKE pattern."]], "Category": ["Column"], "index": 17}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.limit.html#pyspark.sql.DataFrame.limit"], "Title": ["DataFrame.limit"], "Feature": ["DataFrame.limit"], "Description": "Limits the result count to the number specified.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.limit(1).show()\n+---+----+\n|age|name|\n+---+----+\n| 14| Tom|\n+---+----+\n>>> df.limit(0).show()\n+---+----+\n|age|name|\n+---+----+\n+---+----+"], "Parameters": [["num int", "Number of records to return. Will return this number of records\nor all records if the DataFrame contains less than this number of records."]], "Returns": [["DataFrame", "Subset of the records"]], "Category": ["DataFrame"], "index": 18}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.offset.html#pyspark.sql.DataFrame.offset"], "Title": ["DataFrame.offset"], "Feature": ["DataFrame.offset"], "Description": "Returns a new :class:DataFrameby skipping the firstnrows.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n>>> df.offset(1).show()\n+---+-----+\n|age| name|\n+---+-----+\n| 23|Alice|\n| 16|  Bob|\n+---+-----+\n>>> df.offset(10).show()\n+---+----+\n|age|name|\n+---+----+\n+---+----+"], "Parameters": [["num int", "Number of records to skip."]], "Returns": [["DataFrame", "Subset of the records"]], "Category": ["DataFrame"], "index": 19}
{"HTML": ["https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.distinct.html#pyspark.sql.DataFrame.distinct"], "Title": ["DataFrame.distinct"], "Feature": ["DataFrame.distinct"], "Description": "Returns a newDataFramecontaining the distinct rows in thisDataFrame.\nSee alsoDataFrame.dropDuplicatesRemove duplicate rows from this DataFrame.", "Examples": [">>> df = spark.createDataFrame(\n...     [(14, \"Tom\"), (23, \"Alice\"), (23, \"Alice\")], [\"age\", \"name\"])\n>>> df.distinct().show()\n+---+-----+\n|age| name|\n+---+-----+\n| 14|  Tom|\n| 23|Alice|\n+---+-----+", ">>> df.distinct().count()\n2", ">>> df = spark.createDataFrame(\n...     [(14, \"Tom\", \"M\"), (23, \"Alice\", \"F\"), (23, \"Alice\", \"F\"), (14, \"Tom\", \"M\")],\n...     [\"age\", \"name\", \"gender\"])\n>>> df.distinct().show()\n+---+-----+------+\n|age| name|gender|\n+---+-----+------+\n| 14|  Tom|     M|\n| 23|Alice|     F|\n+---+-----+------+", ">>> df.select(\"name\").distinct().show()\n+-----+\n| name|\n+-----+\n|  Tom|\n|Alice|\n+-----+", ">>> df.select(\"name\").distinct().count()\n2", ">>> df.select(\"name\", \"gender\").distinct().show()\n+-----+------+\n| name|gender|\n+-----+------+\n|  Tom|     M|\n|Alice|     F|\n+-----+------+", ">>> df = spark.createDataFrame(\n...     [(14, \"Tom\", \"M\"), (23, \"Alice\", \"F\"), (23, \"Alice\", \"F\"), (14, \"Tom\", None)],\n...     [\"age\", \"name\", \"gender\"])\n>>> df.distinct().show()\n+---+-----+------+\n|age| name|gender|\n+---+-----+------+\n| 14|  Tom|     M|\n| 23|Alice|     F|\n| 14|  Tom|  NULL|\n+---+-----+------+", ">>> df.distinct().filter(df.gender.isNotNull()).show()\n+---+-----+------+\n|age| name|gender|\n+---+-----+------+\n| 14|  Tom|     M|\n| 23|Alice|     F|\n+---+-----+------+"], "Parameters": [], "Returns": [["DataFrame", "DataFrame with distinct records."]], "Category": ["DataFrame"], "index": 20}