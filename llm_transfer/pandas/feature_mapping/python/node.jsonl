{"HTML": ["https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html#join"], "Title": ["JOIN"], "Feature": ["JOIN"], "Description": "JOIN\nJOINs can be performed with join() or merge(). By default, join() will join the DataFrames on their indices. Each method has parameters allowing you to specify the type of join to perform (LEFT, RIGHT, INNER, FULL) or the columns to join on (column names or indices).\nIn [24]: df1 = pd.DataFrame({\"key\": [\"A\", \"B\", \"C\", \"D\"], \"value\": np.random.randn(4)})\nIn [25]: df2 = pd.DataFrame({\"key\": [\"B\", \"D\", \"D\", \"E\"], \"value\": np.random.randn(4)})\nAssume we have two database tables of the same name and structure as our DataFrames.\nNow let’s go over the various types of JOINs.\nINNER JOIN\nSELECT *\nFROM df1\nINNER JOIN df2\n  ON df1.key = df2.key;\n# merge performs an INNER JOIN by default\nIn [26]: pd.merge(df1, df2, on=\"key\")\nOut[26]: \n  key   value_x   value_y\n0   B -0.282863  1.212112\n1   D -1.135632 -0.173215\n2   D -1.135632  0.119209\nmerge() also offers parameters for cases when you’d like to join one DataFrame’s column with another DataFrame’s index.\nIn [27]: indexed_df2 = df2.set_index(\"key\")\nIn [28]: pd.merge(df1, indexed_df2, left_on=\"key\", right_index=True)\nOut[28]: \n  key   value_x   value_y\n1   B -0.282863  1.212112\n3   D -1.135632 -0.173215\n3   D -1.135632  0.119209\nLEFT OUTER JOIN\nShow all records from df1.\nSELECT *\nFROM df1\nLEFT OUTER JOIN df2\n  ON df1.key = df2.key;\nIn [29]: pd.merge(df1, df2, on=\"key\", how=\"left\")\nOut[29]: \n  key   value_x   value_y\n0   A  0.469112       NaN\n1   B -0.282863  1.212112\n2   C -1.509059       NaN\n3   D -1.135632 -0.173215\n4   D -1.135632  0.119209\nRIGHT JOIN\nShow all records from df2.\nSELECT *\nFROM df1\nRIGHT OUTER JOIN df2\n  ON df1.key = df2.key;\nIn [30]: pd.merge(df1, df2, on=\"key\", how=\"right\")\nOut[30]: \n  key   value_x   value_y\n0   B -0.282863  1.212112\n1   D -1.135632 -0.173215\n2   D -1.135632  0.119209\n3   E       NaN -1.044236\nFULL JOIN\npandas also allows for FULL JOINs, which display both sides of the dataset, whether or not the joined columns find a match. As of writing, FULL JOINs are not supported in all RDBMS (MySQL).\nShow all records from both tables.\nSELECT *\nFROM df1\nFULL OUTER JOIN df2\n  ON df1.key = df2.key;\nIn [31]: pd.merge(df1, df2, on=\"key\", how=\"outer\")\nOut[31]: \n  key   value_x   value_y\n0   A  0.469112       NaN\n1   B -0.282863  1.212112\n2   C -1.509059       NaN\n3   D -1.135632 -0.173215\n4   D -1.135632  0.119209\n5   E       NaN -1.044236\npandas.DataFrame.merge\nMerge DataFrame or named Series objects with a database-style join.\nA named Series object is treated as a DataFrame with a single named column.\nThe join is done on columns or indexes. If joining columns on\ncolumns, the DataFrame indexes will be ignored. Otherwise if joining indexes\non indexes or indexes on a column or columns, the index will be passed on.\nWhen performing a cross merge, no column specifications to merge on are\nallowed.\nWarningIf both key columns contain rows where the key is a null value, those\nrows will be matched against each other. This is different from usual SQL\njoin behaviour and can lead to unexpected results.\nSee also merge_orderedMerge with optional filling/interpolation.merge_as of Merge on nearest keys.DataFrame.joinSimilar method using indices.", "Examples": [">>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> df1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> df2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8"], "Parameters": [["right DataFrame or named Series", "Object to merge with."], ["how {‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’", "Type of merge to be performed. left: use only keys from left frame, similar to a SQL left outer join;\npreserve key order. right: use only keys from right frame, similar to a SQL right outer join;\npreserve key order. outer: use union of keys from both frames, similar to a SQL full outer\njoin; sort keys lexicographically. inner: use intersection of keys from both frames, similar to a SQL inner\njoin; preserve the order of the left keys. cross: creates the cartesian product from both frames, preserves the order\nof the left keys."], ["on label or list", "Column or index level names to join on. These must be found in both\nDataFrames. If on is None and not merging on indexes then this defaults\nto the intersection of the columns in both DataFrames."], ["left_on label or list, or array-like", "Column or index level names to join on in the left DataFrame. Can also\nbe an array or list of arrays of the length of the left DataFrame.\nThese arrays are treated as if they are columns."], ["right_on label or list, or array-like", "Column or index level names to join on in the right DataFrame. Can also\nbe an array or list of arrays of the length of the right DataFrame.\nThese arrays are treated as if they are columns."], ["left_index bool, default False", "Use the index from the left DataFrame as the join key(s). If it is a\nMultiIndex, the number of keys in the other DataFrame (either the index\nor a number of columns) must match the number of levels."], ["right_index bool, default False", "Use the index from the right DataFrame as the join key. Same caveats as\nleft_index."], ["sort bool, default False", "Sort the join keys lexicographically in the result DataFrame. If False,\nthe order of the join keys depends on the join type (how keyword)."], ["suffixes list-like, default is (“_x”, “_y”)", "A length-2 sequence where each element is optionally a string\nindicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead\nof a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the\nvalues must not be None."], ["copy bool, default True", "If False, avoid copy if possible. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and\nignore the copy keyword. The copy keyword will be removed in a\nfuture version of pandas. You can already get the future behavior and improvements through\nenabling copy on write pd.options.mode.copy_on_write = True"], ["indicator bool or str, default False", "If True, adds a column to the output DataFrame called “_merge” with\ninformation on the source of each row. The column can be given a different\nname by providing a string argument. The column will have a Categorical\ntype with the value of “left_only” for observations whose merge key only\nappears in the left DataFrame, “right_only” for observations\nwhose merge key only appears in the right DataFrame, and “both”\nif the observation’s merge key is found in both DataFrames."], ["validate str, optional", "If specified, checks if merge is of specified type. “one_to_one” or “1:1”: check if merge keys are unique in both\nleft and right datasets. “one_to_many” or “1:m”: check if merge keys are unique in left\ndataset. “many_to_one” or “m:1”: check if merge keys are unique in right\ndataset. “many_to_many” or “m:m”: allowed, but does not result in checks."]], "Returns": [["DataFrame", "A DataFrame of the two merged objects."]], "Category": ["Dataframe"], "index": 0}
{"HTML": ["https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html#where"], "Title": ["WHERE"], "Feature": ["WHERE"], "Description": "WHERE\nFiltering in SQL is done via a WHERE clause.\nSELECT *\nFROM tips\nWHERE time = 'Dinner';\nDataFrames can be filtered in multiple ways; the most intuitive of which is using boolean indexing.\nIn [8]: tips[tips[\"total_bill\"] > 10]\nOut[8]: \n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n[227 rows x 7 columns]\nThe above statement is simply passing a Series of True/False objects to the DataFrame, returning all rows with True.\nIn [9]: is_dinner = tips[\"time\"] == \"Dinner\"\nIn [10]: is_dinner\nOut[10]: \n0      True\n1      True\n2      True\n3      True\n4      True\n       ... \n239    True\n240    True\n241    True\n242    True\n243    True\nName: time, Length: 244, dtype: bool\nIn [11]: is_dinner.value_counts()\nOut[11]: \ntime\nTrue     176\nFalse     68\nName: count, dtype: int64\nIn [12]: tips[is_dinner]\nOut[12]: \n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n[176 rows x 7 columns]\nJust like SQL’s OR and AND, multiple conditions can be passed to a DataFrame using | (OR) and & (AND).\nTips of more than $5 at Dinner meal\nSELECT *\nFROM tips\nWHERE time = 'Dinner' AND tip > 5.00;\nIn [13]: tips[(tips[\"time\"] == \"Dinner\") & (tips[\"tip\"] > 5.00)]\nOut[13]: \n     total_bill    tip     sex smoker  day    time  size\n23        39.42   7.58    Male     No  Sat  Dinner     4\n44        30.40   5.60    Male     No  Sun  Dinner     4\n47        32.40   6.00    Male     No  Sun  Dinner     4\n52        34.81   5.20  Female     No  Sun  Dinner     4\n59        48.27   6.73    Male     No  Sat  Dinner     4\n116       29.93   5.07    Male     No  Sun  Dinner     4\n155       29.85   5.14  Female     No  Sun  Dinner     5\n170       50.81  10.00    Male    Yes  Sat  Dinner     3\n172        7.25   5.15    Male    Yes  Sun  Dinner     2\n181       23.33   5.65    Male    Yes  Sun  Dinner     2\n183       23.17   6.50    Male    Yes  Sun  Dinner     4\n211       25.89   5.16    Male    Yes  Sat  Dinner     4\n212       48.33   9.00    Male     No  Sat  Dinner     4\n214       28.17   6.50  Female    Yes  Sat  Dinner     3\n239       29.03   5.92    Male     No  Sat  Dinner     3\nTips by parties of at least 5 diners OR bill total was more than $45:\nSELECT *\nFROM tips\nWHERE size >= 5 OR total_bill > 45;\nIn [14]: tips[(tips[\"size\"] >= 5) | (tips[\"total_bill\"] > 45)]\nOut[14]: \n     total_bill    tip     sex smoker   day    time  size\n59        48.27   6.73    Male     No   Sat  Dinner     4\n125       29.80   4.20  Female     No  Thur   Lunch     6\n141       34.30   6.70    Male     No  Thur   Lunch     6\n142       41.19   5.00    Male     No  Thur   Lunch     5\n143       27.05   5.00  Female     No  Thur   Lunch     6\n155       29.85   5.14  Female     No   Sun  Dinner     5\n156       48.17   5.00    Male     No   Sun  Dinner     6\n170       50.81  10.00    Male    Yes   Sat  Dinner     3\n182       45.35   3.50    Male    Yes   Sun  Dinner     3\n185       20.69   5.00    Male     No   Sun  Dinner     5\n187       30.46   2.00    Male    Yes   Sun  Dinner     5\n212       48.33   9.00    Male     No   Sat  Dinner     4\n216       28.15   3.00    Male    Yes   Sat  Dinner     5\nNULL checking is done using the notna() and isna() methods.\nIn [15]: frame = pd.DataFrame(\n   ....:     {\"col1\": [\"A\", \"B\", np.nan, \"C\", \"D\"], \"col2\": [\"F\", np.nan, \"G\", \"H\", \"I\"]}\n   ....: )\n   ....: \nIn [16]: frame\nOut[16]: \n  col1 col2\n0    A    F\n1    B  NaN\n2  NaN    G\n3    C    H\n4    D    I\nAssume we have a table of the same structure as our DataFrame above. We can see only the records where col2 IS NULL with the following query:\nSELECT *\nFROM frame\nWHERE col2 IS NULL;\nIn [17]: frame[frame[\"col2\"].isna()]\nOut[17]: \n  col1 col2\n1    B  NaN\nGetting items where col1 IS NOT NULL can be done with notna().\nSELECT *\nFROM frame\nWHERE col1 IS NOT NULL;\nIn [18]: frame[frame[\"col1\"].notna()]\nOut[18]: \n  col1 col2\n0    A    F\n1    B  NaN\n3    C    H\n4    D    I\npandas.DataFrame.query\nQuery the columns of a DataFrame with a boolean expression.\nNotes\nThe result of the evaluation of this expression is first passed toDataFrame.loc and if that fails because of a multidimensional key (e.g., a DataFrame) then the result will be passed to DataFrame.__getitem__().\nThis method uses the top-leve leval()function to evaluate the passed query.\nThe query() method uses a slightly modified Python syntax by default. For example, the&and|(bitwise) operators have the precedence of their boolean cousins,and and or. This is syntactically valid Python,however the semantics are different.\nYou can change the semantics of the expression by passing the keyword argumentparser='python'. This enforces the same semantics as evaluation in Python space. Likewise, you can pass engine='python'to evaluate an expression using Python itself as a backend. This is not recommended as it is inefficient compared to using num expras the engine.\nThe DataFrame.index and DataFrame.column sat tributes of the DataFrame instance are placed in the query namespace by default, which allows you to treat both the index and columns of the frame as a column in the frame.\nThe identifier index is used for the frame index; you can also use the name of the index to identify it in a query. Please note that\nPython keywords may not be used as identifiers.\nFor further details and examples see the query documentation in indexing.\nBacktick quoted variables\nBacktick quoted variables are parsed as literal Python code and are converted internally to a Python valid identifier.\nThis can lead to the following problems.\nDuring parsing a number of disallowed characters inside the backtick quoted string are replaced by strings that are allowed as a Python identifier.\nThese characters include all operators in Python, the space character, the question mark, the exclamation mark, the dollar sign, and the euro sign.\nFor other characters that fall outside the ASCII range (U+0001..U+007F) and those that are not further specified in PEP 3131,the query parser will raise an error.\nThis excludes whitespace different than the space character,but also the hashtag (as it is used for comments) and the backtick itself (backtick can also not be escaped).\nIn a special case, quotes that make a pair around a backtick can confuse the parser.\nFor example,`it's`>`that's`will raise an error,as it forms a quoted string ('s>`that') with a backtick inside.", "Examples": [">>> df = pd.DataFrame({'A': range(1, 6),\n...                    'B': range(10, 0, -2),\n...                    'C C': range(10, 5, -1)})\n>>> df\n   A   B  C C\n0  1  10   10\n1  2   8    9\n2  3   6    8\n3  4   4    7\n4  5   2    6\n>>> df.query('A > B')\n   A  B  C C\n4  5  2    6"], "Parameters": [["expr str", "The query string to evaluate. You can refer to variables\nin the environment by prefixing them with an ‘@’ character like @a + b . You can refer to column names that are not valid Python variable names\nby surrounding them in backticks. Thus, column names containing spaces\nor punctuations (besides underscores) or starting with digits must be\nsurrounded by backticks. (For example, a column named “Area (cm^2)” would\nbe referenced as `Area (cm^2)` ). Column names which are Python keywords\n(like “list”, “for”, “import”, etc) cannot be used. For example, if one of your columns is called a a and you want\nto sum it with b , your query should be `a a` + b ."], ["inplace bool", "Whether to modify the DataFrame rather than creating a new one."], ["**kwargs", "See the documentation for eval() for complete details\non the keyword arguments accepted by DataFrame.query() ."]], "Returns": [["DataFrame or None", "DataFrame resulting from the provided query expression or\nNone if inplace=True ."]], "Category": ["Dataframe"], "index": 1}
{"HTML": ["https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html#group-by"], "Title": ["GROUP BY"], "Feature": ["GROUP BY"], "Description": "GROUP BY\nIn pandas, SQL’s GROUP BY operations are performed using the similarly named groupby() method. groupby() typically refers to a process where we’d like to split a dataset into groups, apply some function (typically aggregation) , and then combine the groups together.\nA common SQL operation would be getting the count of records in each group throughout a dataset. For instance, a query getting us the number of tips left by sex:\nSELECT sex, count(*)\nFROM tips\nGROUP BY sex;\n/*\nFemale     87\nMale      157\n*/\nThe pandas equivalent would be:\nIn [19]: tips.groupby(\"sex\").size()\nOut[19]: \nsex\nFemale     87\nMale      157\ndtype: int64\nNotice that in the pandas code we used DataFrameGroupBy.size() and not DataFrameGroupBy.count(). This is because DataFrameGroupBy.count() applies the function to each column, returning the number of NOT NULL records within each.\nIn [20]: tips.groupby(\"sex\").count()\nOut[20]: \n        total_bill  tip  smoker  day  time  size\nsex                                             \nFemale          87   87      87   87    87    87\nMale           157  157     157  157   157   157\nAlternatively, we could have applied the DataFrameGroupBy.count() method to an individual column:\nIn [21]: tips.groupby(\"sex\")[\"total_bill\"].count()\nOut[21]: \nsex\nFemale     87\nMale      157\nName: total_bill, dtype: int64\nMultiple functions can also be applied at once. For instance, say we’d like to see how tip amount differs by day of the week - DataFrameGroupBy.agg() allows you to pass a dictionary to your grouped DataFrame, indicating which functions to apply to specific columns.\nSELECT day, AVG(tip), COUNT(*)\nFROM tips\nGROUP BY day;\n/*\nFri   2.734737   19\nSat   2.993103   87\nSun   3.255132   76\nThu  2.771452   62\n*/\nIn [22]: tips.groupby(\"day\").agg({\"tip\": \"mean\", \"day\": \"size\"})\nOut[22]: \n           tip  day\nday                \nFri   2.734737   19\nSat   2.993103   87\nSun   3.255132   76\nThur  2.771452   62\nGrouping by more than one column is done by passing a list of columns to the groupby() method.\nSELECT smoker, day, COUNT(*), AVG(tip)\nFROM tips\nGROUP BY smoker, day;\n/*\nsmoker day\nNo     Fri      4  2.812500\n       Sat     45  3.102889\n       Sun     57  3.167895\n       Thu    45  2.673778\nYes    Fri     15  2.714000\n       Sat     42  2.875476\n       Sun     19  3.516842\n       Thu    17  3.030000\n*/\nIn [23]: tips.groupby([\"smoker\", \"day\"]).agg({\"tip\": [\"size\", \"mean\"]})\nOut[23]: \n             tip          \n            size      mean\nsmoker day                \nNo     Fri     4  2.812500\n       Sat    45  3.102889\n       Sun    57  3.167895\n       Thur   45  2.673778\nYes    Fri    15  2.714000\n       Sat    42  2.875476\n       Sun    19  3.516842\n       Thur   17  3.030000\npandas.DataFrame.groupby\nGroup DataFrame using a mapper or by a Series of columns.\nA groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups.\nNotes\nSee the user guide for more detailed usage and examples, including splitting an object into groups,iterating through groups, selecting a group, aggregation, and more.", "Examples": [">>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> df\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0\n>>> df.groupby(['Animal']).mean()\n        Max Speed\nAnimal\nFalcon      375.0\nParrot       25.0"], "Parameters": [["by mapping, function, label, pd.Grouper or list of such", "Used to determine the groups for the groupby.\nIf by is a function, it’s called on each value of the object’s\nindex. If a dict or Series is passed, the Series or dict VALUES\nwill be used to determine the groups (the Series’ values are first\naligned; see .align() method). If a list or ndarray of length\nequal to the selected axis is passed (see the groupby user guide ),\nthe values are used as-is to determine the groups. A label or list\nof labels may be passed to group by the columns in self .\nNotice that a tuple is interpreted as a (single) key."], ["axis {0 or ‘index’, 1 or ‘columns’}, default 0", "Split along rows (0) or columns (1). For Series this parameter\nis unused and defaults to 0. Deprecated since version 2.1.0: Will be removed and behave like axis=0 in a future version.\nFor axis=1 , do frame.T.groupby(...) instead."], ["level int, level name, or sequence of such, default None", "If the axis is a MultiIndex (hierarchical), group by a particular\nlevel or levels. Do not specify both by and level ."], ["as_index bool, default True", "Return object with group labels as the\nindex. Only relevant for DataFrame input. as_index=False is\neffectively “SQL-style” grouped output. This argument has no effect\non filtrations (see the filtrations in the user guide ),\nsuch as head() , tail() , nth() and in transformations\n(see the transformations in the user guide )."], ["sort bool, default True", "Sort group keys. Get better performance by turning this off.\nNote this does not influence the order of observations within each\ngroup. Groupby preserves the order of rows within each group. If False,\nthe groups will appear in the same order as they did in the original DataFrame.\nThis argument has no effect on filtrations (see the filtrations in the user guide ),\nsuch as head() , tail() , nth() and in transformations\n(see the transformations in the user guide ). Changed in version 2.0.0: Specifying sort=False with an ordered categorical grouper will no\nlonger sort the values."], ["group_keys bool, default True", "When calling apply and the by argument produces a like-indexed\n(i.e. a transform ) result, add group keys to\nindex to identify pieces. By default group keys are not included\nwhen the result’s index (and column) labels match the inputs, and\nare included otherwise. Changed in version 1.5.0: Warns that group_keys will no longer be ignored when the\nresult from apply is a like-indexed Series or DataFrame.\nSpecify group_keys explicitly to include the group keys or\nnot. Changed in version 2.0.0: group_keys now defaults to True ."], ["observed bool, default False", "This only applies if any of the groupers are Categoricals.\nIf True: only show observed values for categorical groupers.\nIf False: show all values for categorical groupers. Deprecated since version 2.1.0: The default value will change to True in a future version of pandas."], ["dropna bool, default True", "If True, and if group keys contain NA values, NA values together\nwith row/column will be dropped.\nIf False, NA values will also be treated as the key in groups."]], "Returns": [["pandas.api.typing.DataFrameGroupBy", "Returns a groupby object that contains information about the groups."]], "Category": ["Dataframe"], "index": 2}
{"HTML": ["https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html#pandas.DataFrame.sort_values"], "Title": ["DataFrame.sort_values"], "Feature": ["DataFrame.sort_values"], "Description": "Sort by the values along either axis.", "Examples": [">>> df = pd.DataFrame({\n...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n...     'col2': [2, 1, 9, 8, 7, 4],\n...     'col3': [0, 1, 9, 4, 2, 3],\n...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n... })\n>>> df\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F"], "Parameters": [["by str or list of str", "Name or list of names to sort by. if axis is 0 or ‘index’ then by may contain index\nlevels and/or column labels. if axis is 1 or ‘columns’ then by may contain column\nlevels and/or index labels."], ["axis “{0 or ‘index’, 1 or ‘columns’}”, default 0", "Axis to be sorted."], ["ascending bool or list of bool, default True", "Sort ascending vs. descending. Specify list for multiple sort\norders.  If this is a list of bools, must match the length of\nthe by."], ["inplace bool, default False", "If True, perform operation in-place."], ["kind {‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, default ‘quicksort’", "Choice of sorting algorithm. See also numpy.sort() for more\ninformation. mergesort and stable are the only stable algorithms. For\nDataFrames, this option is only applied when sorting on a single\ncolumn or label."], ["na_position {‘first’, ‘last’}, default ‘last’", "Puts NaNs at the beginning if first ; last puts NaNs at the\nend."], ["ignore_index bool, default False", "If True, the resulting axis will be labeled 0, 1, …, n - 1."], ["key callable, optional", "Apply the key function to the values\nbefore sorting. This is similar to the key argument in the\nbuiltin sorted() function, with the notable difference that\nthis key function should be vectorized . It should expect a Series and return a Series with the same shape as the input.\nIt will be applied to each column in by independently."]], "Returns": [["DataFrame or None", "DataFrame with sorted values or None if inplace=True ."]], "Category": ["Dataframe"], "index": 3}
{"HTML": ["https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html#limit"], "Title": ["LIMIT"], "Feature": ["LIMIT"], "Description": "LIMIT\nSELECT * FROM tips\nLIMIT 10;\nIn [36]: tips.head(10)\nOut[36]: \n   total_bill   tip     sex smoker  day    time  size\n0       16.99  1.01  Female     No  Sun  Dinner     2\n1       10.34  1.66    Male     No  Sun  Dinner     3\n2       21.01  3.50    Male     No  Sun  Dinner     3\n3       23.68  3.31    Male     No  Sun  Dinner     2\n4       24.59  3.61  Female     No  Sun  Dinner     4\n5       25.29  4.71    Male     No  Sun  Dinner     4\n6        8.77  2.00    Male     No  Sun  Dinner     2\n7       26.88  3.12    Male     No  Sun  Dinner     4\n8       15.04  1.96    Male     No  Sun  Dinner     2\n9       14.78  3.23    Male     No  Sun  Dinner     2\npandas.DataFrame.head\nReturn the first n rows.\nThis function returns the first n rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it.\nFor negative values ofn, this function returns all rows except the last|n|rows, equivalent todf[:n].\nIf n is larger than the number of rows, this function returns all rows.", "Examples": [">>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n>>> df\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\n5     parrot\n6      shark\n7      whale\n8      zebra"], "Parameters": [["n int, default 5", "Number of rows to select."]], "Returns": [["same type as caller", "The first n rows of the caller object."]], "Category": ["Dataframe"], "index": 4}
{"HTML": ["https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html#pandas.DataFrame.sample"], "Title": ["DataFrame.sample"], "Feature": ["DataFrame.sample"], "Description": "Return a random sample of items from an axis of object.\nYou can userandom_statefor reproducibility.\nNotes\nIf frac> 1,replacement should be set toTrue.", "Examples": [">>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n...                    'num_wings': [2, 0, 0, 0],\n...                    'num_specimen_seen': [10, 2, 1, 8]},\n...                   index=['falcon', 'dog', 'spider', 'fish'])\n>>> df\n        num_legs  num_wings  num_specimen_seen\nfalcon         2          2                 10\ndog            4          0                  2\nspider         8          0                  1\nfish           0          0                  8"], "Parameters": [["n int, optional", "Number of items from axis to return. Cannot be used with frac .\nDefault = 1 if frac = None."], ["frac float, optional", "Fraction of axis items to return. Cannot be used with n ."], ["replace bool, default False", "Allow or disallow sampling of the same row more than once."], ["weights str or ndarray-like, optional", "Default ‘None’ results in equal probability weighting.\nIf passed a Series, will align with target object on index. Index\nvalues in weights not found in sampled object will be ignored and\nindex values in sampled object not in weights will be assigned\nweights of zero.\nIf called on a DataFrame, will accept the name of a column\nwhen axis = 0.\nUnless weights are a Series, weights must be same length as axis\nbeing sampled.\nIf weights do not sum to 1, they will be normalized to sum to 1.\nMissing values in the weights column will be treated as zero.\nInfinite values not allowed."], ["random_state int, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optional", "If int, array-like, or BitGenerator, seed for random number generator.\nIf np.random.RandomState or np.random.Generator, use as given. Changed in version 1.4.0: np.random.Generator objects now accepted"], ["axis {0 or ‘index’, 1 or ‘columns’, None}, default None", "Axis to sample. Accepts axis number or name. Default is stat axis\nfor given data type. For Series this parameter is unused and defaults to None ."], ["ignore_index bool, default False", "If True, the resulting index will be labeled 0, 1, …, n - 1."]], "Returns": [["Series or DataFrame", "A new object of same type as caller containing n items randomly\nsampled from the caller object."]], "Category": ["Dataframe"], "index": 5}
{"HTML": ["https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html#select"], "Title": ["SELECT"], "Feature": ["SELECT"], "Description": "In SQL, selection is done using a comma-separated list of columns you’d like to select (or a * to select all columns):\nSELECT total_bill, tip, smoker, time\nFROM tips;\nWith pandas, column selection is done by passing a list of column names to your DataFrame:\nIn [6]: tips[[\"total_bill\", \"tip\", \"smoker\", \"time\"]]\nOut[6]: \n     total_bill   tip smoker    time\n0         16.99  1.01     No  Dinner\n1         10.34  1.66     No  Dinner\n2         21.01  3.50     No  Dinner\n3         23.68  3.31     No  Dinner\n4         24.59  3.61     No  Dinner\n..          ...   ...    ...     ...\n239       29.03  5.92     No  Dinner\n240       27.18  2.00    Yes  Dinner\n241       22.67  2.00    Yes  Dinner\n242       17.82  1.75     No  Dinner\n243       18.78  3.00     No  Dinner\n[244 rows x 4 columns]\nCalling the DataFrame without the list of column names would display all columns (akin to SQL’s *).\nIn SQL, you can add a calculated column:\nSELECT *, tip/total_bill as tip_rate\nFROM tips;\nWith pandas, you can use the DataFrame.assign() method of a DataFrame to append a new column:\nIn [7]: tips.assign(tip_rate=tips[\"tip\"] / tips[\"total_bill\"])\nOut[7]: \n     total_bill   tip     sex smoker   day    time  size  tip_rate\n0         16.99  1.01  Female     No   Sun  Dinner     2  0.059447\n1         10.34  1.66    Male     No   Sun  Dinner     3  0.160542\n2         21.01  3.50    Male     No   Sun  Dinner     3  0.166587\n3         23.68  3.31    Male     No   Sun  Dinner     2  0.139780\n4         24.59  3.61  Female     No   Sun  Dinner     4  0.146808\n..          ...   ...     ...    ...   ...     ...   ...       ...\n239       29.03  5.92    Male     No   Sat  Dinner     3  0.203927\n240       27.18  2.00  Female    Yes   Sat  Dinner     2  0.073584\n241       22.67  2.00    Male    Yes   Sat  Dinner     2  0.088222\n242       17.82  1.75    Male     No   Sat  Dinner     2  0.098204\n243       18.78  3.00  Female     No  Thur  Dinner     2  0.159744\n[244 rows x 8 columns]", "Examples":[],"Category": ["Dataframe"], "index": 6}
{"HTML": ["https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html#pandas.DataFrame.iloc"], "Title": ["DataFrame.iloc"], "Feature": ["DataFrame.iloc"], "Description": "Purely integer-location based indexing for selection by position.\nDeprecated since version 2.2.0:Returning a tuple from a callable is deprecated.\n.iloc[]is primarily integer position based (from0tolength-1of the axis), but may also be used with a boolean\narray.\nAllowed inputs are:\nAn integer, e.g.5.A list or array of integers, e.g.[4,3,0].A slice object with ints, e.g.1:7.A boolean array.Acallablefunction with one argument (the calling Series or\nDataFrame) and that returns valid output for indexing (one of the above).\nThis is useful in method chains, when you don’t have a reference to the\ncalling object, but would like to base your selection on\nsome value.A tuple of row and column indexes. The tuple elements consist of one of the\nabove inputs, e.g.(0,1).\n.ilocwill raiseIndexErrorif a requested indexer is\nout-of-bounds, exceptsliceindexers which allow out-of-bounds\nindexing (this conforms with python/numpyslicesemantics).\nSee more atSelection by Position.\nSee alsoDataFrame.iatFast integer location scalar accessor.DataFrame.locPurely label-location based indexer for selection by label.Series.ilocPurely integer-location based indexing for selection by position.", "Examples": [">>> mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000}]\n>>> df = pd.DataFrame(mydict)\n>>> df\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000"], "Parameters": [], "Returns": [], "Category": ["Dataframe"], "index": 7}
{"HTML": ["https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nunique.html"], "Title": ["DISTINCT"], "Feature": ["DISTINCT"], "Description": "pandas.DataFrame.nunique\nCount number of distinct elements in specified axis.\nReturn Series with number of distinct elements. Can ignore NaN values.Parameters\n:\naxis\n{0 or ‘index’, 1 or ‘columns’}, default 0\nThe axis to use. 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise.\ndropna\nbool, default True\nDon’t include NaN in the counts.\nReturns\n:\nSeries\nExamples\n>>> df = pd.DataFrame({'A': [4, 5, 6], 'B': [4, 1, 1]})\n>>> df.nunique()\nA    3\nB    2\ndtype: int64\n>>> df.nunique(axis=1)\n0    1\n1    2\n2    2\ndtype: int64\npandas.DataFrame.drop_duplicates\nReturn DataFrame with duplicate rows removed.\nConsidering certain columns is optional. Indexes, including time indexes are ignored.\nParameters:\nsubset : column label or sequence of labels, optional\nOnly consider certain columns for identifying duplicates, by default use all of the columns.\nkeep : {‘first’, ‘last’, False}, default ‘first’\nDetermines which duplicates (if any) to keep.\n‘first’ : Drop duplicates except for the first occurrence.\n‘last’ : Drop duplicates except for the last occurrence.\nFalse : Drop all duplicates.\ninplace : bool, default False\nWhether to modify the DataFrame rather than creating a new one.\nignore_index : bool, default False\nIf True, the resulting axis will be labeled 0, 1, …, n - 1.\nReturns:\nDataFrame or None\nDataFrame with duplicates removed or None if inplace=True.\nExamples\nConsider dataset containing ramen rating.\n>>> df = pd.DataFrame({\n...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n...     'rating': [4, 4, 3.5, 15, 5]\n... })\n>>> df\n    brand style  rating\n0  Yum Yum   cup     4.0\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\nBy default, it removes duplicate rows based on all columns.\n>>> df.drop_duplicates()\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\nTo remove duplicates on specific column(s), use subset.\n>>> df.drop_duplicates(subset=['brand'])\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\nTo remove duplicates and keep last occurrences, use keep.\n>>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n    brand style  rating\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n4  Indomie  pack     5.0", "Examples": [], "Parameters": [], "Returns": [], "Category": ["Dataframe"], "index": 8}
{"HTML": ["https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.filter.html#pandas.core.groupby.DataFrameGroupBy.filter"], "Title": ["DataFrameGroupBy.filter"], "Feature": ["DataFrameGroupBy.filter"], "Description": "Filter elements from groups that don’t satisfy a criterion.\nElements from groups are filtered if they do not satisfy the boolean criterion specified by func.", "Examples": [">>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n...                           'foo', 'bar'],\n...                    'B' : [1, 2, 3, 4, 5, 6],\n...                    'C' : [2.0, 5., 8., 1., 2., 9.]})\n>>> grouped = df.groupby('A')\n>>> grouped.filter(lambda x: x['B'].mean() > 3.)\n     A  B    C\n1  bar  2  5.0\n3  bar  4  1.0\n5  bar  6  9.0"], ",Parameters": ["func:function\nCriterion to apply to each group. Should return True or False.\ndropna:bool\nDrop groups that do not pass the filter. True by default; if False, groups that evaluate False are filled with NaNs."], ",Returns": ["DataFrame"], ",Category": ["Dataframe"], "index": 9}
{"HTML": ["https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html#union"], "Title": ["UNION"], "Feature": ["UNION"], "Description": "UNION ALL can be performed using concat().\nIn [32]: df1 = pd.DataFrame(\n   ....:     {\"city\": [\"Chicago\", \"San Francisco\", \"New York City\"], \"rank\": range(1, 4)}\n   ....: )\n   ....: \nIn [33]: df2 = pd.DataFrame(\n   ....:     {\"city\": [\"Chicago\", \"Boston\", \"Los Angeles\"], \"rank\": [1, 4, 5]}\n   ....: )\n   ....: ", "Examples": [">>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n...                           'foo', 'bar'],\n...                    'B' : [1, 2, 3, 4, 5, 6],\n...                    'C' : [2.0, 5., 8., 1., 2., 9.]})\n>>> grouped = df.groupby('A')\n>>> grouped.filter(lambda x: x['B'].mean() > 3.)\n     A  B    C\n1  bar  2  5.0\n3  bar  4  1.0\n5  bar  6  9.0\nSELECT city, rank\nFROM df1\nUNION ALL\nSELECT city, rank\nFROM df2;\n/*\n         city  rank\n      Chicago     1\nSan Francisco     2\nNew York City     3\n      Chicago     1\n       Boston     4\n  Los Angeles     5\n*/\nIn [34]: pd.concat([df1, df2])\nOut[34]: \n            city  rank\n0        Chicago     1\n1  San Francisco     2\n2  New York City     3\n0        Chicago     1\n1         Boston     4\n2    Los Angeles     5\nSQL’s UNION is similar to UNION ALL, however UNION will remove duplicate rows.\nSELECT city, rank\nFROM df1\nUNION\nSELECT city, rank\nFROM df2;\n-- notice that there is only one Chicago record this time\n/*\n         city  rank\n      Chicago     1\nSan Francisco     2\nNew York City     3\n       Boston     4\n  Los Angeles     5\n*/\nIn pandas, you can use concat() in conjunction with drop_duplicates().\nIn [35]: pd.concat([df1, df2]).drop_duplicates()\nOut[35]: \n            city  rank\n0        Chicago     1\n1  San Francisco     2\n2  New York City     3\n1         Boston     4\n2    Los Angeles     5"], ",Parameters": [""], ",Returns": [""], ",Category": ["Dataframe"], "index": 10}
